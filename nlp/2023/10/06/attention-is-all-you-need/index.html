<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="utf-8">
    <meta name="google-site-verification" content="pO3QI7LuxPEWOOJ2Be7QSkgX_hkw6dHUPx0IlrrPLJY" />
    <title>Attention is All You Need - Tuan-Dung Bui</title>
    <link rel="icon" type="image/x-icon" href="/assets/media/blog-icon.jpg">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Product+Sans&family=Google+Sans+Display:ital@0;1&family=Google+Sans_old:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,700&family=Google+Sans+Text:ital,wght@0,400;0,500;0,600;0,700;1,400;1,500;1,700&display=swap"
          rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Be+Vietnam+Pro&display=swap" rel="stylesheet">
    <link href='https://fonts.googleapis.com/css?family=Inter' rel='stylesheet'>
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/github.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
    <script src="https://unpkg.com/typed.js@2.0.16/dist/typed.umd.js"></script>
    <script src="/assets/js/main.js"></script>
    <link type="application/atom+xml" rel="alternate" href="https://zhaospei.github.io//feed.xml" title="Tuan-Dung Bui" />
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Attention is All You Need | Tuan-Dung Bui</title>
<meta name="generator" content="Jekyll v4.3.3" />
<meta property="og:title" content="Attention is All You Need" />
<meta name="author" content="zhao" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Transformer là mô hình seq2seq được Google Brain đề xuất trong một bài báo xuất bản vào cuối năm 2017. Giờ đây, nó đã đạt được nhiều ứng dụng và tiện ích mở rộng và BERT là mô hình ngôn ngữ được đào tạo trước có nguồn gốc từ Transformer." />
<meta property="og:description" content="Transformer là mô hình seq2seq được Google Brain đề xuất trong một bài báo xuất bản vào cuối năm 2017. Giờ đây, nó đã đạt được nhiều ứng dụng và tiện ích mở rộng và BERT là mô hình ngôn ngữ được đào tạo trước có nguồn gốc từ Transformer." />
<link rel="canonical" href="https://zhaospei.github.io//nlp/2023/10/06/attention-is-all-you-need/" />
<meta property="og:url" content="https://zhaospei.github.io//nlp/2023/10/06/attention-is-all-you-need/" />
<meta property="og:site_name" content="Tuan-Dung Bui" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-10-06T00:00:00+07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Attention is All You Need" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"zhao"},"dateModified":"2023-10-06T00:00:00+07:00","datePublished":"2023-10-06T00:00:00+07:00","description":"Transformer là mô hình seq2seq được Google Brain đề xuất trong một bài báo xuất bản vào cuối năm 2017. Giờ đây, nó đã đạt được nhiều ứng dụng và tiện ích mở rộng và BERT là mô hình ngôn ngữ được đào tạo trước có nguồn gốc từ Transformer.","headline":"Attention is All You Need","mainEntityOfPage":{"@type":"WebPage","@id":"https://zhaospei.github.io//nlp/2023/10/06/attention-is-all-you-need/"},"url":"https://zhaospei.github.io//nlp/2023/10/06/attention-is-all-you-need/"}</script>
<!-- End Jekyll SEO tag -->

  </head>
  <body class="bg-white dark:bg-zinc-900 dark:text-white">
    <div class="mx-auto max-w-3xl px-3 sm:px-6 xl:max-w-5xl xl:px-0">
      <div id="navbar" class="hidden md:block fixed left-0 bottom-[-100px] w-full z-50 will-change-transform mt-8"
    style="transition: transform .4s ease-in, -webkit-transform .4s ease-in; transition: bottom 0.4s;"
    data-comp="JumplinksV2" aria-hidden="true">
    <ul class="bg-gray-200 border-2 rounded-full flex justify-center mx-auto p-2 w-fit"
        style="box-shadow: 0 1px 2px rgba(32,33,36,.15), 0 1px 8px rgba(32,33,36,.08);">
        
        
        <li class="shrink-0 px-1">
            <a href="/"
                class="text-[#5f6368] rounded-full px-4 py-2 leading-6 inline-block font-medium hover:bg-[#f8f9fa]"
                data-ga-config="{&quot;click&quot;: {&quot;event&quot;: &quot;anchor_link_click&quot;, &quot;link_url&quot;: &quot;fast&quot;, &quot;link_text&quot;: true, &quot;link_type&quot;: true, &quot;module_name&quot;: true, &quot;section_header&quot;: true}}"
                tabindex="-1">
                Home </a>
        </li>
        
        
        
        <li class="shrink-0 px-1">
            <a href="/works.html"
                class="text-[#5f6368] rounded-full px-4 py-2 leading-6 inline-block font-medium hover:bg-[#f8f9fa]"
                data-ga-config="{&quot;click&quot;: {&quot;event&quot;: &quot;anchor_link_click&quot;, &quot;link_url&quot;: &quot;fast&quot;, &quot;link_text&quot;: true, &quot;link_type&quot;: true, &quot;module_name&quot;: true, &quot;section_header&quot;: true}}"
                tabindex="-1">
                Works </a>
        </li>
        
        
        
        <li class="shrink-0 px-1">
            <a href="/blog.html"
                class="text-[#5f6368] rounded-full px-4 py-2 leading-6 inline-block font-medium hover:bg-[#f8f9fa]"
                data-ga-config="{&quot;click&quot;: {&quot;event&quot;: &quot;anchor_link_click&quot;, &quot;link_url&quot;: &quot;fast&quot;, &quot;link_text&quot;: true, &quot;link_type&quot;: true, &quot;module_name&quot;: true, &quot;section_header&quot;: true}}"
                tabindex="-1">
                Blog </a>
        </li>
        
        
        
        <li class="shrink-0 px-1">
            <a href="/assets/BuiTuanDung-Resume-1.pdf"
                class="text-[#5f6368] rounded-full px-4 py-2 leading-6 inline-block font-medium hover:bg-[#f8f9fa]"
                data-ga-config="{&quot;click&quot;: {&quot;event&quot;: &quot;anchor_link_click&quot;, &quot;link_url&quot;: &quot;fast&quot;, &quot;link_text&quot;: true, &quot;link_type&quot;: true, &quot;module_name&quot;: true, &quot;section_header&quot;: true}}"
                tabindex="-1">
                Resume </a>
        </li>
        
        
        
        <li class="shrink-0 px-1">
            <a href="/about/"
                class="text-[#5f6368] rounded-full px-4 py-2 leading-6 inline-block font-medium hover:bg-[#f8f9fa]"
                data-ga-config="{&quot;click&quot;: {&quot;event&quot;: &quot;anchor_link_click&quot;, &quot;link_url&quot;: &quot;fast&quot;, &quot;link_text&quot;: true, &quot;link_type&quot;: true, &quot;module_name&quot;: true, &quot;section_header&quot;: true}}"
                tabindex="-1">
                About </a>
        </li>
        
        
    </ul>
</div>
<div
    class="hidden fixed w-full h-screen inset-0 bg-gray-200 opacity-95 z-50 transition-transform transform ease-in-out duration-300 translate-x-0 mobile-menu dark:bg-zinc-800">
    <button type="button" aria-label="toggle modal"
        class="cursor-pointer fixed right-4 top-4 h-8 w-8 cursor-auto focus:outline-none border-none mobile-menu-btn-close"><svg
            xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor"
            class="text-gray-900 svg-change-color">
            <path fill-rule="evenodd"
                d="M4.293 4.293a1 1 0 011.414 0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z"
                clip-rule="evenodd"></path>
        </svg></button>
    <nav class="fixed mt-8 h-full">
        
        <div class="px-8 py-4"><a class="text-xl font-bold text-gray-900 dark:text-white" href="/"> Home </a></div>
        
        <div class="px-8 py-4"><a class="text-xl font-bold text-gray-900 dark:text-white" href="/works.html"> Works </a></div>
        
        <div class="px-8 py-4"><a class="text-xl font-bold text-gray-900 dark:text-white" href="/blog.html"> Blog </a></div>
        
        <div class="px-8 py-4"><a class="text-xl font-bold text-gray-900 dark:text-white" href="/assets/BuiTuanDung-Resume-1.pdf"> Resume </a></div>
        
        <div class="px-8 py-4"><a class="text-xl font-bold text-gray-900 dark:text-white" href="/about/"> About </a></div>
        

    </nav>
</div>

<header class="grid justify-center w-full relative pt-4" style="grid-template-columns: 2fr auto 1fr;">
    <a class="border-2 border-solid rounded-xl px-4 border-black w-1/2 text-white bg-black" href="/">
        <div class="text-dark dark:text-white flex items-center text-xl font-semibold" style="height: 48px;">
            <span class="blog-link">~/ </span>
            <div class="Typewriter font-normal ml-1" data-testid="typewriter-wrapper"><span
                    class="Typewriter__wrapper"></span><span class="Typewriter__cursor">|</span></div>
        </div>
    </a>
    <nav class="flex items-center justify-between relative max-w-3xl mx-auto text-gray-900">
        
        <a class="text-base font-semibold text-gray-800 hidden md:inline-block mx-1 p-1 sm:px-3 rounded-3xl hover:bg-gray-100 transition-all dark:text-gray-100 dark:hover:bg-zinc-950"
            href="/"><span class="capsize">Home</span></a>
        
        <a class="text-base font-semibold text-gray-800 hidden md:inline-block mx-1 p-1 sm:px-3 rounded-3xl hover:bg-gray-100 transition-all dark:text-gray-100 dark:hover:bg-zinc-950"
            href="/works.html"><span class="capsize">Works</span></a>
        
        <a class="text-base font-semibold text-gray-800 hidden md:inline-block mx-1 p-1 sm:px-3 rounded-3xl hover:bg-gray-100 transition-all dark:text-gray-100 dark:hover:bg-zinc-950"
            href="/blog.html"><span class="capsize">Blog</span></a>
        
        <a class="text-base font-semibold text-gray-800 hidden md:inline-block mx-1 p-1 sm:px-3 rounded-3xl hover:bg-gray-100 transition-all dark:text-gray-100 dark:hover:bg-zinc-950"
            href="/assets/BuiTuanDung-Resume-1.pdf"><span class="capsize">Resume</span></a>
        
        <a class="text-base font-semibold text-gray-800 hidden md:inline-block mx-1 p-1 sm:px-3 rounded-3xl hover:bg-gray-100 transition-all dark:text-gray-100 dark:hover:bg-zinc-950"
            href="/about/"><span class="capsize">About</span></a>
        
        <button
            class="cursor-pointer bg-gray-200 bg-opacity-25	backdrop-blur-sm rounded-lg text-sm border-y border-transparent mobile-menu-btn-open visible md:hidden fixed z-50 right-4 bottom-4 h-8 w-8 cursor-auto focus:outline-none border-none mobile-menu-btn"
            type="button">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor"
                class="text-gray-900 svg-change-color">
                <path fill-rule="evenodd"
                    d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z"
                    clip-rule="evenodd"></path>
            </svg>
        </button>
    </nav>
    <div class="flex justify-self-end items-center gap-8">
        <label class="relative cursor-pointer">
            <span class="sr-only">
                "change theme to"
                <!-- -->
                "dark"
            </span>
            <div class="themeSwitchToggle w-12 h-6 bg-gray-200 rounded-3xl p-1 ml-4 dark:bg-zinc-800">
                <div class="themeSwitchIcon w-6 h-6 bg-white rounded-full flex items-center justify-center dark:bg-zinc-950"
                    style="transition: transform .2s ease-in-out;">

                </div>
            </div>
        </label>
        <a class="contact-header flex font-semibold items-center gap-2 text-black text-sm dark:text-white"
            href="/contact">
            Contact
            <svg class="svg-change-color dark:text-white" viewBox="0 0 513 513" xmlns="http://www.w3.org/2000/svg"
                style="width:1.2rem;">
                <path
                    d="M440 6.5L24 246.4c-34.4 19.9-31.1 70.8 5.7 85.9L144 379.6V464c0 46.4 59.2 65.5 86.6 28.6l43.8-59.1 111.9 46.2c5.9 2.4 12.1 3.6 18.3 3.6 8.2 0 16.3-2.1 23.6-6.2 12.8-7.2 21.6-20 23.9-34.5l59.4-387.2c6.1-40.1-36.9-68.8-71.5-48.9zM192 464v-64.6l36.6 15.1L192 464zm212.6-28.7l-153.8-63.5L391 169.5c10.7-15.5-9.5-33.5-23.7-21.2L155.8 332.6 48 288 464 48l-59.4 387.3z"
                    fill=""></path>
            </svg>
        </a>
    </div>
</header>
      <main class="flex flex-col justify-center py-16">
          <!-- <style>
  .table-of-contents-container li a {
    color: rgb(122, 118, 119);
    font-weight: 500;
  }

  .table-of-contents-container li.active>a {
    color: rgb(175, 165, 248);
  }

  .table-of-contents-container li a:hover {
    filter: brightness(60%);
  }

  .table-of-contents-container {
    font-size: 14px;
  }

  .table-of-contents-container nav {}

  .toc-h1 {
    margin-top: 1em;
  }

  .toc-h2 {
    margin-top: 0.75em;
  }

  .toc-h3 {
    margin-top: 0.5em;
  }


  .toc-h1 {
    font-weight: 600;
  }

  .toc-h2 {
    padding-left: 1em;
    font-weight: 500;
  }

  .toc-h3 {
    padding-left: 2em;
    font-weight: 400;
  }

  .toc-h1:first-child {
    margin-top: 0;
  }


  @media only screen and (max-width:1280px) {
    .table-of-contents-container {
      display: none;
    }
  }
</style> -->

<!-- <style>
    .reading-line {
        position: fixed;
        top: 0;
        left: 0;
        border-bottom: 4px solid #66aacb;
        z-index: 404;
    }
</style>

<div class="reading-line">
</div> -->
<div class="post-container">
  <article class="post-content-container flex flex-col items-start justify-center">
    <header class="flex flex-col gap-12 w-full max-w-full pb-8 mb-4" style="background-image: url('/assets/media/index/cta_sparkles.svg'); background-repeat: no-repeat;
    background-size: cover;">
      <div class="flex flex-col justify-center gap-4">
        <div class="text-lg">
          
          <a class="inline-block px-4 py-1 mb-4 mr-4 border border-solid rounded-full bg-gray-100 border-slate-200 dark:text-white dark:bg-zinc-800"
            href="/tag/nlp"> nlp </a>
          
          <a class="inline-block px-4 py-1 mb-4 mr-4 border border-solid rounded-full bg-gray-100 border-slate-200 dark:text-white dark:bg-zinc-800"
            href="/tag/paper"> paper </a>
          
          <a class="inline-block px-4 py-1 mb-4 mr-4 border border-solid rounded-full bg-gray-100 border-slate-200 dark:text-white dark:bg-zinc-800"
            href="/tag/model"> model </a>
          
        </div>
        <h1 class="text-6xl font-bold leading-tight"> Attention is All You Need </h1>
        <div class="flex flex-row items-baseline mb-8">
          <div class="article-meta__abstract-container">
            <div class="flex w-max flex-col pr-4" style="border-right: 2px solid #dadce0">
              <p class="text-gray-500 dark:text-gray-400">October 6, 2023</p>

              <p class="text-gray-500 dark:text-gray-400" data-reading-time-render="">
                

14 min read

              </p>

            </div>
          </div>
          <p class="text-gray-700 pl-4 dark:text-gray-300">
            Transformer là mô hình seq2seq được Google Brain đề xuất trong một bài báo xuất bản vào cuối năm 2017. Giờ đây, nó đã đạt được nhiều ứng dụng và tiện ích mở rộng và BERT là mô hình ngôn ngữ được đào tạo trước có nguồn gốc từ Transformer.
          </p>

        </div>
        <div class="flex justify-between">
          <div class="flex">
            
            

            <div class="mr-4" data-component="uni-monogram" data-author="Tuan-Dung Bui">
              <img class="object-cover w-[48px] h-[48px] rounded-full" src="/assets/media/author/zhao-img.png" alt="Tuan-Dung Bui">
              
            </div>

            <div class="article-meta__author-info">
              <div class="font-extrabold text-gray-900 text-lg dark:text-gray-200">Tuan-Dung Bui</div>

              <div class="text-sm font-medium text-gray-600 dark:text-gray-400">
                Developer and author at Zhao.
              </div>


            </div>
            
          </div>
            <div class="uni-social-share uni-social-share--desktop" data-component="uni-social-share-dropdowm">
              <a aria-label="Share" class="flex gap-2 text-black" role="button" tabindex="0" aria-expanded="false"
                data-analytics="{
                &quot;event&quot;: &quot;page interaction&quot;,
                &quot;category&quot;: &quot;social&quot;,
                &quot;action&quot;: &quot;menu&quot;,
                &quot;label&quot;: &quot;label&quot;
              }" target="_blank">
              <svg xmlns="http://www.w3.org/2000/svg" class="icon icon-tabler icon-tabler-share inline-block" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round">
                <path stroke="none" d="M0 0h24v24H0z" fill="none"></path>
                <path d="M6 12m-3 0a3 3 0 1 0 6 0a3 3 0 1 0 -6 0"></path>
                <path d="M18 6m-3 0a3 3 0 1 0 6 0a3 3 0 1 0 -6 0"></path>
                <path d="M18 18m-3 0a3 3 0 1 0 6 0a3 3 0 1 0 -6 0"></path>
                <path d="M8.7 10.7l6.6 -3.4"></path>
                <path d="M8.7 13.3l6.6 3.4"></path>
             </svg>
                <div class="uni-social-share__button">Share</div>
              </a>

            </div>

        </div>
      </div>
    </header>
    <div class="flex justify-center w-full flex-row-reverse">
      <!-- <aside class="table-of-contents-container ml-16 sticky top-16 mt-16" style="max-height: calc(100vh - 16.5rem);">
        <h2 class="uppercase tracking-widest text-base	font-extrabold mb-6"> Table of contents </h2>
        <nav class="overflow-auto pr-2" style="max-height: calc(100vh - 16.5rem);">
          <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#-self-attention"># Self-Attention</a></li>
<li class="toc-entry toc-h1"><a href="#-encoder"># Encoder</a>
<ul>
<li class="toc-entry toc-h2"><a href="#position-wise-fully-connected-feed-forward-network">position-wise fully connected feed-forward network</a></li>
<li class="toc-entry toc-h2"><a href="#residual-connection">residual connection</a></li>
<li class="toc-entry toc-h2"><a href="#batch-norm-và-layer-norm">Batch Norm và Layer Norm</a></li>
<li class="toc-entry toc-h2"><a href="#toàn-bộ-kiến-trúc-encoder">Toàn bộ kiến trúc Encoder</a>
<ul>
<li class="toc-entry toc-h3"><a href="#input--positional-embedding">input &amp; positional embedding</a></li>
<li class="toc-entry toc-h3"><a href="#multi-head-attention">multi-head attention</a></li>
<li class="toc-entry toc-h3"><a href="#add--norm">add &amp; norm</a></li>
<li class="toc-entry toc-h3"><a href="#feed-forward">feed forward</a></li>
<li class="toc-entry toc-h3"><a href="#add--norm-1">add &amp; norm</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#-decoder"># Decoder</a></li>
<li class="toc-entry toc-h1"><a href="#-word-embedding-và-positional-embedding"># Word Embedding và Positional Embedding</a>
<ul>
<li class="toc-entry toc-h2"><a href="#word-embedding">Word Embedding</a></li>
<li class="toc-entry toc-h2"><a href="#positional-embedding">Positional Embedding</a>
<ul>
<li class="toc-entry toc-h3"><a href="#nhúng-vị-trí-tùy-chinh">Nhúng vị trí tùy chinh</a></li>
<li class="toc-entry toc-h3"><a href="#nhúng-từ-vị-trí-lý-tuởng">Nhúng từ vị trí “lý tuởng”</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#-q--a"># Q &amp; A</a>
<ul>
<li class="toc-entry toc-h2"><a href="#tại-sao-transformer-cần-multi-head-attention-">Tại sao Transformer cần Multi-head Attention ?</a></li>
<li class="toc-entry toc-h2"><a href="#ưu-điểm-của-transformer-so-với-rnnlstm-là-gì-tại-sao">Ưu điểm của Transformer so với RNN/LSTM là gì? Tại sao?</a></li>
<li class="toc-entry toc-h2"><a href="#tại-sao-transformer-có-thể-thay-thế-seq2seq">Tại sao Transformer có thể thay thế seq2seq?</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#tham-khảo">Tham khảo</a></li>
</ul>
        </nav>
      </aside> -->
      <div class="items-center pb-8 w-full" style="grid-template-rows: auto 1fr;">
        <div class="thumbnail-image relative not-prose rounded-2xl overflow-hidden">
          
          <img alt="Post image feature" src="/assets/media/feature/transformers.jpg" loading="lazy" class="w-full">
          
        </div>
        <div class="post-content prose w-full mb-16">
          <p><code class="language-plaintext highlighter-rouge">Transformer</code> là mô hình seq2seq được Google Brain đề xuất trong một bài báo xuất bản vào cuối năm 2017. Giờ đây, nó đã đạt được nhiều ứng dụng và tiện ích mở rộng và <code class="language-plaintext highlighter-rouge">BERT</code> là mô hình ngôn ngữ được đào tạo trước có nguồn gốc từ Transformer.</p>

<p>Việc đào tạo <code class="language-plaintext highlighter-rouge">RNN</code> truyền thống là nối tiếp và nó phải đợi từ hiện tại được xử lý trước khi có thể xử lý từ tiếp theo. Transformer được huấn luyện song song, tức là tất cả các từ đều được huấn luyện cùng một lúc, điêu này làm tăng đáng kể hiệu quả tính toán.</p>

<figure class="image" style="text-align: center;">
    <img src="/assets/media/post/transformer-model-architecture.png" alt="Kiến trúc mô hình Transformer" style="margin: auto;" />
    <figcaption style="font-style: italic;">Kiến trúc mô hình Transformer</figcaption>
</figure>

<h1 id="-self-attention"># Self-Attention</h1>
<p><code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> là tích chấm chuẩn hóa Attention, chi tiết cụ thể được thể hiện trong hình.</p>

<figure class="image" style="text-align: center;">
    <img src="/assets/media/post/attention.png" alt="" style="margin: auto;" />
    <figcaption style="font-style: italic;"></figcaption>
</figure>

\[Attention(Q,K,V)=softmax(\dfrac{QK^T}{\sqrt{d_k}})V\]

<p>Sự chú ý của nhiều đầu vào sử dụng nhiều bộ trọng số (<code class="language-plaintext highlighter-rouge">weights</code>) (\(W_q,W_k,W_v\)), ghép lại cho ra kết quả cuối cùng.</p>

\[MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O\]

<p>trong đó</p>

\[head_i=Attention(QW^Q_i,KW^K_i,VW^V_i)\]

<p>Trong đó \(h = 8\), \(d_q=d_k=d_v=d_{model}/4=64\).</p>

<h1 id="-encoder"># Encoder</h1>
<p>Encoder được xếp chồng lên nhau bởi sáu lớp giống hệt nhau, mỗi lớp bao gồm hai lớp con - cơ chế tự chú ý nhiều đầu (<code class="language-plaintext highlighter-rouge">multi-head self-attention mechanism</code>) và mạng nơ ron vị trí chuyển tiếp được kết nối đầy đủ (<code class="language-plaintext highlighter-rouge">position-wise fully connected feed-forward network</code>). Mỗi lớp con sử dụng các kết nối dư (<code class="language-plaintext highlighter-rouge">residual connection</code>) và lớp chuẩn hóa (<code class="language-plaintext highlighter-rouge">layer normalization</code>). Kích thước đầu ra của các lớp con là \(d_{model} = 512\).</p>

<p>Đầu ra của lớp con có thể được biểu diễn dưới dạng:</p>

\[LayerNorm(x+Sublayer(x))\]

<h2 id="position-wise-fully-connected-feed-forward-network">position-wise fully connected feed-forward network</h2>
<p>Mạng nơ-ron chuyển tiếp được kết nối đầy đủ (<code class="language-plaintext highlighter-rouge">position-wise fully connected feed-forward network</code>) bao gồm hai phép biến đổi tuyến tính với kích hoạt <code class="language-plaintext highlighter-rouge">ReLU</code> ở giữa.</p>

\[FFN(x)=ReLU(xW_1+b_1)W_2+b_2\]

<p>Kích thước lớp bên trong (inner-layer) là 2048.</p>

<h2 id="residual-connection">residual connection</h2>
<p>Mạng dư (<code class="language-plaintext highlighter-rouge">Residual Network</code>), các kết nối tắt có khả năng bỏ qua một hoặc nhiều lớp, do sự tồn tại của kết nối tắt nên hiệu suất của mạng sâu (có nhiều lớp) không kém hơn so với các mạng nông (mạng có ít lớp). Phương pháp này giải quyết vấn đề suy thoái do các lớp chập xếp chồng lên nhau gây ra, số lượng lớp của mạng nơ-ron tích chập đã được tăng lên rất nhiều lên hàng trăm lớp, và cải thiện đáng kể hiệu suất của mạng thần kinh tích chập (<code class="language-plaintext highlighter-rouge">resnet</code>).</p>

<figure class="image" style="text-align: center;">
    <img src="/assets/media/post/resnet.png" alt="" style="margin: auto;" />
    <figcaption style="font-style: italic;"></figcaption>
</figure>

<h2 id="batch-norm-và-layer-norm">Batch Norm và Layer Norm</h2>

<figure class="image" style="text-align: center;">
    <img src="/assets/media/post/normalization.png" alt="" style="margin: auto;" />
    <figcaption style="font-style: italic;"></figcaption>
</figure>

<p>Đặt kích thước hình ảnh đầu vào là \([N, C, H, W]\):</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">Batch Norm</code>, chuẩn hóa theo từng batch NHW, là để chuẩn hóa đầu vào từng kênh đơn, đều này không hiệu quả đối với <code class="language-plaintext highlighter-rouge">batch-size</code> nhỏ.</li>
  <li><code class="language-plaintext highlighter-rouge">Layer Norm</code>, chuẩn hóa theo từng layer CHW, là để chuẩn hóa đầu vào ở mỗi độ sâu, chủ yếu có tác dụng rõ ràng trên RNN.</li>
</ul>

<p>Sự hiểu biết cá nhân:</p>

<ul>
  <li>Dành cho CNN, nếu hạt nhân tích chập quét hình ảnh đầu vào, nó được tính là thao tác tích chập, cần có tổng thao tác batchsize. Do đó, chuẩn hóa cần được thực hiện theo batch.</li>
  <li>Dành cho RNN, batchsize thường là 1, số vòng lặp là số độ dài đầu vào (số channel). Do đó, chuẩn hóa cần được thực hiện theo channel.</li>
</ul>

<h2 id="toàn-bộ-kiến-trúc-encoder">Toàn bộ kiến trúc Encoder</h2>
<h3 id="input--positional-embedding">input &amp; positional embedding</h3>

\[X=Embedding-Lookup(X)+Positional-Encoding\]

<h3 id="multi-head-attention">multi-head attention</h3>

\[Q=Linear_q(X)=XW_q\]

\[K=Linear_q(X)=XW_k\]

\[V=Linear_v(X)=XW_v\]

\[X_{attention}=Self-Attention(Q,K,V)\]

<h3 id="add--norm">add &amp; norm</h3>

\[X_{attention}=LayerNorm(X+X_{attention})\]

<h3 id="feed-forward">feed forward</h3>

\[X_{hidden}=Linear(ReLU(Linear(X_{attention})))\]

<h3 id="add--norm-1">add &amp; norm</h3>

\[X_{hidden}=LayerNorm(X_{hidden}+X_{attention})\]

<p><code class="language-plaintext highlighter-rouge">multi-head attention</code> trong <code class="language-plaintext highlighter-rouge">Encoder</code> là một cơ chế tự chú ý (<code class="language-plaintext highlighter-rouge">self-attention mechanism</code>). \(k\), \(q\) và \(v\) trong cơ chế tự chú ý đều xuất phát từ cùng một vị trí, mỗi lớp của Encoder có thể nhận được tất cả vị trí của lớp trước.</p>

<h1 id="-decoder"># Decoder</h1>
<p>Decoder bao gồm sáu lớp giống hệt xếp chồng lên nhau; trong Multi-head Attention, \(q\) được đến từ lớp trước đó của Decoder, k và v đến từ đầu ra của Encoder. Điều cho phép mỗi vị trí trong Decoder nhận biết được tất cả các vị trí của chuỗi đầu vào.</p>

<p>Ngoài hai lớp con trong Encoder, Decoder thêm một lớp con mới xử lý đầu ra của Encoder - <code class="language-plaintext highlighter-rouge">masked multi-head self-attention mechanism</code>. Encoder trong seq2seq truyền thống sử dụng mô hình RNN, vì vậy nếu các từ tại thời điểm t được nhập vào trong quá trình huấn luyện thì mô hình sẽ không thể nhìn thấy các từ trước đó vào các thời điểm trong tương lai, bởi vì RNN hoạt động theo thời gian và chỉ khi thao tác tại thời điểm t hoàn thành, chỉ khi đó ta mới có thể nhìn thấy các từ tại thời điểm t + 1. Và Transformer Decoder đã không sử dụng RNN, thay đổi sang Self-Attention, điều này tạo ra một vấn đề, trong quá trình huấn luyện, toàn bộ ground truth đã được hiển thị với Decoder, điều này rõ ràng là sai, chúng ta cần phải thực hiện một số xử lý trên đầu vào của Decoder, quá trình này được gọi là <code class="language-plaintext highlighter-rouge">Mask</code> - Đặt tất cả các giá trị sau postion thành \(-\infty\) trước khi vào softmax.</p>

<p>Ví dụ, ground truth của Decoder là “&lt;start&gt; I am fine”, chúng ta cho câu này vào bộ Decoder, sau khi Word Embedding và Positional Encoding, thực hiện phép biến đổi tuyến tính bậc 3 trên ma trận thu được \((W_Q,W_K,W_V)\) Sau đó thực hiện self-attention, trước tiên, nhận Scaled Scores thông qua \(\dfrac{Q×K^T}{\sqrt{d_k}}\), bước tiếp theo rất quan trọng, chúng ta cần mask theo Scaled Scores, ví dụ, khi nhập “I”, hiện tại mô hình chỉ biết thông tin của tất cả các từ trước đó của “I”, tức thông tin của “&lt;start&gt;” và “I”, không được phép biết được thông tin của các từ sau “I”. Lý do rất đơn giản, khi dự đoán là chúng ta dự đoán theo thứ tự từng chữ, làm sao có thể biết được thông tin của những từ sau trước khi dự đoán xong từ này? Mask rất đơn giản, đầu tiên tạo một ma trận có tam giác hoàn toàn phía dưới bằng 0 và tam giác hoàn tòan phía trên bằng âm vô cùng, sau đó chỉ cần thêm nó vào Scaled Scores.</p>

<h1 id="-word-embedding-và-positional-embedding"># Word Embedding và Positional Embedding</h1>
<h2 id="word-embedding">Word Embedding</h2>
<p>Phần nhúng từ sử dụng nhúng từ có thể học được, kích thước của nó là \(d_{model}\).
Hình thức mã hóa <code class="language-plaintext highlighter-rouge">One-hot</code> ngắn gọn, nhưng quá thưa thớt, nó không phản ánh sự giống nhau về nghĩa của từ. Vì vậy hãy sử dụng <code class="language-plaintext highlighter-rouge">the Skip-Gram Model</code> hoặc <code class="language-plaintext highlighter-rouge">continuous bag of words model</code> hoặc các nhúng từ khác có thể học được khác.</p>

<h2 id="positional-embedding">Positional Embedding</h2>
<p>Bởi vì mô hình không bao gồm các cấu trúc tuần hoàn, vì vậy nắm bắt được các thông tin thứ tự tuần tự, ví dụ nếu \(K\) và \(V\) được xóa trộn theo từng hàng thì kết quả sau Attention sẽ giống nhau. Tuy nhiên, thông tin tuần tự rất quan trọng và thể hiện cấu trúc toàn cầu, do đó thông tin position tuyệt đối và tương đối của token tuần tự phải được sử dụng.</p>
<h3 id="nhúng-vị-trí-tùy-chinh">Nhúng vị trí tùy chinh</h3>
<p>Một ý tưởng là lấy một số trong khoảng \([0, 1]\) và gán nó cho mỗi từ, trong đó 0 được trao cho từ đầu tiên, 1 cho từ cuối cùng, công thức cụ thể là \(PE=\dfrac{pos}{T−1}\). Vấn đề của việc gán theo công thức này là nó bị phụ thuộc và kích thước của văn bản. Tức
là văn bản có số kí tự là 30. Khi đó theo công thức trên, thì khoảng cách giữa hai từ sẽ là 0.0333. Khi văn bản khác có số lượng kí từ &lt; 30, thì con số 0.0333 vẫn mô tả đúng vị trí tương đối giữa chúng, tuy nhiên với văn bản &gt; 30, ví dụ 90 thì 0.0333 đang gộp khoảng cách thực tế đang được phân tách bởi hai ký tự. Điều này rõ ràng là không phù hợp, vì sự khác biệt giống nhau không có nghĩa là giống nhau trong các câu khác nhau.</p>

<p>Một ý tưởng khác là gắn tuyến tính mỗi bước theo thời gian, nghĩa là từ đầu tiên được gán là 1, từ thứ hai được gán là 2, … Phương pháp này cũng có những vấn đề lớn: 1. Nó lớn hơn giá trị nhúng từ thông từ, có thể gây nhiễu cho mô hình; 2. Ký tự cuối cùng lớn hơn nhiều ký tự đầu tiên, sau khi hợp nhất với các từ nhúng, giá trị của các đặc trưng sẽ bị sai lệch.</p>

<h3 id="nhúng-từ-vị-trí-lý-tuởng">Nhúng từ vị trí “lý tuởng”</h3>
<p>Một lý tưởng là thiết kế nhúng vị trí phải đáp ứng những tiêu chí sau:</p>
<ul>
  <li>Nó sẽ xuất ra mã hóa duy nhất cho mỗi từ.</li>
  <li>Sự khác biệt giữa hai từ phải nhất quán giữa các câu có độ dài khác nhau.</li>
  <li>Giá trị của nó phải được giới hạn.</li>
</ul>

<p>Do đó việc nhúng vị trí sin và cosin đã được sử dụng cho Transformer.</p>

<p>Bây giờ hãy định nghĩa lại Positional Embedding, kích thước của việc nhúng vị trí là <code class="language-plaintext highlighter-rouge">[max_sequence_length, embedding_dimension]</code>, kích thước của phần nhúng vị trí giống với kích thước của vector từ, đều bằng <code class="language-plaintext highlighter-rouge">embedding_dimension</code>. <code class="language-plaintext highlighter-rouge">max_sequence_length</code> là một hyperparameter, đề cập đến số lượng tối đa mà một câu bao gồm.</p>

<p>Kích thước của việc nhúng vị trí cũng giống như kích thước của việc nhúng từ, cùng là \(d_{model}\). Công thước tính toán của nó là:</p>

\[PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})\]

\[PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})\]

<p>Trong đó, \(pos\) đại diện cho chỉ mục vị trí, \(i\) đại diện cho chỉ số chiều. Nghĩa là mỗi chiều \(i\) của positional embedding pos tương ứng với một sóng sin.</p>

<p>Trong hình dưới này minh họa cho cách tính position embedding của tác giả với số chiều là 6. Giá trị của các vector tại mỗi vị trí được tính toán theo công thức ở hình dưới.</p>
<figure class="image" style="text-align: center;">
    <img src="/assets/media/post/pe.png" alt="" style="margin: auto;" />
    <figcaption style="font-style: italic;"></figcaption>
</figure>

<p>Bản thân việc nhúng vị trí là một thông tin vị trí tuyệt đối, nhưng trong ngôn ngữ, vị trí tương đối cũng rất quan trọng, bởi vì</p>

\[sin(\alpha+\beta)=sin\alpha cos\beta+cos\alpha sin\beta\cos(\alpha+\beta)=cos\alpha cos\beta-sin\alpha sin\beta\]

<p>cho thấy vector tại vị trí \(p + k\) có thể được biểu diễn dưới dạng phép biến đổi tuyến tính của vectơ tại vị trí \(p\), điều này cung cấp khả năng thể hiện thông tin vị trí tương đối. Phiên bản hình sin cũng cho phép mô hình ngoại suy với độ dài chuỗi dài hơn so với độ dài chuỗi gặp phải trong quá trình huấn luyện.</p>

<h1 id="-q--a"># Q &amp; A</h1>
<h2 id="tại-sao-transformer-cần-multi-head-attention-">Tại sao Transformer cần Multi-head Attention ?</h2>
<p>Bài báo đề cập lý do việc tiến hành Multi-head Attention là để chia mô hình thành nhiều đầu để tạo thành nhiều không gian con, cho phép mô hình chú ý đến các khía cạnh khác nhau của thông tin và cuối cùng tổng hợp thông tin từ tất cả các khía cạnh. Trên thực tế, có thể hình dung bằng trực giác rằng nếu bạn tự thiết kế một mô hình như vậy, attention sẽ không chỉ được thực hiện một lần, kết quả tổng hợp của nhiều lần chú ý ít nhất có thể nâng cao mô hình và cũng có thể được so sánh với vai trò của việc sử dụng nhiều tích chập cùng lúc trong CNN, theo trực giác, sự chú ý của nhiều người đứng đầu giúp mạng nắm bắt được các tính năng/ thông tin phong phú hơn.</p>
<h2 id="ưu-điểm-của-transformer-so-với-rnnlstm-là-gì-tại-sao">Ưu điểm của Transformer so với RNN/LSTM là gì? Tại sao?</h2>
<ol>
  <li>Các mô hình RNN không thể tính toán song song vì việc tính toán tại thời điểm T phụ thuộc vào kết quả tính toán của lớp ẩn tại thời điểm T - 1, còn việc tính toán tại thời điểm T - 1 lại phụ thuộc tính toán của lớp ẩn tại thời điểm T - 2.</li>
  <li>Khả năng trích xuất đặc trưng của Transformer tốt hơn so với các mô hình RNN.</li>
</ol>

<h2 id="tại-sao-transformer-có-thể-thay-thế-seq2seq">Tại sao Transformer có thể thay thế seq2seq?</h2>
<p>Từ thay thế ở đây hơi không phù hợp, seq2seq tuy cũ nhưng vẫn có chỗ đứng, vấn đề lớn nhất của seq2seq là ở chỗ <strong>Nén thông tin ở phía Encoder thành một vector có độ dài cố định</strong> và sử dụng nó làm đầu vào của trạng thái đầu tiên ở phía Decoder, để dự đoán trạng thái ẩn của từ đầu tiên (mã thông báo) ở phía Decoder. Khi chuỗi đầu vào tương đối dài, điều này rõ ràng sẽ mất rất nhiều thông tin ở phía Encoder và vector cố định sẽ được gửi đến phía Decoder cùng một lúc, <strong>bên Decoder không thể chú ý đến thông tin mà nó muốn chú ý</strong>. Mô hinh transformer không chỉ cải thiện đáng kể hai khuyết điểm này của mô hình seq2seq (Mô-đun attention tương tác nhiều đầu), và cũng giới thiệu mô-đun self-attention, trước tiên hãy để trình tự nguồn và trình tự đích được “tự liên kết”, trong trường hợp này, thông tin chứa trong embedding của trình tự nguồn và trình tự đích sẽ phong phú hơn và lớp FFN tiếp theo cũng nâng cao khả năng biểu đạt của mô hình, và tính toán song song của Transfomer vượt xa các model seq2seq.</p>

<h1 id="tham-khảo">Tham khảo</h1>

<p>[1] <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></p>


           
            <p class="text-right italic">Last Update: October 8, 2023 </p>
          
        </div>

        <div class="flex justify-between items-center w-full flex-wrap gap-4 pb-4"><a class="" rel="noreferrer noopener"
            target="_blank" href="https://github.com/zhaospei/zhaospei.github.io/tree/main/_posts/2023-10-06-attention-is-all-you-need.md"><span
              class="font-normal link-hover">Edit on Github</span><span class="inline-block w-3 h-3 ml-2 mt-1 mr-4"><svg
                class="block w-full h-full" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
                <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2" stroke-linecap="round"
                  stroke-linejoin="round"></path>
              </svg></span></a>
          <div class="flex justify-between items-center flex-wrap gap-8"><a class="cursor-pointer"
              rel="noreferrer noopener" target="_blank"
              onclick="window.open('ttps://twitter.com/intent/tweet?text=https://zhaospei.github.io/');"><span
                class="font-normal link-hover">Tweet this article</span><span
                class="inline-block w-3 h-3 ml-2 mt-1 mr-4"><svg class="block w-full h-full" viewBox="0 0 14 14"
                  fill="none" xmlns="http://www.w3.org/2000/svg">
                  <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2" stroke-linecap="round"
                    stroke-linejoin="round"></path>
                </svg></span></a><a class="cursor-pointer" rel="noreferrer noopener" target="_blank"
              onclick="window.open('http://www.facebook.com/share.php?u=https://zhaospei.github.io/');"><span
                class="font-normal link-hover">Share on Facebook</span><span
                class="inline-block w-3 h-3 ml-2 mt-1 mr-4"><svg class="block w-full h-full" viewBox="0 0 14 14"
                  fill="none" xmlns="http://www.w3.org/2000/svg">
                  <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2" stroke-linecap="round"
                    stroke-linejoin="round"></path>
                </svg></span></a></div>
          <div>
            <div
              class="flex justify-between items-start w-full gap-16 flex-col md:flex-row flew-nowrap pt-4 pl-2 pr-2 border-gray-300 dark:boder-gray-900 border-0 border-t border-solid">
              <div class="relative w-36 h-36">
                <div class="relative overflow-hidden h-full rounded-full">
                  <div class="">
                    <div class="skeleton_skeleton__w5sWr" style="max-width: 100%; height: 100%;"></div>
                  </div>
                  <div class="rounded-full" data-loaded="true"><img alt="Bartosz Zagrodzki" loading="lazy"
                      decoding="async" data-nimg="fill" sizes="100vw" src="/assets/media/author/zhao-img.png"
                      style="position: absolute; height: 100%; width: 100%; inset: 0px; object-fit: cover; object-position: center center; color: transparent;">
                  </div>
                </div>
              </div>
              <div class="" style="flex: 1 1">
                <div class="font-bold text-2xl mb-4">Written by Tuan-Dung Bui</div>
                <p class="text-gray-700 dark:text-gray-200">Tuan-Dung Bui is a blogger, software engineer and the main
                  coordinator of this blog, he has lots of ideas and won't hesitate to use them! He lives in Vietnam.
                </p>
                <div class="mt-4"><a class="" rel="noreferrer noopener" target="_blank" href="/about"><span
                      class="font-normal link-hover">Learn more about
                      Tuan-Dung Bui</span><span class="inline-block w-3 h-3 ml-2 mt-1 mr-4"><svg viewBox="0 0 14 14"
                        fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2" stroke-linecap="round"
                          stroke-linejoin="round"></path>
                      </svg></span></a></div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>


    <section class="page-navigation flex justify-center items-start flex-col flex-wrap w-full p-0 mt-16 mb-8">
      <h2 class="mb-4 text-3xl max-w-3xl font-bold mb-8">Check my other posts 📚</h2>
      <div class="w-full items-start justify-center gap-8 flex flex-col md:flex-row">
        
        <a class="w-full container-hover" href="/nlp/2023/10/20/word2vec/">
          <div class="w-full">
            <div class="mb-4 h-48 content-hover" style="transform: none;">
              <div class="h-full relative">
                <div data-loaded="true"><img class="transition-all" alt="" loading="lazy" decoding="async"
                    data-nimg="fill" sizes="100vw" src="/assets/media/feature/pytorch.png"
                    style="position: absolute; height: 100%; width: 100%; inset: 0px; object-fit: cover; object-position: center center; color: transparent; border-radius: 1rem;">
                </div>
              </div>
            </div>
            <div class="flex items-center justify-start text-gray-600 dark:text-gray-300 text-sm gap-2 px-2"><time>October 20, 2023</time>•
              <span>
                </span>
            </div>
            <h3 class="px-2 py-2 text-black dark:text-white text-2xl font-bold">Phép thuật tuyệt vời của việc đếm tần số Word2Vec, Glove, Fasttext</h3>
          </div>
        </a>
        
        <a class="w-full container-hover" href="/nlp/2023/10/06/seq2seq-pytorch/">
          <div class="w-full">
            <div class="mb-4 h-48 content-hover" style="transform: none;">
              <div class="h-full relative">
                <div data-loaded="true"><img class="transition-all" alt="" loading="lazy" decoding="async"
                    data-nimg="fill" sizes="100vw" src="/assets/media/feature/pytorch.png"
                    style="position: absolute; height: 100%; width: 100%; inset: 0px; object-fit: cover; object-position: center center; color: transparent; border-radius: 1rem;">
                </div>
              </div>
            </div>
            <div class="flex items-center justify-start text-gray-600 dark:text-gray-300 text-sm gap-2 px-2"><time>October 6, 2023</time>•
              <span>
                
                14 min read
                </span>
            </div>
            <h3 class="px-2 py-2 text-black dark:text-white text-2xl font-bold">Triển khai seq2seq với Pytorch</h3>
          </div>
        </a>
        
        <a class="w-full container-hover" href="/nlp/2023/10/06/nlp-papers-to-be-read/">
          <div class="w-full">
            <div class="mb-4 h-48 content-hover" style="transform: none;">
              <div class="h-full relative">
                <div data-loaded="true"><img class="transition-all" alt="" loading="lazy" decoding="async"
                    data-nimg="fill" sizes="100vw" src="/assets/media/feature/nlp.png"
                    style="position: absolute; height: 100%; width: 100%; inset: 0px; object-fit: cover; object-position: center center; color: transparent; border-radius: 1rem;">
                </div>
              </div>
            </div>
            <div class="flex items-center justify-start text-gray-600 dark:text-gray-300 text-sm gap-2 px-2"><time>October 6, 2023</time>•
              <span>
                
                12 min read
                </span>
            </div>
            <h3 class="px-2 py-2 text-black dark:text-white text-2xl font-bold">NLP Papers to be Read</h3>
          </div>
        </a>
        
      </div>
    </section>

  </article>
</div>

<div id="disqus_thread"></div>
<script>
  /**
  *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
  *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
  /*
  var disqus_config = function () {
  this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
  this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
  };
  */
  (function () { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://zhaospei.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by
    Disqus.</a></noscript>

<!-- <script async src="/assets/js/scroll.js"></script> -->
<script async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  type="text/javascript"></script>
      </main>
    </div>
    <footer class="flex flex-col justify-center items-center w-full bg-gray-100 dark:bg-zinc-800">
        <div
                class="pt-8 pb-4 px-4 flex justify-center items-start w-full flex-col flex-wrap max-w-5xl box-border gap-4">
                <h4 class="text-3xl font-extrabold"> Let's build something together!</h4>
                <p class="max-w-xl">Feel free to reach out if you're looking for a developer, have a question or just
                        want to connect 📭</p>
                <a rel="noreferrer noopener" target="_blank" href="mailto:dungbuit1k28@gmail.com">
                        <span class="link-hover">dungbuit1k28@gmail.com</span>
                        <span class="inline-block w-3 h-3 mt-2 ml-1"><svg class="block w-full h-full"
                                        viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
                                        <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2"
                                                stroke-linecap="round" stroke-linejoin="round"></path>
                                </svg></span>
                </a>

                <div class="flex justify-between items-center w-full mt-8">
                        <a style="max-width: 3rem;" href="/">
                                <span class="sr-only">home</span>
                                <img class="rounded-full" alt="blog-icon" src="/assets/media/blog-icon.jpg">
                        </a>
                        <div class="justify-center items-center gap-8 hidden md:flex">
                                <a href="/sitemap.xml" rel="noreferrer noopener" target="_blank">
                                        <span class="link-hover"> sitemap</span>
                                        <span class="inline-block w-3 h-3 mt-2 ml-1"><svg class="block w-full h-full"
                                                        viewBox="0 0 14 14" fill="none"
                                                        xmlns="http://www.w3.org/2000/svg">
                                                        <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2"
                                                                stroke-linecap="round" stroke-linejoin="round"></path>
                                                </svg></span>
                                </a>
                                <a href="/feed.xml" rel="noreferrer noopener" target="_blank">
                                        <span class="link-hover"> rss</span>
                                        <span class="inline-block w-3 h-3 mt-2 ml-1"><svg class="block w-full h-full"
                                                        viewBox="0 0 14 14" fill="none"
                                                        xmlns="http://www.w3.org/2000/svg">
                                                        <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2"
                                                                stroke-linecap="round" stroke-linejoin="round"></path>
                                                </svg></span>
                                </a>
                                <a href="/assets/BuiTuanDung-Resume-1.pdf" rel="noreferrer noopener" target="_blank">
                                        <span class="link-hover"> resume</span>
                                        <span class="inline-block w-3 h-3 mt-2 ml-1"><svg class="block w-full h-full"
                                                        viewBox="0 0 14 14" fill="none"
                                                        xmlns="http://www.w3.org/2000/svg">
                                                        <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2"
                                                                stroke-linecap="round" stroke-linejoin="round"></path>
                                                </svg></span>
                                </a>
                                <a href="https://github.com/zhaospei" rel="noreferrer noopener" target="_blank">
                                        <span class="link-hover"> github</span>
                                        <span class="inline-block w-3 h-3 mt-2 ml-1"><svg class="block w-full h-full"
                                                        viewBox="0 0 14 14" fill="none"
                                                        xmlns="http://www.w3.org/2000/svg">
                                                        <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2"
                                                                stroke-linecap="round" stroke-linejoin="round"></path>
                                                </svg></span>
                                </a>
                        </div>

                </div>

                <span class="text-sm text-gray-700 dark:text-gray-200">
                        © 2023 <span class="text-cyan-900 dark:text-cyan-600">real</span> day, <span
                                class="text-cyan-900 dark:text-cyan-600">real</span>
                        time. Dung BTuan. All rights reserved.
                </span>
                <small class="text-gray-600 dark:text-gray-300"> <a
                                href="https://github.com/zhaospei/zhaospei.github.io" target="_blank"
                                rel="noreferrer noopener">polygon</a> theme on <a href="https://jekyllrb.com"
                                target="_blank" rel="noreferrer noopener">jekyll</a> </small>
        </div>
        <div class="cap text-right mx-auto my-4" data-about-footer="" data-ng-class="{'footer--hidden': footerCtrl.footerHidden}">
                <a href="#" class="scroll-to-top text-[#1a73e8] font-semibold tracking-wide" data-scroll="" data-track-event="" data-track-event-category="about us"
                        data-track-event-action="back-to-top"
                        data-track-event-label="/intl/ALL_vn/stories/machine-learning-qa/index.html">
                        Quay lại đầu trang <svg alt="" class="scroll-to-top-icon fill-current truncate align-middle inline-block" xmlns="http://www.w3.org/2000/svg"
                                width="16px" height="16px" viewBox="0 0 20 20">
                                <g stroke="none" stroke-width="1" fill="inherit">
                                        <circle fill="inherit" cx="9" cy="9" r="9"></circle>
                                        <polygon fill="#fff"
                                                points="3 9 4.0575 10.0575 8.25 5.8725 8.25 15 9.75 15 9.75 5.8725 13.935 10.065 15 9 9 3">
                                        </polygon>
                                </g>
                        </svg>
        
                </a>
        </div>
</footer>

<div class="fill-dark fill-white"></div>
    <script src="/assets/js/vanilla-tilt.js"></script>
    <script src="/assets/js/app.js"></script>
  </body>
</html>