<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="utf-8">
    <title>Attention is All You Need - Tuan-Dung Bui</title>
    <link rel="icon" type="image/x-icon" href="/assets/media/blog-icon.jpg">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/github.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
    <script src="https://unpkg.com/typed.js@2.0.16/dist/typed.umd.js"></script>
    <script src="/assets/js/main.js"></script>
    <link type="application/atom+xml" rel="alternate" href="https://zhaospei.github.io//feed.xml" title="Tuan-Dung Bui" />
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Attention is All You Need | Tuan-Dung Bui</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Attention is All You Need" />
<meta name="author" content="zhao" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Transformer lÃ  mÃ´ hÃ¬nh seq2seq Ä‘Æ°á»£c Google Brain Ä‘á» xuáº¥t trong má»™t bÃ i bÃ¡o xuáº¥t báº£n vÃ o cuá»‘i nÄƒm 2017. Giá» Ä‘Ã¢y, nÃ³ Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c nhiá»u á»©ng dá»¥ng vÃ  tiá»‡n Ã­ch má»Ÿ rá»™ng vÃ  BERT lÃ  mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c cÃ³ nguá»“n gá»‘c tá»« Transformer." />
<meta property="og:description" content="Transformer lÃ  mÃ´ hÃ¬nh seq2seq Ä‘Æ°á»£c Google Brain Ä‘á» xuáº¥t trong má»™t bÃ i bÃ¡o xuáº¥t báº£n vÃ o cuá»‘i nÄƒm 2017. Giá» Ä‘Ã¢y, nÃ³ Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c nhiá»u á»©ng dá»¥ng vÃ  tiá»‡n Ã­ch má»Ÿ rá»™ng vÃ  BERT lÃ  mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c cÃ³ nguá»“n gá»‘c tá»« Transformer." />
<link rel="canonical" href="https://zhaospei.github.io//nlp/2023/10/06/attention-is-all-you-need/" />
<meta property="og:url" content="https://zhaospei.github.io//nlp/2023/10/06/attention-is-all-you-need/" />
<meta property="og:site_name" content="Tuan-Dung Bui" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-10-06T00:00:00+07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Attention is All You Need" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"zhao"},"dateModified":"2023-10-06T00:00:00+07:00","datePublished":"2023-10-06T00:00:00+07:00","description":"Transformer lÃ  mÃ´ hÃ¬nh seq2seq Ä‘Æ°á»£c Google Brain Ä‘á» xuáº¥t trong má»™t bÃ i bÃ¡o xuáº¥t báº£n vÃ o cuá»‘i nÄƒm 2017. Giá» Ä‘Ã¢y, nÃ³ Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c nhiá»u á»©ng dá»¥ng vÃ  tiá»‡n Ã­ch má»Ÿ rá»™ng vÃ  BERT lÃ  mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c cÃ³ nguá»“n gá»‘c tá»« Transformer.","headline":"Attention is All You Need","mainEntityOfPage":{"@type":"WebPage","@id":"https://zhaospei.github.io//nlp/2023/10/06/attention-is-all-you-need/"},"url":"https://zhaospei.github.io//nlp/2023/10/06/attention-is-all-you-need/"}</script>
<!-- End Jekyll SEO tag -->

  </head>
  <body class="bg-gray-100 dark:bg-zinc-900 dark:text-white">
    <div class="bg-gray-100 dark:bg-zinc-900 mx-auto max-w-3xl px-3 sm:px-6 xl:max-w-5xl xl:px-0">
      <div
    class="hidden fixed w-full h-screen inset-0 bg-gray-200 opacity-95 z-50 transition-transform transform ease-in-out duration-300 translate-x-0 mobile-menu dark:bg-zinc-800">
    <button type="button" aria-label="toggle modal"
        class="cursor-pointer fixed right-4 top-4 h-8 w-8 cursor-auto focus:outline-none border-none mobile-menu-btn-close"><svg
            xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="text-gray-900 svg-change-color">
            <path fill-rule="evenodd"
                d="M4.293 4.293a1 1 0 011.414 0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z"
                clip-rule="evenodd"></path>
        </svg></button>
    <nav class="fixed mt-8 h-full">
        
        <div class="px-8 py-4"><a class="text-xl font-bold text-gray-900 dark:text-white" href="/"> Home </a></div>
        
        <div class="px-8 py-4"><a class="text-xl font-bold text-gray-900 dark:text-white" href="/works.html"> Works </a></div>
        
        <div class="px-8 py-4"><a class="text-xl font-bold text-gray-900 dark:text-white" href="/blog.html"> Blog </a></div>
        
        <div class="px-8 py-4"><a class="text-xl font-bold text-gray-900 dark:text-white" href="/assets/BuiTuanDung-Resume-1.pdf"> Resume </a></div>
        
        <div class="px-8 py-4"><a class="text-xl font-bold text-gray-900 dark:text-white" href="/about/"> About </a></div>
        

    </nav>
</div>

<header class="grid justify-center w-full relative pt-4" style="grid-template-columns: 2fr auto 1fr;">
    <a style="" href="/">
        <div class="text-dark dark:text-white flex items-center text-xl font-semibold" style="height: 48px;"> 
            <span class="blog-link">~/</span>
            <div class="Typewriter font-normal ml-1" data-testid="typewriter-wrapper"><span class="Typewriter__wrapper"></span><span class="Typewriter__cursor">|</span></div>
        </div>
    </a>
    <nav
        class="flex items-center justify-between relative max-w-3xl mx-auto text-gray-900">
        
        <a class="font-bold text-gray-800 hidden md:inline-block mx-1 p-1 sm:px-3 rounded-3xl hover:bg-white transition-all dark:text-gray-100 dark:hover:bg-zinc-950"
             href="/"><span
                class="capsize">Home</span></a>
        
        <a class="font-bold text-gray-800 hidden md:inline-block mx-1 p-1 sm:px-3 rounded-3xl hover:bg-white transition-all dark:text-gray-100 dark:hover:bg-zinc-950"
             href="/works.html"><span
                class="capsize">Works</span></a>
        
        <a class="font-bold text-gray-800 hidden md:inline-block mx-1 p-1 sm:px-3 rounded-3xl hover:bg-white transition-all dark:text-gray-100 dark:hover:bg-zinc-950"
             href="/blog.html"><span
                class="capsize">Blog</span></a>
        
        <a class="font-bold text-gray-800 hidden md:inline-block mx-1 p-1 sm:px-3 rounded-3xl hover:bg-white transition-all dark:text-gray-100 dark:hover:bg-zinc-950"
             href="/assets/BuiTuanDung-Resume-1.pdf"><span
                class="capsize">Resume</span></a>
        
        <a class="font-bold text-gray-800 hidden md:inline-block mx-1 p-1 sm:px-3 rounded-3xl hover:bg-white transition-all dark:text-gray-100 dark:hover:bg-zinc-950"
             href="/about/"><span
                class="capsize">About</span></a>
        
        <button
            class="cursor-pointer bg-gray-200 bg-opacity-25	backdrop-blur-sm rounded-lg text-sm border-y border-transparent mobile-menu-btn-open visible md:hidden fixed z-50 right-4 bottom-4 h-8 w-8 cursor-auto focus:outline-none border-none mobile-menu-btn"
            type="button">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="text-gray-900 svg-change-color">
                <path fill-rule="evenodd"
                    d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z"
                    clip-rule="evenodd"></path>
            </svg>
        </button>
    </nav>
    <div class="flex justify-self-end items-center gap-8">
        <label class="relative cursor-pointer">
            <span class="sr-only">
                "change theme to"
                <!-- -->
                "dark"
            </span>
            <div class="themeSwitchToggle w-12 h-6 bg-gray-200 rounded-3xl p-1 ml-4 dark:bg-zinc-800">
                <div class="themeSwitchIcon w-6 h-6 bg-white rounded-full flex items-center justify-center dark:bg-zinc-950" style="transition: transform .2s ease-in-out;">
                    
                </div>
            </div>
        </label>
        <a class="contact-header flex font-semibold items-center gap-2 text-black text-sm dark:text-white" href="/contact">
            Contact
            <svg class="svg-change-color dark:text-white"viewBox="0 0 513 513" xmlns="http://www.w3.org/2000/svg" style="width:1.2rem;">
                <path
                    d="M440 6.5L24 246.4c-34.4 19.9-31.1 70.8 5.7 85.9L144 379.6V464c0 46.4 59.2 65.5 86.6 28.6l43.8-59.1 111.9 46.2c5.9 2.4 12.1 3.6 18.3 3.6 8.2 0 16.3-2.1 23.6-6.2 12.8-7.2 21.6-20 23.9-34.5l59.4-387.2c6.1-40.1-36.9-68.8-71.5-48.9zM192 464v-64.6l36.6 15.1L192 464zm212.6-28.7l-153.8-63.5L391 169.5c10.7-15.5-9.5-33.5-23.7-21.2L155.8 332.6 48 288 464 48l-59.4 387.3z"
                    fill=""></path>
            </svg>
        </a>
    </div>
</header>
      <main class="flex flex-col justify-center py-16">
          <!-- <style>
  .table-of-contents-container li a {
    color: rgb(122, 118, 119);
    font-weight: 500;
  }

  .table-of-contents-container li.active>a {
    color: rgb(175, 165, 248);
  }

  .table-of-contents-container li a:hover {
    filter: brightness(60%);
  }

  .table-of-contents-container {
    font-size: 14px;
  }

  .table-of-contents-container nav {}

  .toc-h1 {
    margin-top: 1em;
  }

  .toc-h2 {
    margin-top: 0.75em;
  }

  .toc-h3 {
    margin-top: 0.5em;
  }


  .toc-h1 {
    font-weight: 600;
  }

  .toc-h2 {
    padding-left: 1em;
    font-weight: 500;
  }

  .toc-h3 {
    padding-left: 2em;
    font-weight: 400;
  }

  .toc-h1:first-child {
    margin-top: 0;
  }


  @media only screen and (max-width:1280px) {
    .table-of-contents-container {
      display: none;
    }
  }
</style> -->

<!-- <style>
    .reading-line {
        position: fixed;
        top: 0;
        left: 0;
        border-bottom: 4px solid #66aacb;
        z-index: 404;
    }
</style>

<div class="reading-line">
</div> -->
<div class="post-container">
  <article class="post-content-container flex flex-col items-start justify-center">
    <header class="flex flex-col gap-12 w-full max-w-full pb-8">
      <div class="flex flex-col justify-center gap-4">
        <h1 class="text-5xl font-extrabold leading-tight"> Attention is All You Need </h1>
        <div class="flex flex-col items-start justify-between mt-2 md:flex-row md:items-center">
          <div class="flex items-center">
            
            
            
            <img class="object-cover w-[40px] h-[40px] rounded-full" src="/assets/media/author/zhao-img.png" alt="Tuan-Dung Bui">
            
            <p class="ml-2 text-gray-700 dark:text-gray-300">
              Tuan-Dung Bui / October 6, 2023
            </p>
            
          </div>
          <p class="mt-2 text-gray-600 min-w-32 md:mt-0 dark:text-gray-300">
            

14 min read

          </p>
        </div>
      </div>
    </header>
    <div class="flex justify-center w-full flex-row-reverse">
      <!-- <aside class="table-of-contents-container ml-16 sticky top-16 mt-16" style="max-height: calc(100vh - 16.5rem);">
        <h2 class="uppercase tracking-widest text-base	font-extrabold mb-6"> Table of contents </h2>
        <nav class="overflow-auto pr-2" style="max-height: calc(100vh - 16.5rem);">
          <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#-self-attention"># Self-Attention</a></li>
<li class="toc-entry toc-h1"><a href="#-encoder"># Encoder</a>
<ul>
<li class="toc-entry toc-h2"><a href="#position-wise-fully-connected-feed-forward-network">position-wise fully connected feed-forward network</a></li>
<li class="toc-entry toc-h2"><a href="#residual-connection">residual connection</a></li>
<li class="toc-entry toc-h2"><a href="#batch-norm-vÃ -layer-norm">Batch Norm vÃ  Layer Norm</a></li>
<li class="toc-entry toc-h2"><a href="#toÃ n-bá»™-kiáº¿n-trÃºc-encoder">ToÃ n bá»™ kiáº¿n trÃºc Encoder</a>
<ul>
<li class="toc-entry toc-h3"><a href="#input--positional-embedding">input &amp; positional embedding</a></li>
<li class="toc-entry toc-h3"><a href="#multi-head-attention">multi-head attention</a></li>
<li class="toc-entry toc-h3"><a href="#add--norm">add &amp; norm</a></li>
<li class="toc-entry toc-h3"><a href="#feed-forward">feed forward</a></li>
<li class="toc-entry toc-h3"><a href="#add--norm-1">add &amp; norm</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#-decoder"># Decoder</a></li>
<li class="toc-entry toc-h1"><a href="#-word-embedding-vÃ -positional-embedding"># Word Embedding vÃ  Positional Embedding</a>
<ul>
<li class="toc-entry toc-h2"><a href="#word-embedding">Word Embedding</a></li>
<li class="toc-entry toc-h2"><a href="#positional-embedding">Positional Embedding</a>
<ul>
<li class="toc-entry toc-h3"><a href="#nhÃºng-vá»‹-trÃ­-tÃ¹y-chinh">NhÃºng vá»‹ trÃ­ tÃ¹y chinh</a></li>
<li class="toc-entry toc-h3"><a href="#nhÃºng-tá»«-vá»‹-trÃ­-lÃ½-tuá»Ÿng">NhÃºng tá»« vá»‹ trÃ­ â€œlÃ½ tuá»Ÿngâ€</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#-q--a"># Q &amp; A</a>
<ul>
<li class="toc-entry toc-h2"><a href="#táº¡i-sao-transformer-cáº§n-multi-head-attention-">Táº¡i sao Transformer cáº§n Multi-head Attention ?</a></li>
<li class="toc-entry toc-h2"><a href="#Æ°u-Ä‘iá»ƒm-cá»§a-transformer-so-vá»›i-rnnlstm-lÃ -gÃ¬-táº¡i-sao">Æ¯u Ä‘iá»ƒm cá»§a Transformer so vá»›i RNN/LSTM lÃ  gÃ¬? Táº¡i sao?</a></li>
<li class="toc-entry toc-h2"><a href="#táº¡i-sao-transformer-cÃ³-thá»ƒ-thay-tháº¿-seq2seq">Táº¡i sao Transformer cÃ³ thá»ƒ thay tháº¿ seq2seq?</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#tham-kháº£o">Tham kháº£o</a></li>
</ul>
        </nav>
      </aside> -->
      <div class="items-center pb-8" style="grid-template-rows: auto 1fr;">
        <div class="thumbnail-image relative not-prose rounded-2xl overflow-hidden">
          
          <img alt="Post image feature" src="/assets/media/feature/transformers.jpg" loading="lazy" class="w-full">
          
        </div>
        <div class="post-content prose">
          <p><code class="language-plaintext highlighter-rouge">Transformer</code> lÃ  mÃ´ hÃ¬nh seq2seq Ä‘Æ°á»£c Google Brain Ä‘á» xuáº¥t trong má»™t bÃ i bÃ¡o xuáº¥t báº£n vÃ o cuá»‘i nÄƒm 2017. Giá» Ä‘Ã¢y, nÃ³ Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c nhiá»u á»©ng dá»¥ng vÃ  tiá»‡n Ã­ch má»Ÿ rá»™ng vÃ  <code class="language-plaintext highlighter-rouge">BERT</code> lÃ  mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÆ°á»›c cÃ³ nguá»“n gá»‘c tá»« Transformer.</p>

<p>Viá»‡c Ä‘Ã o táº¡o <code class="language-plaintext highlighter-rouge">RNN</code> truyá»n thá»‘ng lÃ  ná»‘i tiáº¿p vÃ  nÃ³ pháº£i Ä‘á»£i tá»« hiá»‡n táº¡i Ä‘Æ°á»£c xá»­ lÃ½ trÆ°á»›c khi cÃ³ thá»ƒ xá»­ lÃ½ tá»« tiáº¿p theo. Transformer Ä‘Æ°á»£c huáº¥n luyá»‡n song song, tá»©c lÃ  táº¥t cáº£ cÃ¡c tá»« Ä‘á»u Ä‘Æ°á»£c huáº¥n luyá»‡n cÃ¹ng má»™t lÃºc, Ä‘iÃªu nÃ y lÃ m tÄƒng Ä‘Ã¡ng ká»ƒ hiá»‡u quáº£ tÃ­nh toÃ¡n.</p>

<figure class="image" style="text-align: center;">
    <img src="/assets/media/post/transformer-model-architecture.png" alt="Kiáº¿n trÃºc mÃ´ hÃ¬nh Transformer" style="margin: auto;" />
    <figcaption style="font-style: italic;">Kiáº¿n trÃºc mÃ´ hÃ¬nh Transformer</figcaption>
</figure>

<h1 id="-self-attention"># Self-Attention</h1>
<p><code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> lÃ  tÃ­ch cháº¥m chuáº©n hÃ³a Attention, chi tiáº¿t cá»¥ thá»ƒ Ä‘Æ°á»£c thá»ƒ hiá»‡n trong hÃ¬nh.</p>

<figure class="image" style="text-align: center;">
    <img src="/assets/media/post/attention.png" alt="" style="margin: auto;" />
    <figcaption style="font-style: italic;"></figcaption>
</figure>

\[Attention(Q,K,V)=softmax(\dfrac{QK^T}{\sqrt{d_k}})V\]

<p>Sá»± chÃº Ã½ cá»§a nhiá»u Ä‘áº§u vÃ o sá»­ dá»¥ng nhiá»u bá»™ trá»ng sá»‘ (<code class="language-plaintext highlighter-rouge">weights</code>) (\(W_q,W_k,W_v\)), ghÃ©p láº¡i cho ra káº¿t quáº£ cuá»‘i cÃ¹ng.</p>

\[MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O\]

<p>trong Ä‘Ã³</p>

\[head_i=Attention(QW^Q_i,KW^K_i,VW^V_i)\]

<p>Trong Ä‘Ã³ \(h = 8\), \(d_q=d_k=d_v=d_{model}/4=64\).</p>

<h1 id="-encoder"># Encoder</h1>
<p>Encoder Ä‘Æ°á»£c xáº¿p chá»“ng lÃªn nhau bá»Ÿi sÃ¡u lá»›p giá»‘ng há»‡t nhau, má»—i lá»›p bao gá»“m hai lá»›p con - cÆ¡ cháº¿ tá»± chÃº Ã½ nhiá»u Ä‘áº§u (<code class="language-plaintext highlighter-rouge">multi-head self-attention mechanism</code>) vÃ  máº¡ng nÆ¡ ron vá»‹ trÃ­ chuyá»ƒn tiáº¿p Ä‘Æ°á»£c káº¿t ná»‘i Ä‘áº§y Ä‘á»§ (<code class="language-plaintext highlighter-rouge">position-wise fully connected feed-forward network</code>). Má»—i lá»›p con sá»­ dá»¥ng cÃ¡c káº¿t ná»‘i dÆ° (<code class="language-plaintext highlighter-rouge">residual connection</code>) vÃ  lá»›p chuáº©n hÃ³a (<code class="language-plaintext highlighter-rouge">layer normalization</code>). KÃ­ch thÆ°á»›c Ä‘áº§u ra cá»§a cÃ¡c lá»›p con lÃ  \(d_{model} = 512\).</p>

<p>Äáº§u ra cá»§a lá»›p con cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng:</p>

\[LayerNorm(x+Sublayer(x))\]

<h2 id="position-wise-fully-connected-feed-forward-network">position-wise fully connected feed-forward network</h2>
<p>Máº¡ng nÆ¡-ron chuyá»ƒn tiáº¿p Ä‘Æ°á»£c káº¿t ná»‘i Ä‘áº§y Ä‘á»§ (<code class="language-plaintext highlighter-rouge">position-wise fully connected feed-forward network</code>) bao gá»“m hai phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh vá»›i kÃ­ch hoáº¡t <code class="language-plaintext highlighter-rouge">ReLU</code> á»Ÿ giá»¯a.</p>

\[FFN(x)=ReLU(xW_1+b_1)W_2+b_2\]

<p>KÃ­ch thÆ°á»›c lá»›p bÃªn trong (inner-layer) lÃ  2048.</p>

<h2 id="residual-connection">residual connection</h2>
<p>Máº¡ng dÆ° (<code class="language-plaintext highlighter-rouge">Residual Network</code>), cÃ¡c káº¿t ná»‘i táº¯t cÃ³ kháº£ nÄƒng bá» qua má»™t hoáº·c nhiá»u lá»›p, do sá»± tá»“n táº¡i cá»§a káº¿t ná»‘i táº¯t nÃªn hiá»‡u suáº¥t cá»§a máº¡ng Ã­t khÃ´ng kÃ©m hÆ¡n so vá»›i cÃ¡c máº¡ng nÃ´ng. PhÆ°Æ¡ng phÃ¡p nÃ y giáº£i quyáº¿t váº¥n Ä‘á» suy thoÃ¡i do cÃ¡c lá»›p cháº­p xáº¿p chá»“ng lÃªn nhau gÃ¢y ra, sá»‘ lÆ°á»£ng lá»›p cá»§a máº¡ng nÆ¡-ron tÃ­ch cháº­p Ä‘Ã£ Ä‘Æ°á»£c tÄƒng lÃªn ráº¥t nhiá»u lÃªn hÃ ng trÄƒm lá»›p, vÃ  cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u suáº¥t cá»§a máº¡ng tháº§n kinh tÃ­ch cháº­p (<code class="language-plaintext highlighter-rouge">resnet</code>).</p>

<figure class="image" style="text-align: center;">
    <img src="/assets/media/post/resnet.png" alt="" style="margin: auto;" />
    <figcaption style="font-style: italic;"></figcaption>
</figure>

<h2 id="batch-norm-vÃ -layer-norm">Batch Norm vÃ  Layer Norm</h2>

<figure class="image" style="text-align: center;">
    <img src="/assets/media/post/normalization.png" alt="" style="margin: auto;" />
    <figcaption style="font-style: italic;"></figcaption>
</figure>

<p>Äáº·t kÃ­ch thÆ°á»›c hÃ¬nh áº£nh Ä‘áº§u vÃ o lÃ  \([N, C, H, W]\):</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">Batch Norm</code>, chuáº©n hÃ³a theo tá»«ng batch NHW, lÃ  Ä‘á»ƒ chuáº©n hÃ³a Ä‘áº§u vÃ o tá»«ng kÃªnh Ä‘Æ¡n, Ä‘á»u nÃ y khÃ´ng hiá»‡u quáº£ Ä‘á»‘i vá»›i <code class="language-plaintext highlighter-rouge">batch-size</code> nhá».</li>
  <li><code class="language-plaintext highlighter-rouge">Layer Norm</code>, chuáº©n hÃ³a theo tá»«ng layer CHW, lÃ  Ä‘á»ƒ chuáº©n hÃ³a Ä‘áº§u vÃ o á»Ÿ má»—i Ä‘á»™ sÃ¢u, chá»§ yáº¿u cÃ³ tÃ¡c dá»¥ng rÃµ rÃ ng trÃªn RNN.</li>
</ul>

<p>Sá»± hiá»ƒu biáº¿t cÃ¡ nhÃ¢n:</p>

<ul>
  <li>DÃ nh cho CNN, náº¿u háº¡t nhÃ¢n tÃ­ch cháº­p quÃ©t hÃ¬nh áº£nh Ä‘áº§u vÃ o, nÃ³ Ä‘Æ°á»£c tÃ­nh lÃ  thao tÃ¡c tÃ­ch cháº­p, cáº§n cÃ³ tá»•ng thao tÃ¡c batchsize. Do Ä‘Ã³, chuáº©n hÃ³a cáº§n Ä‘Æ°á»£c thá»±c hiá»‡n theo batch.</li>
  <li>DÃ nh cho RNN, batchsize thÆ°á»ng lÃ  1, sá»‘ vÃ²ng láº·p lÃ  sá»‘ Ä‘á»™ dÃ i Ä‘áº§u vÃ o (sá»‘ channel). Do Ä‘Ã³, chuáº©n hÃ³a cáº§n Ä‘Æ°á»£c thá»±c hiá»‡n theo channel.</li>
</ul>

<h2 id="toÃ n-bá»™-kiáº¿n-trÃºc-encoder">ToÃ n bá»™ kiáº¿n trÃºc Encoder</h2>
<h3 id="input--positional-embedding">input &amp; positional embedding</h3>

\[X=Embedding-Lookup(X)+Positional-Encoding\]

<h3 id="multi-head-attention">multi-head attention</h3>

\[Q=Linear_q(X)=XW_q\]

\[K=Linear_q(X)=XW_k\]

\[V=Linear_v(X)=XW_v\]

\[X_{attention}=Self-Attention(Q,K,V)\]

<h3 id="add--norm">add &amp; norm</h3>

\[X_{attention}=LayerNorm(X+X_{attention})\]

<h3 id="feed-forward">feed forward</h3>

\[X_{hidden}=Linear(ReLU(Linear(X_{attention})))\]

<h3 id="add--norm-1">add &amp; norm</h3>

\[X_{hidden}=LayerNorm(X_{hidden}+X_{attention})\]

<p><code class="language-plaintext highlighter-rouge">multi-head attention</code> trong <code class="language-plaintext highlighter-rouge">Encoder</code> lÃ  má»™t cÆ¡ cháº¿ tá»± chÃº Ã½ (<code class="language-plaintext highlighter-rouge">self-attention mechanism</code>). \(k\), \(q\) vÃ  \(v\) trong cÆ¡ cháº¿ tá»± chÃº Ã½ Ä‘á»u xuáº¥t phÃ¡t tá»« cÃ¹ng má»™t vá»‹ trÃ­, má»—i lá»›p cá»§a Encoder cÃ³ thá»ƒ nháº­n Ä‘Æ°á»£c táº¥t cáº£ vá»‹ trÃ­ cá»§a lá»›p trÆ°á»›c.</p>

<h1 id="-decoder"># Decoder</h1>
<p>Decoder bao gá»“m sÃ¡u lá»›p giá»‘ng há»‡t xáº¿p chá»“ng lÃªn nhau; trong Multi-head Attention, \(q\) Ä‘Æ°á»£c Ä‘áº¿n tá»« lá»›p trÆ°á»›c Ä‘Ã³ cá»§a Decoder, k vÃ  v Ä‘áº¿n tá»« Ä‘áº§u ra cá»§a Encoder. Äiá»u cho phÃ©p má»—i vá»‹ trÃ­ trong Decoder nháº­n biáº¿t Ä‘Æ°á»£c táº¥t cáº£ cÃ¡c vá»‹ trÃ­ cá»§a chuá»—i Ä‘áº§u vÃ o.</p>

<p>NgoÃ i hai lá»›p con trong Encoder, Decoder thÃªm má»™t lá»›p con má»›i xá»­ lÃ½ Ä‘áº§u ra cá»§a Encoder - <code class="language-plaintext highlighter-rouge">masked multi-head self-attention mechanism</code>. Encoder trong seq2seq truyá»n thá»‘ng sá»­ dá»¥ng mÃ´ hÃ¬nh RNN, vÃ¬ váº­y náº¿u cÃ¡c tá»« táº¡i thá»i Ä‘iá»ƒm t Ä‘Æ°á»£c nháº­p vÃ o trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n thÃ¬ mÃ´ hÃ¬nh sáº½ khÃ´ng thá»ƒ nhÃ¬n tháº¥y cÃ¡c tá»« trÆ°á»›c Ä‘Ã³ vÃ o cÃ¡c thá»i Ä‘iá»ƒm trong tÆ°Æ¡ng lai, bá»Ÿi vÃ¬ RNN hoáº¡t Ä‘á»™ng theo thá»i gian vÃ  chá»‰ khi thao tÃ¡c táº¡i thá»i Ä‘iá»ƒm t hoÃ n thÃ nh, chá»‰ khi Ä‘Ã³ ta má»›i cÃ³ thá»ƒ nhÃ¬n tháº¥y cÃ¡c tá»« táº¡i thá»i Ä‘iá»ƒm t + 1. VÃ  Transformer Decoder Ä‘Ã£ khÃ´ng sá»­ dá»¥ng RNN, thay Ä‘á»•i sang Self-Attention, Ä‘iá»u nÃ y táº¡o ra má»™t váº¥n Ä‘á», trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, toÃ n bá»™ ground truth Ä‘Ã£ Ä‘Æ°á»£c hiá»ƒn thá»‹ vá»›i Decoder, Ä‘iá»u nÃ y rÃµ rÃ ng lÃ  sai, chÃºng ta cáº§n pháº£i thá»±c hiá»‡n má»™t sá»‘ xá»­ lÃ½ trÃªn Ä‘áº§u vÃ o cá»§a Decoder, quÃ¡ trÃ¬nh nÃ y Ä‘Æ°á»£c gá»i lÃ  <code class="language-plaintext highlighter-rouge">Mask</code> - Äáº·t táº¥t cáº£ cÃ¡c giÃ¡ trá»‹ sau postion thÃ nh \(-\infty\) trÆ°á»›c khi vÃ o softmax.</p>

<p>VÃ­ dá»¥, ground truth cá»§a Decoder lÃ  â€œ&lt;start&gt; I am fineâ€, chÃºng ta cho cÃ¢u nÃ y vÃ o bá»™ Decoder, sau khi Word Embedding vÃ  Positional Encoding, thá»±c hiá»‡n phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh báº­c 3 trÃªn ma tráº­n thu Ä‘Æ°á»£c \((W_Q,W_K,W_V)\) Sau Ä‘Ã³ thá»±c hiá»‡n self-attention, trÆ°á»›c tiÃªn, nháº­n Scaled Scores thÃ´ng qua \(\dfrac{QÃ—K^T}{\sqrt{d_k}}\), bÆ°á»›c tiáº¿p theo ráº¥t quan trá»ng, chÃºng ta cáº§n mask theo Scaled Scores, vÃ­ dá»¥, khi nháº­p â€œIâ€, hiá»‡n táº¡i mÃ´ hÃ¬nh chá»‰ biáº¿t thÃ´ng tin cá»§a táº¥t cáº£ cÃ¡c tá»« trÆ°á»›c Ä‘Ã³ cá»§a â€œIâ€, tá»©c thÃ´ng tin cá»§a â€œ&lt;start&gt;â€ vÃ  â€œIâ€, khÃ´ng Ä‘Æ°á»£c phÃ©p biáº¿t Ä‘Æ°á»£c thÃ´ng tin cá»§a cÃ¡c tá»« sau â€œIâ€. LÃ½ do ráº¥t Ä‘Æ¡n giáº£n, khi dá»± Ä‘oÃ¡n lÃ  chÃºng ta dá»± Ä‘oÃ¡n theo thá»© tá»± tá»«ng chá»¯, lÃ m sao cÃ³ thá»ƒ biáº¿t Ä‘Æ°á»£c thÃ´ng tin cá»§a nhá»¯ng tá»« sau trÆ°á»›c khi dá»± Ä‘oÃ¡n xong tá»« nÃ y? Mask ráº¥t Ä‘Æ¡n giáº£n, Ä‘áº§u tiÃªn táº¡o má»™t ma tráº­n cÃ³ tam giÃ¡c hoÃ n toÃ n phÃ­a dÆ°á»›i báº±ng 0 vÃ  tam giÃ¡c hoÃ n tÃ²an phÃ­a trÃªn báº±ng Ã¢m vÃ´ cÃ¹ng, sau Ä‘Ã³ chá»‰ cáº§n thÃªm nÃ³ vÃ o Scaled Scores.</p>

<h1 id="-word-embedding-vÃ -positional-embedding"># Word Embedding vÃ  Positional Embedding</h1>
<h2 id="word-embedding">Word Embedding</h2>
<p>Pháº§n nhÃºng tá»« sá»­ dá»¥ng nhÃºng tá»« cÃ³ thá»ƒ há»c Ä‘Æ°á»£c, kÃ­ch thÆ°á»›c cá»§a nÃ³ lÃ  \(d_{model}\).
HÃ¬nh thá»©c mÃ£ hÃ³a <code class="language-plaintext highlighter-rouge">One-hot</code> ngáº¯n gá»n, nhÆ°ng quÃ¡ thÆ°a thá»›t, nÃ³ khÃ´ng pháº£n Ã¡nh sá»± giá»‘ng nhau vá» nghÄ©a cá»§a tá»«. VÃ¬ váº­y hÃ£y sá»­ dá»¥ng <code class="language-plaintext highlighter-rouge">the Skip-Gram Model</code> hoáº·c <code class="language-plaintext highlighter-rouge">continuous bag of words model</code> hoáº·c cÃ¡c nhÃºng tá»« khÃ¡c cÃ³ thá»ƒ há»c Ä‘Æ°á»£c khÃ¡c.</p>

<h2 id="positional-embedding">Positional Embedding</h2>
<p>Bá»Ÿi vÃ¬ mÃ´ hÃ¬nh khÃ´ng bao gá»“m cÃ¡c cáº¥u trÃºc tuáº§n hoÃ n, vÃ¬ váº­y náº¯m báº¯t Ä‘Æ°á»£c cÃ¡c thÃ´ng tin thá»© tá»± tuáº§n tá»±, vÃ­ dá»¥ náº¿u \(K\) vÃ  \(V\) Ä‘Æ°á»£c xÃ³a trá»™n theo tá»«ng hÃ ng thÃ¬ káº¿t quáº£ sau Attention sáº½ giá»‘ng nhau. Tuy nhiÃªn, thÃ´ng tin tuáº§n tá»± ráº¥t quan trá»ng vÃ  thá»ƒ hiá»‡n cáº¥u trÃºc toÃ n cáº§u, do Ä‘Ã³ thÃ´ng tin position tuyá»‡t Ä‘á»‘i vÃ  tÆ°Æ¡ng Ä‘á»‘i cá»§a token tuáº§n tá»± pháº£i Ä‘Æ°á»£c sá»­ dá»¥ng.</p>
<h3 id="nhÃºng-vá»‹-trÃ­-tÃ¹y-chinh">NhÃºng vá»‹ trÃ­ tÃ¹y chinh</h3>
<p>Má»™t Ã½ tÆ°á»Ÿng lÃ  láº¥y má»™t sá»‘ trong khoáº£ng \([0, 1]\) vÃ  gÃ¡n nÃ³ cho má»—i tá»«, trong Ä‘Ã³ 0 Ä‘Æ°á»£c trao cho tá»« Ä‘áº§u tiÃªn, 1 cho tá»« cuá»‘i cÃ¹ng, cÃ´ng thá»©c cá»¥ thá»ƒ lÃ  \(PE=\dfrac{pos}{Tâˆ’1}\). Váº¥n Ä‘á» cá»§a viá»‡c gÃ¡n theo cÃ´ng thá»©c nÃ y lÃ  nÃ³ bá»‹ phá»¥ thuá»™c vÃ  kÃ­ch thÆ°á»›c cá»§a vÄƒn báº£n. Tá»©c
lÃ  vÄƒn báº£n cÃ³ sá»‘ kÃ­ tá»± lÃ  30. Khi Ä‘Ã³ theo cÃ´ng thá»©c trÃªn, thÃ¬ khoáº£ng cÃ¡ch giá»¯a hai tá»« sáº½ lÃ  0.0333. Khi vÄƒn báº£n khÃ¡c cÃ³ sá»‘ lÆ°á»£ng kÃ­ tá»« &lt; 30, thÃ¬ con sá»‘ 0.0333 váº«n mÃ´ táº£ Ä‘Ãºng vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i giá»¯a chÃºng, tuy nhiÃªn vá»›i vÄƒn báº£n &gt; 30, vÃ­ dá»¥ 90 thÃ¬ 0.0333 Ä‘ang gá»™p khoáº£ng cÃ¡ch thá»±c táº¿ Ä‘ang Ä‘Æ°á»£c phÃ¢n tÃ¡ch bá»Ÿi hai kÃ½ tá»±. Äiá»u nÃ y rÃµ rÃ ng lÃ  khÃ´ng phÃ¹ há»£p, vÃ¬ sá»± khÃ¡c biá»‡t giá»‘ng nhau khÃ´ng cÃ³ nghÄ©a lÃ  giá»‘ng nhau trong cÃ¡c cÃ¢u khÃ¡c nhau.</p>

<p>Má»™t Ã½ tÆ°á»Ÿng khÃ¡c lÃ  gáº¯n tuyáº¿n tÃ­nh má»—i bÆ°á»›c theo thá»i gian, nghÄ©a lÃ  tá»« Ä‘áº§u tiÃªn Ä‘Æ°á»£c gÃ¡n lÃ  1, tá»« thá»© hai Ä‘Æ°á»£c gÃ¡n lÃ  2, â€¦ PhÆ°Æ¡ng phÃ¡p nÃ y cÅ©ng cÃ³ nhá»¯ng váº¥n Ä‘á» lá»›n: 1. NÃ³ lá»›n hÆ¡n giÃ¡ trá»‹ nhÃºng tá»« thÃ´ng tá»«, cÃ³ thá»ƒ gÃ¢y nhiá»…u cho mÃ´ hÃ¬nh; 2. KÃ½ tá»± cuá»‘i cÃ¹ng lá»›n hÆ¡n nhiá»u kÃ½ tá»± Ä‘áº§u tiÃªn, sau khi há»£p nháº¥t vá»›i cÃ¡c tá»« nhÃºng, giÃ¡ trá»‹ cá»§a cÃ¡c Ä‘áº·c trÆ°ng sáº½ bá»‹ sai lá»‡ch.</p>

<h3 id="nhÃºng-tá»«-vá»‹-trÃ­-lÃ½-tuá»Ÿng">NhÃºng tá»« vá»‹ trÃ­ â€œlÃ½ tuá»Ÿngâ€</h3>
<p>Má»™t lÃ½ tÆ°á»Ÿng lÃ  thiáº¿t káº¿ nhÃºng vá»‹ trÃ­ pháº£i Ä‘Ã¡p á»©ng nhá»¯ng tiÃªu chÃ­ sau:</p>
<ul>
  <li>NÃ³ sáº½ xuáº¥t ra mÃ£ hÃ³a duy nháº¥t cho má»—i tá»«.</li>
  <li>Sá»± khÃ¡c biá»‡t giá»¯a hai tá»« pháº£i nháº¥t quÃ¡n giá»¯a cÃ¡c cÃ¢u cÃ³ Ä‘á»™ dÃ i khÃ¡c nhau.</li>
  <li>GiÃ¡ trá»‹ cá»§a nÃ³ pháº£i Ä‘Æ°á»£c giá»›i háº¡n.</li>
</ul>

<p>Do Ä‘Ã³ viá»‡c nhÃºng vá»‹ trÃ­ sin vÃ  cosin Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng cho Transformer.</p>

<p>BÃ¢y giá» hÃ£y Ä‘á»‹nh nghÄ©a láº¡i Positional Embedding, kÃ­ch thÆ°á»›c cá»§a viá»‡c nhÃºng vá»‹ trÃ­ lÃ  <code class="language-plaintext highlighter-rouge">[max_sequence_length, embedding_dimension]</code>, kÃ­ch thÆ°á»›c cá»§a pháº§n nhÃºng vá»‹ trÃ­ giá»‘ng vá»›i kÃ­ch thÆ°á»›c cá»§a vector tá»«, Ä‘á»u báº±ng <code class="language-plaintext highlighter-rouge">embedding_dimension</code>. <code class="language-plaintext highlighter-rouge">max_sequence_length</code> lÃ  má»™t hyperparameter, Ä‘á» cáº­p Ä‘áº¿n sá»‘ lÆ°á»£ng tá»‘i Ä‘a mÃ  má»™t cÃ¢u bao gá»“m.</p>

<p>KÃ­ch thÆ°á»›c cá»§a viá»‡c nhÃºng vá»‹ trÃ­ cÅ©ng giá»‘ng nhÆ° kÃ­ch thÆ°á»›c cá»§a viá»‡c nhÃºng tá»«, cÃ¹ng lÃ  \(d_{model}\). CÃ´ng thÆ°á»›c tÃ­nh toÃ¡n cá»§a nÃ³ lÃ :</p>

\[PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})\]

\[PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})\]

<p>Trong Ä‘Ã³, \(pos\) Ä‘áº¡i diá»‡n cho chá»‰ má»¥c vá»‹ trÃ­, \(i\) Ä‘áº¡i diá»‡n cho chá»‰ sá»‘ chiá»u. NghÄ©a lÃ  má»—i chiá»u \(i\) cá»§a positional embedding pos tÆ°Æ¡ng á»©ng vá»›i má»™t sÃ³ng sin.</p>

<p>Trong hÃ¬nh dÆ°á»›i nÃ y minh há»a cho cÃ¡ch tÃ­nh position embedding cá»§a tÃ¡c giáº£ vá»›i sá»‘ chiá»u lÃ  6. GiÃ¡ trá»‹ cá»§a cÃ¡c vector táº¡i má»—i vá»‹ trÃ­ Ä‘Æ°á»£c tÃ­nh toÃ¡n theo cÃ´ng thá»©c á»Ÿ hÃ¬nh dÆ°á»›i.</p>
<figure class="image" style="text-align: center;">
    <img src="/assets/media/post/pe.png" alt="" style="margin: auto;" />
    <figcaption style="font-style: italic;"></figcaption>
</figure>

<p>Báº£n thÃ¢n viá»‡c nhÃºng vá»‹ trÃ­ lÃ  má»™t thÃ´ng tin vá»‹ trÃ­ tuyá»‡t Ä‘á»‘i, nhÆ°ng trong ngÃ´n ngá»¯, vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i cÅ©ng ráº¥t quan trá»ng, bá»Ÿi vÃ¬</p>

\[sin(\alpha+\beta)=sin\alpha cos\beta+cos\alpha sin\beta\cos(\alpha+\beta)=cos\alpha cos\beta-sin\alpha sin\beta\]

<p>cho tháº¥y vector táº¡i vá»‹ trÃ­ \(p + k\) cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh cá»§a vectÆ¡ táº¡i vá»‹ trÃ­ \(p\), Ä‘iá»u nÃ y cung cáº¥p kháº£ nÄƒng thá»ƒ hiá»‡n thÃ´ng tin vá»‹ trÃ­ tÆ°Æ¡ng Ä‘á»‘i. PhiÃªn báº£n hÃ¬nh sin cÅ©ng cho phÃ©p mÃ´ hÃ¬nh ngoáº¡i suy vá»›i Ä‘á»™ dÃ i chuá»—i dÃ i hÆ¡n so vá»›i Ä‘á»™ dÃ i chuá»—i gáº·p pháº£i trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n.</p>

<h1 id="-q--a"># Q &amp; A</h1>
<h2 id="táº¡i-sao-transformer-cáº§n-multi-head-attention-">Táº¡i sao Transformer cáº§n Multi-head Attention ?</h2>
<p>BÃ i bÃ¡o Ä‘á» cáº­p lÃ½ do viá»‡c tiáº¿n hÃ nh Multi-head Attention lÃ  Ä‘á»ƒ chia mÃ´ hÃ¬nh thÃ nh nhiá»u Ä‘áº§u Ä‘á»ƒ táº¡o thÃ nh nhiá»u khÃ´ng gian con, cho phÃ©p mÃ´ hÃ¬nh chÃº Ã½ Ä‘áº¿n cÃ¡c khÃ­a cáº¡nh khÃ¡c nhau cá»§a thÃ´ng tin vÃ  cuá»‘i cÃ¹ng tá»•ng há»£p thÃ´ng tin tá»« táº¥t cáº£ cÃ¡c khÃ­a cáº¡nh. TrÃªn thá»±c táº¿, cÃ³ thá»ƒ hÃ¬nh dung báº±ng trá»±c giÃ¡c ráº±ng náº¿u báº¡n tá»± thiáº¿t káº¿ má»™t mÃ´ hÃ¬nh nhÆ° váº­y, attention sáº½ khÃ´ng chá»‰ Ä‘Æ°á»£c thá»±c hiá»‡n má»™t láº§n, káº¿t quáº£ tá»•ng há»£p cá»§a nhiá»u láº§n chÃº Ã½ Ã­t nháº¥t cÃ³ thá»ƒ nÃ¢ng cao mÃ´ hÃ¬nh vÃ  cÅ©ng cÃ³ thá»ƒ Ä‘Æ°á»£c so sÃ¡nh vá»›i vai trÃ² cá»§a viá»‡c sá»­ dá»¥ng nhiá»u tÃ­ch cháº­p cÃ¹ng lÃºc trong CNN, theo trá»±c giÃ¡c, sá»± chÃº Ã½ cá»§a nhiá»u ngÆ°á»i Ä‘á»©ng Ä‘áº§u giÃºp máº¡ng náº¯m báº¯t Ä‘Æ°á»£c cÃ¡c tÃ­nh nÄƒng/ thÃ´ng tin phong phÃº hÆ¡n.</p>
<h2 id="Æ°u-Ä‘iá»ƒm-cá»§a-transformer-so-vá»›i-rnnlstm-lÃ -gÃ¬-táº¡i-sao">Æ¯u Ä‘iá»ƒm cá»§a Transformer so vá»›i RNN/LSTM lÃ  gÃ¬? Táº¡i sao?</h2>
<ol>
  <li>CÃ¡c mÃ´ hÃ¬nh RNN khÃ´ng thá»ƒ tÃ­nh toÃ¡n song song vÃ¬ viá»‡c tÃ­nh toÃ¡n táº¡i thá»i Ä‘iá»ƒm T phá»¥ thuá»™c vÃ o káº¿t quáº£ tÃ­nh toÃ¡n cá»§a lá»›p áº©n táº¡i thá»i Ä‘iá»ƒm T - 1, cÃ²n viá»‡c tÃ­nh toÃ¡n táº¡i thá»i Ä‘iá»ƒm T - 1 láº¡i phá»¥ thuá»™c tÃ­nh toÃ¡n cá»§a lá»›p áº©n táº¡i thá»i Ä‘iá»ƒm T - 2.</li>
  <li>Kháº£ nÄƒng trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng cá»§a Transformer tá»‘t hÆ¡n so vá»›i cÃ¡c mÃ´ hÃ¬nh RNN.</li>
</ol>

<h2 id="táº¡i-sao-transformer-cÃ³-thá»ƒ-thay-tháº¿-seq2seq">Táº¡i sao Transformer cÃ³ thá»ƒ thay tháº¿ seq2seq?</h2>
<p>Tá»« thay tháº¿ á»Ÿ Ä‘Ã¢y hÆ¡i khÃ´ng phÃ¹ há»£p, seq2seq tuy cÅ© nhÆ°ng váº«n cÃ³ chá»— Ä‘á»©ng, váº¥n Ä‘á» lá»›n nháº¥t cá»§a seq2seq lÃ  á»Ÿ chá»— <strong>NÃ©n thÃ´ng tin á»Ÿ phÃ­a Encoder thÃ nh má»™t vector cÃ³ Ä‘á»™ dÃ i cá»‘ Ä‘á»‹nh</strong> vÃ  sá»­ dá»¥ng nÃ³ lÃ m Ä‘áº§u vÃ o cá»§a tráº¡ng thÃ¡i Ä‘áº§u tiÃªn á»Ÿ phÃ­a Decoder, Ä‘á»ƒ dá»± Ä‘oÃ¡n tráº¡ng thÃ¡i áº©n cá»§a tá»« Ä‘áº§u tiÃªn (mÃ£ thÃ´ng bÃ¡o) á»Ÿ phÃ­a Decoder. Khi chuá»—i Ä‘áº§u vÃ o tÆ°Æ¡ng Ä‘á»‘i dÃ i, Ä‘iá»u nÃ y rÃµ rÃ ng sáº½ máº¥t ráº¥t nhiá»u thÃ´ng tin á»Ÿ phÃ­a Encoder vÃ  vector cá»‘ Ä‘á»‹nh sáº½ Ä‘Æ°á»£c gá»­i Ä‘áº¿n phÃ­a Decoder cÃ¹ng má»™t lÃºc, <strong>bÃªn Decoder khÃ´ng thá»ƒ chÃº Ã½ Ä‘áº¿n thÃ´ng tin mÃ  nÃ³ muá»‘n chÃº Ã½</strong>. MÃ´ hinh transformer khÃ´ng chá»‰ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hai khuyáº¿t Ä‘iá»ƒm nÃ y cá»§a mÃ´ hÃ¬nh seq2seq (MÃ´-Ä‘un attention tÆ°Æ¡ng tÃ¡c nhiá»u Ä‘áº§u), vÃ  cÅ©ng giá»›i thiá»‡u mÃ´-Ä‘un self-attention, trÆ°á»›c tiÃªn hÃ£y Ä‘á»ƒ trÃ¬nh tá»± nguá»“n vÃ  trÃ¬nh tá»± Ä‘Ã­ch Ä‘Æ°á»£c â€œtá»± liÃªn káº¿tâ€, trong trÆ°á»ng há»£p nÃ y, thÃ´ng tin chá»©a trong embedding cá»§a trÃ¬nh tá»± nguá»“n vÃ  trÃ¬nh tá»± Ä‘Ã­ch sáº½ phong phÃº hÆ¡n vÃ  lá»›p FFN tiáº¿p theo cÅ©ng nÃ¢ng cao kháº£ nÄƒng biá»ƒu Ä‘áº¡t cá»§a mÃ´ hÃ¬nh, vÃ  tÃ­nh toÃ¡n song song cá»§a Transfomer vÆ°á»£t xa cÃ¡c model seq2seq.</p>

<h1 id="tham-kháº£o">Tham kháº£o</h1>

<p>[1] <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></p>

        </div>

        <div class="ml-8 mt-8 mb-8 font-normal">
          <b class="font-semibold text-lg"> Tags: </b>
          
          <a class="text-sm	inline-block px-4 py-1 mx-2 my-2 border border-solid rounded-lg bg-gray-100 border-slate-400 dark:text-white dark:bg-zinc-800"
            href="/tag/nlp"> nlp </a>
          
          <a class="text-sm	inline-block px-4 py-1 mx-2 my-2 border border-solid rounded-lg bg-gray-100 border-slate-400 dark:text-white dark:bg-zinc-800"
            href="/tag/paper"> paper </a>
          
          <a class="text-sm	inline-block px-4 py-1 mx-2 my-2 border border-solid rounded-lg bg-gray-100 border-slate-400 dark:text-white dark:bg-zinc-800"
            href="/tag/model"> model </a>
          
        </div>

        <div class="flex justify-between items-center w-full flex-wrap gap-4 pb-4"><a class="" rel="noreferrer noopener"
            target="_blank" href="https://github.com/zhaospei/zhaospei.github.io/tree/main/_posts/2023-10-06-attention-is-all-you-need.md"><span
              class="font-normal link-hover">Edit on Github</span><span class="inline-block w-3 h-3 ml-2 mt-1 mr-4"><svg
                class="block w-full h-full" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
                <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2" stroke-linecap="round"
                  stroke-linejoin="round"></path>
              </svg></span></a>
          <div class="flex justify-between items-center flex-wrap gap-8"><a class="cursor-pointer"
              rel="noreferrer noopener" target="_blank"
              onclick="window.open('ttps://twitter.com/intent/tweet?text=https://zhaospei.github.io/');"><span
                class="font-normal link-hover">Tweet this article</span><span
                class="inline-block w-3 h-3 ml-2 mt-1 mr-4"><svg class="block w-full h-full" viewBox="0 0 14 14"
                  fill="none" xmlns="http://www.w3.org/2000/svg">
                  <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2" stroke-linecap="round"
                    stroke-linejoin="round"></path>
                </svg></span></a><a class="cursor-pointer" rel="noreferrer noopener" target="_blank"
              onclick="window.open('http://www.facebook.com/share.php?u=https://zhaospei.github.io/');"><span
                class="font-normal link-hover">Share on Facebook</span><span
                class="inline-block w-3 h-3 ml-2 mt-1 mr-4"><svg class="block w-full h-full" viewBox="0 0 14 14"
                  fill="none" xmlns="http://www.w3.org/2000/svg">
                  <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2" stroke-linecap="round"
                    stroke-linejoin="round"></path>
                </svg></span></a></div>
          <div>
            <div
              class="flex justify-between items-start w-full gap-16 flex-row flew-nowrap pt-16 pl-2 pr-2 border-gray-300 dark:boder-gray-900 border-0 border-t border-solid">
              <div class="relative w-36 h-36">
                <div class="relative overflow-hidden h-full rounded-full">
                  <div class="">
                    <div class="skeleton_skeleton__w5sWr" style="max-width: 100%; height: 100%;"></div>
                  </div>
                  <div class="rounded-full" data-loaded="true"><img alt="Bartosz Zagrodzki" loading="lazy"
                      decoding="async" data-nimg="fill" sizes="100vw" src="/assets/media/author/zhao-img.png"
                      style="position: absolute; height: 100%; width: 100%; inset: 0px; object-fit: cover; object-position: center center; color: transparent;">
                  </div>
                </div>
              </div>
              <div class="" style="flex: 1 1">
                <div class="font-bold text-2xl mb-4">Written by Tuan-Dung Bui</div>
                <p class="text-gray-700 dark:text-gray-200">Tuan-Dung Bui is a blogger, software engineer and the main
                  coordinator of this blog, he has lots of ideas and won't hesitate to use them! He lives in Vietnam.
                </p>
                <div class="mt-4"><a class="" rel="noreferrer noopener" target="_blank"
                    href="/about"><span class="font-normal link-hover">Learn more about
                      Tuan-Dung Bui</span><span class="inline-block w-3 h-3 ml-2 mt-1 mr-4"><svg viewBox="0 0 14 14"
                        fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2" stroke-linecap="round"
                          stroke-linejoin="round"></path>
                      </svg></span></a></div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>


    <section class="page-navigation flex justify-center items-start flex-col flex-wrap w-full p-0 mt-16 mb-8">
      <h2 class="mb-4 text-3xl max-w-3xl font-bold mb-8">Check my other posts ğŸ“š</h2>
      <div class="w-full items-start justify-center grid gap-8" style="grid-template-columns: 1fr 1fr 1fr; grid-gap: 3rem">
        
        <a class="w-full container-hover" href="/nlp/2023/10/06/seq2seq-pytorch/">
          <div class="w-full">
            <div class="mb-4 h-48 content-hover" style="transform: none;">
              <div class="h-full relative">
                <div data-loaded="true"><img class="transition-all" alt="" loading="lazy"
                    decoding="async" data-nimg="fill" sizes="100vw"
                    src="/assets/media/feature/pytorch.png"
                    style="position: absolute; height: 100%; width: 100%; inset: 0px; object-fit: cover; object-position: center center; color: transparent; border-radius: 1rem;">
                </div>
              </div>
            </div>
            <div class="flex items-center justify-start text-gray-600 dark:text-gray-300 text-sm gap-2 px-2"><time>October 6, 2023</time>â€¢
              <span>
              </span></div>
            <h3 class="px-2 py-2 text-black dark:text-white text-2xl font-bold">Triá»ƒn khai seq2seq vá»›i Pytorch</h3>
          </div>
        </a>
        
        <a class="w-full container-hover" href="/nlp/2023/10/06/nlp-papers-to-be-read/">
          <div class="w-full">
            <div class="mb-4 h-48 content-hover" style="transform: none;">
              <div class="h-full relative">
                <div data-loaded="true"><img class="transition-all" alt="" loading="lazy"
                    decoding="async" data-nimg="fill" sizes="100vw"
                    src="/assets/media/feature/nlp.png"
                    style="position: absolute; height: 100%; width: 100%; inset: 0px; object-fit: cover; object-position: center center; color: transparent; border-radius: 1rem;">
                </div>
              </div>
            </div>
            <div class="flex items-center justify-start text-gray-600 dark:text-gray-300 text-sm gap-2 px-2"><time>October 6, 2023</time>â€¢
              <span>
              
              12 min read
              </span></div>
            <h3 class="px-2 py-2 text-black dark:text-white text-2xl font-bold">NLP Papers to be Read</h3>
          </div>
        </a>
        
        <a class="w-full container-hover" href="/nlp/2023/10/06/attention-is-all-you-need/">
          <div class="w-full">
            <div class="mb-4 h-48 content-hover" style="transform: none;">
              <div class="h-full relative">
                <div data-loaded="true"><img class="transition-all" alt="" loading="lazy"
                    decoding="async" data-nimg="fill" sizes="100vw"
                    src="/assets/media/feature/transformers.jpg"
                    style="position: absolute; height: 100%; width: 100%; inset: 0px; object-fit: cover; object-position: center center; color: transparent; border-radius: 1rem;">
                </div>
              </div>
            </div>
            <div class="flex items-center justify-start text-gray-600 dark:text-gray-300 text-sm gap-2 px-2"><time>October 6, 2023</time>â€¢
              <span>
              
              14 min read
              </span></div>
            <h3 class="px-2 py-2 text-black dark:text-white text-2xl font-bold">Attention is All You Need</h3>
          </div>
        </a>
        
      </div>
    </section>

  </article>
</div>

<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://zhaospei.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

<!-- <script async src="/assets/js/scroll.js"></script> -->
<script async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  type="text/javascript"></script>
      </main>
    </div>
    <footer class="flex flex-col justify-center items-center w-full bg-gray-200 dark:bg-zinc-800">
        <div
                class="pt-8 pb-4 px-4 flex justify-center items-start w-full flex-col flex-wrap max-w-5xl box-border gap-4">
                <h4 class="text-3xl font-extrabold"> Let's build something together!</h4>
                <p class="max-w-xl">Feel free to reach out if you're looking for a developer, have a question or just
                        want to connect ğŸ“­</p>
                <a rel="noreferrer noopener" target="_blank" href="mailto:dungbuit1k28@gmail.com">
                        <span>dungbuit1k28@gmail.com</span>
                        <span class="inline-block w-3 h-3 mt-2 ml-1"><svg class="block w-full h-full"
                                        viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
                                        <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2"
                                                stroke-linecap="round" stroke-linejoin="round"></path>
                                </svg></span>
                </a>

                <div class="flex justify-between items-center w-full mt-8">
                        <a style="max-width: 3rem;" href="/">
                                <span class="sr-only">home</span>
                                <img class="rounded-full" alt="blog-icon" src="/assets/media/blog-icon.jpg">
                        </a>
                        <div class="justify-center items-center gap-8 hidden md:flex">
                                <a href="/sitemap.xml" rel="noreferrer noopener" target="_blank">
                                        <span> sitemap</span>
                                        <span class="inline-block w-3 h-3 mt-2 ml-1"><svg class="block w-full h-full"
                                                        viewBox="0 0 14 14" fill="none"
                                                        xmlns="http://www.w3.org/2000/svg">
                                                        <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2"
                                                                stroke-linecap="round" stroke-linejoin="round"></path>
                                                </svg></span>
                                </a>
                                <a href="/feed.xml" rel="noreferrer noopener" target="_blank">
                                        <span> rss</span>
                                        <span class="inline-block w-3 h-3 mt-2 ml-1"><svg class="block w-full h-full"
                                                        viewBox="0 0 14 14" fill="none"
                                                        xmlns="http://www.w3.org/2000/svg">
                                                        <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2"
                                                                stroke-linecap="round" stroke-linejoin="round"></path>
                                                </svg></span>
                                </a>
                                <a href="/assets/BuiTuanDung-Resume-1.pdf" rel="noreferrer noopener" target="_blank">
                                        <span> resume</span>
                                        <span class="inline-block w-3 h-3 mt-2 ml-1"><svg class="block w-full h-full"
                                                        viewBox="0 0 14 14" fill="none"
                                                        xmlns="http://www.w3.org/2000/svg">
                                                        <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2"
                                                                stroke-linecap="round" stroke-linejoin="round"></path>
                                                </svg></span>
                                </a>
                                <a href="https://github.com/zhaospei" rel="noreferrer noopener" target="_blank">
                                        <span> github</span>
                                        <span class="inline-block w-3 h-3 mt-2 ml-1"><svg class="block w-full h-full"
                                                        viewBox="0 0 14 14" fill="none"
                                                        xmlns="http://www.w3.org/2000/svg">
                                                        <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2"
                                                                stroke-linecap="round" stroke-linejoin="round"></path>
                                                </svg></span>
                                </a>
                        </div>

                </div>

                <span class="text-sm text-gray-700 dark:text-gray-200">
                        Â© 2023 <span class="text-cyan-900 dark:text-cyan-600">real</span> day, <span class="text-cyan-900 dark:text-cyan-600">real</span>
                        time. Dung BTuan. All rights reserved.
                </span>
                <small class="text-gray-600 dark:text-gray-300"> <a href="https://github.com/zhaospei/zhaospei.github.io" target="_blank"
                                rel="noreferrer noopener">polygon</a> theme on <a href="https://jekyllrb.com"
                                target="_blank" rel="noreferrer noopener">jekyll</a> </small>
        </div>

</footer>

<div class="fill-dark fill-white"></div>
    <script src="/assets/js/vanilla-tilt.js"></script>
    <script src="/assets/js/app.js"></script>
  </body>
</html>