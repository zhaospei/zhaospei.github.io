<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="utf-8">
    <title>Attention is All You Need - Tuan-Dung Bui</title>
    <link rel="icon" type="image/x-icon" href="/assets/media/blog-icon.jpg">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="stylesheet" href="/assets/css/github.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
    <script src="https://unpkg.com/typed.js@2.0.16/dist/typed.umd.js"></script>
    <script src="/assets/js/main.js"></script>
    <link type="application/atom+xml" rel="alternate" href="https://zhaospei.github.io//feed.xml" title="Tuan-Dung Bui" />
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Attention is All You Need | Tuan-Dung Bui</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Attention is All You Need" />
<meta name="author" content="zhao" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Transformer l√† m√¥ h√¨nh seq2seq ƒë∆∞·ª£c Google Brain ƒë·ªÅ xu·∫•t trong m·ªôt b√†i b√°o xu·∫•t b·∫£n v√†o cu·ªëi nƒÉm 2017. Gi·ªù ƒë√¢y, n√≥ ƒë√£ ƒë·∫°t ƒë∆∞·ª£c nhi·ªÅu ·ª©ng d·ª•ng v√† ti·ªán √≠ch m·ªü r·ªông v√† BERT l√† m√¥ h√¨nh ng√¥n ng·ªØ ƒë∆∞·ª£c ƒë√†o t·∫°o tr∆∞·ªõc c√≥ ngu·ªìn g·ªëc t·ª´ Transformer." />
<meta property="og:description" content="Transformer l√† m√¥ h√¨nh seq2seq ƒë∆∞·ª£c Google Brain ƒë·ªÅ xu·∫•t trong m·ªôt b√†i b√°o xu·∫•t b·∫£n v√†o cu·ªëi nƒÉm 2017. Gi·ªù ƒë√¢y, n√≥ ƒë√£ ƒë·∫°t ƒë∆∞·ª£c nhi·ªÅu ·ª©ng d·ª•ng v√† ti·ªán √≠ch m·ªü r·ªông v√† BERT l√† m√¥ h√¨nh ng√¥n ng·ªØ ƒë∆∞·ª£c ƒë√†o t·∫°o tr∆∞·ªõc c√≥ ngu·ªìn g·ªëc t·ª´ Transformer." />
<link rel="canonical" href="https://zhaospei.github.io//nlp/2023/10/06/attention-is-all-you-need/" />
<meta property="og:url" content="https://zhaospei.github.io//nlp/2023/10/06/attention-is-all-you-need/" />
<meta property="og:site_name" content="Tuan-Dung Bui" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-10-06T00:00:00+07:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Attention is All You Need" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"zhao"},"dateModified":"2023-10-06T00:00:00+07:00","datePublished":"2023-10-06T00:00:00+07:00","description":"Transformer l√† m√¥ h√¨nh seq2seq ƒë∆∞·ª£c Google Brain ƒë·ªÅ xu·∫•t trong m·ªôt b√†i b√°o xu·∫•t b·∫£n v√†o cu·ªëi nƒÉm 2017. Gi·ªù ƒë√¢y, n√≥ ƒë√£ ƒë·∫°t ƒë∆∞·ª£c nhi·ªÅu ·ª©ng d·ª•ng v√† ti·ªán √≠ch m·ªü r·ªông v√† BERT l√† m√¥ h√¨nh ng√¥n ng·ªØ ƒë∆∞·ª£c ƒë√†o t·∫°o tr∆∞·ªõc c√≥ ngu·ªìn g·ªëc t·ª´ Transformer.","headline":"Attention is All You Need","mainEntityOfPage":{"@type":"WebPage","@id":"https://zhaospei.github.io//nlp/2023/10/06/attention-is-all-you-need/"},"url":"https://zhaospei.github.io//nlp/2023/10/06/attention-is-all-you-need/"}</script>
<!-- End Jekyll SEO tag -->

  </head>
  <body class="bg-gray-100 dark:bg-zinc-900 dark:text-white">
    <div class="bg-gray-100 dark:bg-zinc-900 mx-auto max-w-3xl px-3 sm:px-6 xl:max-w-5xl xl:px-0">
      <div
    class="hidden fixed w-full h-screen inset-0 bg-gray-200 opacity-95 z-50 transition-transform transform ease-in-out duration-300 translate-x-0 mobile-menu dark:bg-zinc-800">
    <button type="button" aria-label="toggle modal"
        class="cursor-pointer fixed right-4 top-4 h-8 w-8 cursor-auto focus:outline-none border-none mobile-menu-btn-close"><svg
            xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="text-gray-900 svg-change-color">
            <path fill-rule="evenodd"
                d="M4.293 4.293a1 1 0 011.414 0L10 8.586l4.293-4.293a1 1 0 111.414 1.414L11.414 10l4.293 4.293a1 1 0 01-1.414 1.414L10 11.414l-4.293 4.293a1 1 0 01-1.414-1.414L8.586 10 4.293 5.707a1 1 0 010-1.414z"
                clip-rule="evenodd"></path>
        </svg></button>
    <nav class="fixed mt-8 h-full">
        
        <div class="px-8 py-4"><a class="text-xl font-bold text-gray-900 dark:text-white" href="/"> Home </a></div>
        
        <div class="px-8 py-4"><a class="text-xl font-bold text-gray-900 dark:text-white" href="/works.html"> Works </a></div>
        
        <div class="px-8 py-4"><a class="text-xl font-bold text-gray-900 dark:text-white" href="/blog.html"> Blog </a></div>
        
        <div class="px-8 py-4"><a class="text-xl font-bold text-gray-900 dark:text-white" href="/assets/BuiTuanDung-Resume-1.pdf"> Resume </a></div>
        
        <div class="px-8 py-4"><a class="text-xl font-bold text-gray-900 dark:text-white" href="/about/"> About </a></div>
        

    </nav>
</div>

<header class="grid justify-center w-full relative" style="grid-template-columns: 2fr auto 1fr;">
    <a style="" href="/">
        <div class="text-dark dark:text-white flex items-center text-xl font-semibold" style="height: 48px;"> 
            <span class="blog-link">~/</span>
            <div class="Typewriter font-normal ml-1" data-testid="typewriter-wrapper"><span class="Typewriter__wrapper"></span><span class="Typewriter__cursor">|</span></div>
        </div>
    </a>
    <nav
        class="flex items-center justify-between relative max-w-3xl mx-auto text-gray-900">
        
        <a class="font-bold text-gray-800 hidden md:inline-block mx-1 p-1 sm:px-3 rounded-3xl hover:bg-white transition-all dark:text-gray-100 dark:hover:bg-zinc-950"
             href="/"><span
                class="capsize">Home</span></a>
        
        <a class="font-bold text-gray-800 hidden md:inline-block mx-1 p-1 sm:px-3 rounded-3xl hover:bg-white transition-all dark:text-gray-100 dark:hover:bg-zinc-950"
             href="/works.html"><span
                class="capsize">Works</span></a>
        
        <a class="font-bold text-gray-800 hidden md:inline-block mx-1 p-1 sm:px-3 rounded-3xl hover:bg-white transition-all dark:text-gray-100 dark:hover:bg-zinc-950"
             href="/blog.html"><span
                class="capsize">Blog</span></a>
        
        <a class="font-bold text-gray-800 hidden md:inline-block mx-1 p-1 sm:px-3 rounded-3xl hover:bg-white transition-all dark:text-gray-100 dark:hover:bg-zinc-950"
             href="/assets/BuiTuanDung-Resume-1.pdf"><span
                class="capsize">Resume</span></a>
        
        <a class="font-bold text-gray-800 hidden md:inline-block mx-1 p-1 sm:px-3 rounded-3xl hover:bg-white transition-all dark:text-gray-100 dark:hover:bg-zinc-950"
             href="/about/"><span
                class="capsize">About</span></a>
        
        <button
            class="cursor-pointer bg-gray-200 bg-opacity-25	backdrop-blur-sm rounded-lg text-sm border-y border-transparent mobile-menu-btn-open visible md:hidden fixed z-50 right-4 bottom-4 h-8 w-8 cursor-auto focus:outline-none border-none mobile-menu-btn"
            type="button">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" fill="currentColor" class="text-gray-900 svg-change-color">
                <path fill-rule="evenodd"
                    d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 10a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM3 15a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z"
                    clip-rule="evenodd"></path>
            </svg>
        </button>
    </nav>
    <div class="flex justify-self-end items-center gap-8">
        <label class="relative cursor-pointer">
            <span class="sr-only">
                "change theme to"
                <!-- -->
                "dark"
            </span>
            <div class="themeSwitchToggle w-12 h-6 bg-gray-200 rounded-3xl p-1 ml-4 dark:bg-zinc-800">
                <div class="themeSwitchIcon w-6 h-6 bg-white rounded-full flex items-center justify-center dark:bg-zinc-950" style="transition: transform .2s ease-in-out;">
                    
                </div>
            </div>
        </label>
        <a class="contact-header flex font-semibold items-center gap-2 text-black text-sm dark:text-white" href="/contact">
            Contact
            <svg class="svg-change-color dark:text-white"viewBox="0 0 513 513" xmlns="http://www.w3.org/2000/svg" style="width:1.2rem;">
                <path
                    d="M440 6.5L24 246.4c-34.4 19.9-31.1 70.8 5.7 85.9L144 379.6V464c0 46.4 59.2 65.5 86.6 28.6l43.8-59.1 111.9 46.2c5.9 2.4 12.1 3.6 18.3 3.6 8.2 0 16.3-2.1 23.6-6.2 12.8-7.2 21.6-20 23.9-34.5l59.4-387.2c6.1-40.1-36.9-68.8-71.5-48.9zM192 464v-64.6l36.6 15.1L192 464zm212.6-28.7l-153.8-63.5L391 169.5c10.7-15.5-9.5-33.5-23.7-21.2L155.8 332.6 48 288 464 48l-59.4 387.3z"
                    fill=""></path>
            </svg>
        </a>
    </div>
</header>
      <main class="flex flex-col justify-center py-16">
          <!-- <style>
  .table-of-contents-container li a {
    color: rgb(122, 118, 119);
    font-weight: 500;
  }

  .table-of-contents-container li.active>a {
    color: rgb(175, 165, 248);
  }

  .table-of-contents-container li a:hover {
    filter: brightness(60%);
  }

  .table-of-contents-container {
    font-size: 14px;
  }

  .table-of-contents-container nav {}

  .toc-h1 {
    margin-top: 1em;
  }

  .toc-h2 {
    margin-top: 0.75em;
  }

  .toc-h3 {
    margin-top: 0.5em;
  }


  .toc-h1 {
    font-weight: 600;
  }

  .toc-h2 {
    padding-left: 1em;
    font-weight: 500;
  }

  .toc-h3 {
    padding-left: 2em;
    font-weight: 400;
  }

  .toc-h1:first-child {
    margin-top: 0;
  }


  @media only screen and (max-width:1280px) {
    .table-of-contents-container {
      display: none;
    }
  }
</style> -->

<!-- <style>
    .reading-line {
        position: fixed;
        top: 0;
        left: 0;
        border-bottom: 4px solid #66aacb;
        z-index: 404;
    }
</style>

<div class="reading-line">
</div> -->
<div class="post-container">
  <article class="post-content-container flex flex-col items-start justify-center">
    <header class="flex flex-col gap-12 w-full max-w-full pb-8">
      <div class="flex flex-col justify-center gap-4">
        <h1 class="text-5xl font-extrabold leading-tight"> Attention is All You Need </h1>
        <div class="flex flex-col items-start justify-between mt-2 md:flex-row md:items-center">
          <div class="flex items-center">
            
            
            
            <img class="object-cover w-[40px] h-[40px] rounded-full" src="/assets/media/author/zhao-img.png" alt="Tuan-Dung Bui">
            
            <p class="ml-2 text-sm text-gray-700 dark:text-gray-300">
              Tuan-Dung Bui / October 6, 2023
            </p>
            
          </div>
          <p class="mt-2 text-sm text-gray-600 min-w-32 md:mt-0 dark:text-gray-300">
            

11 min read

          </p>
        </div>
      </div>
    </header>
    <div class="flex justify-center w-full flex-row-reverse">
      <!-- <aside class="table-of-contents-container ml-16 sticky top-16 mt-16" style="max-height: calc(100vh - 16.5rem);">
        <h2 class="uppercase tracking-widest text-base	font-extrabold mb-6"> Table of contents </h2>
        <nav class="overflow-auto pr-2" style="max-height: calc(100vh - 16.5rem);">
          <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#-self-attention"># Self-Attention</a></li>
<li class="toc-entry toc-h1"><a href="#-encoder"># Encoder</a>
<ul>
<li class="toc-entry toc-h2"><a href="#position-wise-fully-connected-feed-forward-network">position-wise fully connected feed-forward network</a></li>
<li class="toc-entry toc-h2"><a href="#residual-connection">residual connection</a></li>
<li class="toc-entry toc-h2"><a href="#batch-norm-v√†-layer-norm">Batch Norm v√† Layer Norm</a></li>
<li class="toc-entry toc-h2"><a href="#to√†n-b·ªô-ki·∫øn-tr√∫c-encoder">To√†n b·ªô ki·∫øn tr√∫c Encoder</a>
<ul>
<li class="toc-entry toc-h3"><a href="#input--positional-embedding">input &amp; positional embedding</a></li>
<li class="toc-entry toc-h3"><a href="#multi-head-attention">multi-head attention</a></li>
<li class="toc-entry toc-h3"><a href="#add--norm">add &amp; norm</a></li>
<li class="toc-entry toc-h3"><a href="#feed-forward">feed forward</a></li>
<li class="toc-entry toc-h3"><a href="#add--norm-1">add &amp; norm</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#-decoder"># Decoder</a></li>
<li class="toc-entry toc-h1"><a href="#-word-embedding-v√†-positional-embedding"># Word Embedding v√† Positional Embedding</a>
<ul>
<li class="toc-entry toc-h2"><a href="#word-embedding">Word Embedding</a></li>
<li class="toc-entry toc-h2"><a href="#positional-embedding">Positional Embedding</a>
<ul>
<li class="toc-entry toc-h3"><a href="#nh√∫ng-v·ªã-tr√≠-t√πy-chinh">Nh√∫ng v·ªã tr√≠ t√πy chinh</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#-q--a"># Q &amp; A</a>
<ul>
<li class="toc-entry toc-h2"><a href="#t·∫°i-sao-transformer-c·∫ßn-multi-head-attention-">T·∫°i sao Transformer c·∫ßn Multi-head Attention ?</a></li>
<li class="toc-entry toc-h2"><a href="#∆∞u-ƒëi·ªÉm-c·ªßa-transformer-so-v·ªõi-rnnlstm-l√†-g√¨-t·∫°i-sao">∆Øu ƒëi·ªÉm c·ªßa Transformer so v·ªõi RNN/LSTM l√† g√¨? T·∫°i sao?</a></li>
<li class="toc-entry toc-h2"><a href="#t·∫°i-sao-transformer-c√≥-th·ªÉ-thay-th·∫ø-seq2seq">T·∫°i sao Transformer c√≥ th·ªÉ thay th·∫ø seq2seq?</a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#tham-kh·∫£o">Tham kh·∫£o</a></li>
</ul>
        </nav>
      </aside> -->
      <div class="items-center pb-8" style="grid-template-rows: auto 1fr;">
        <div class="thumbnail-image relative not-prose rounded-2xl overflow-hidden">
          
          <img alt="Post image feature" src="/assets/media/feature/transformers.jpg" loading="lazy" class="w-full">
          
        </div>
        <div class="post-content prose">
          <p><code class="language-plaintext highlighter-rouge">Transformer</code> l√† m√¥ h√¨nh seq2seq ƒë∆∞·ª£c Google Brain ƒë·ªÅ xu·∫•t trong m·ªôt b√†i b√°o xu·∫•t b·∫£n v√†o cu·ªëi nƒÉm 2017. Gi·ªù ƒë√¢y, n√≥ ƒë√£ ƒë·∫°t ƒë∆∞·ª£c nhi·ªÅu ·ª©ng d·ª•ng v√† ti·ªán √≠ch m·ªü r·ªông v√† <code class="language-plaintext highlighter-rouge">BERT</code> l√† m√¥ h√¨nh ng√¥n ng·ªØ ƒë∆∞·ª£c ƒë√†o t·∫°o tr∆∞·ªõc c√≥ ngu·ªìn g·ªëc t·ª´ Transformer.</p>

<p>Vi·ªác ƒë√†o t·∫°o <code class="language-plaintext highlighter-rouge">RNN</code> truy·ªÅn th·ªëng l√† n·ªëi ti·∫øp v√† n√≥ ph·∫£i ƒë·ª£i t·ª´ hi·ªán t·∫°i ƒë∆∞·ª£c x·ª≠ l√Ω tr∆∞·ªõc khi c√≥ th·ªÉ x·ª≠ l√Ω t·ª´ ti·∫øp theo. Transformer ƒë∆∞·ª£c hu·∫•n luy·ªán song song, t·ª©c l√† t·∫•t c·∫£ c√°c t·ª´ ƒë·ªÅu ƒë∆∞·ª£c hu·∫•n luy·ªán c√πng m·ªôt l√∫c, ƒëi√™u n√†y l√†m tƒÉng ƒë√°ng k·ªÉ hi·ªáu qu·∫£ t√≠nh to√°n.</p>

<figure class="image" style="text-align: center;">
    <img src="/assets/media/post/transformer-model-architecture.png" alt="Ki·∫øn tr√∫c m√¥ h√¨nh Transformer" style="margin: auto;" />
    <figcaption style="font-style: italic;">Ki·∫øn tr√∫c m√¥ h√¨nh Transformer</figcaption>
</figure>

<h1 id="-self-attention"># Self-Attention</h1>
<p><code class="language-plaintext highlighter-rouge">Scaled Dot-Product Attention</code> l√† t√≠ch ch·∫•m chu·∫©n h√≥a Attention, chi ti·∫øt c·ª• th·ªÉ ƒë∆∞·ª£c th·ªÉ hi·ªán trong h√¨nh.</p>

<figure class="image" style="text-align: center;">
    <img src="/assets/media/post/attention.png" alt="" style="margin: auto;" />
    <figcaption style="font-style: italic;"></figcaption>
</figure>

\[Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V\]

<p>S·ª± ch√∫ √Ω c·ªßa nhi·ªÅu ƒë·∫ßu v√†o s·ª≠ d·ª•ng nhi·ªÅu b·ªô tr·ªçng s·ªë (<code class="language-plaintext highlighter-rouge">weights</code>) (\(W_q,W_k,W_v\)), gh√©p l·∫°i cho ra k·∫øt qu·∫£ cu·ªëi c√πng.</p>

\[MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O\]

<p>trong ƒë√≥</p>

\[head_i=Attention(QW^Q_i,KW^K_i,VW^V_i)\]

<p>Trong ƒë√≥ \(h = 8\), \(d_q=d_k=d_v=d_{model}/4=64\).</p>

<h1 id="-encoder"># Encoder</h1>
<p>B·ªô m√£ h√≥a ƒë∆∞·ª£c x·∫øp ch·ªìng l√™n nhau b·ªüi s√°u l·ªõp gi·ªëng h·ªát nhau, m·ªói l·ªõp bao g·ªìm hai l·ªõp con - c∆° ch·∫ø t·ª± ch√∫ √Ω nhi·ªÅu ƒë·∫ßu (<code class="language-plaintext highlighter-rouge">multi-head self-attention mechanism</code>) v√† m·∫°ng n∆° ron v·ªã tr√≠ chuy·ªÉn ti·∫øp ƒë∆∞·ª£c k·∫øt n·ªëi ƒë·∫ßy ƒë·ªß (<code class="language-plaintext highlighter-rouge">position-wise fully connected feed-forward network</code>). M·ªói l·ªõp con s·ª≠ d·ª•ng c√°c k·∫øt n·ªëi d∆∞ (<code class="language-plaintext highlighter-rouge">residual connection</code>) v√† l·ªõp chu·∫©n h√≥a (<code class="language-plaintext highlighter-rouge">layer normalization</code>). K√≠ch th∆∞·ªõc ƒë·∫ßu ra c·ªßa c√°c l·ªõp con l√† \(d_{model} = 512\).</p>

<p>ƒê·∫ßu ra c·ªßa l·ªõp con c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng:</p>

\[LayerNorm(x+Sublayer(x))\]

<h2 id="position-wise-fully-connected-feed-forward-network">position-wise fully connected feed-forward network</h2>
<p>M·∫°ng n∆°-ron chuy·ªÉn ti·∫øp ƒë∆∞·ª£c k·∫øt n·ªëi ƒë·∫ßy ƒë·ªß (<code class="language-plaintext highlighter-rouge">position-wise fully connected feed-forward network</code>) bao g·ªìm hai ph√©p bi·∫øn ƒë·ªïi tuy·∫øn t√≠nh v·ªõi k√≠ch ho·∫°t <code class="language-plaintext highlighter-rouge">ReLU</code> ·ªü gi·ªØa.</p>

\[FFN(x)=ReLU(xW_1+b_1)W_2+b_2\]

<p>K√≠ch th∆∞·ªõc l·ªõp b√™n trong (inner-layer) l√† 2048.</p>

<h2 id="residual-connection">residual connection</h2>
<p>M·∫°ng d∆∞ (<code class="language-plaintext highlighter-rouge">Residual Network</code>), c√°c k·∫øt n·ªëi t·∫Øt c√≥ kh·∫£ nƒÉng b·ªè qua m·ªôt ho·∫∑c nhi·ªÅu l·ªõp, do s·ª± t·ªìn t·∫°i c·ªßa k·∫øt n·ªëi t·∫Øt n√™n hi·ªáu su·∫•t c·ªßa m·∫°ng √≠t kh√¥ng k√©m h∆°n so v·ªõi c√°c m·∫°ng n√¥ng. Ph∆∞∆°ng ph√°p n√†y gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ suy tho√°i do c√°c l·ªõp ch·∫≠p x·∫øp ch·ªìng l√™n nhau g√¢y ra, s·ªë l∆∞·ª£ng l·ªõp c·ªßa m·∫°ng n∆°-ron t√≠ch ch·∫≠p ƒë√£ ƒë∆∞·ª£c tƒÉng l√™n r·∫•t nhi·ªÅu l√™n h√†ng trƒÉm l·ªõp, v√† c·∫£i thi·ªán ƒë√°ng k·ªÉ hi·ªáu su·∫•t c·ªßa m·∫°ng th·∫ßn kinh t√≠ch ch·∫≠p (<code class="language-plaintext highlighter-rouge">resnet</code>).</p>

<figure class="image" style="text-align: center;">
    <img src="/assets/media/post/resnet.png" alt="" style="margin: auto;" />
    <figcaption style="font-style: italic;"></figcaption>
</figure>

<h2 id="batch-norm-v√†-layer-norm">Batch Norm v√† Layer Norm</h2>

<figure class="image" style="text-align: center;">
    <img src="/assets/media/post/normalization.png" alt="" style="margin: auto;" />
    <figcaption style="font-style: italic;"></figcaption>
</figure>

<p>ƒê·∫∑t k√≠ch th∆∞·ªõc h√¨nh ·∫£nh ƒë·∫ßu v√†o l√† \([N, C, H, W]\):</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">Batch Norm</code>, chu·∫©n h√≥a theo t·ª´ng batch NHW, l√† ƒë·ªÉ chu·∫©n h√≥a ƒë·∫ßu v√†o t·ª´ng k√™nh ƒë∆°n, ƒë·ªÅu n√†y kh√¥ng hi·ªáu qu·∫£ ƒë·ªëi v·ªõi <code class="language-plaintext highlighter-rouge">batch-size</code> nh·ªè.</li>
  <li><code class="language-plaintext highlighter-rouge">Layer Norm</code>, chu·∫©n h√≥a theo t·ª´ng layer CHW, l√† ƒë·ªÉ chu·∫©n h√≥a ƒë·∫ßu v√†o ·ªü m·ªói ƒë·ªô s√¢u, ch·ªß y·∫øu c√≥ t√°c d·ª•ng r√µ r√†ng tr√™n RNN.</li>
</ul>

<p>S·ª± hi·ªÉu bi·∫øt c√° nh√¢n:</p>

<ul>
  <li>D√†nh cho CNN, n·∫øu h·∫°t nh√¢n t√≠ch ch·∫≠p qu√©t h√¨nh ·∫£nh ƒë·∫ßu v√†o, n√≥ ƒë∆∞·ª£c t√≠nh l√† thao t√°c t√≠ch ch·∫≠p, c·∫ßn c√≥ t·ªïng thao t√°c batchsize. Do ƒë√≥, chu·∫©n h√≥a c·∫ßn ƒë∆∞·ª£c th·ª±c hi·ªán theo batch.</li>
  <li>D√†nh cho RNN, batchsize th∆∞·ªùng l√† 1, s·ªë v√≤ng l·∫∑p l√† s·ªë ƒë·ªô d√†i ƒë·∫ßu v√†o (s·ªë channel). Do ƒë√≥, chu·∫©n h√≥a c·∫ßn ƒë∆∞·ª£c th·ª±c hi·ªán theo channel.</li>
</ul>

<h2 id="to√†n-b·ªô-ki·∫øn-tr√∫c-encoder">To√†n b·ªô ki·∫øn tr√∫c Encoder</h2>
<h3 id="input--positional-embedding">input &amp; positional embedding</h3>

\[X=Embedding-Lookup(X)+Positional-Encoding\]

<h3 id="multi-head-attention">multi-head attention</h3>

\[Q=Linear_q(X)=XW_q\]

\[K=Linear_q(X)=XW_k\]

\[V=Linear_v(X)=XW_v\]

\[X_{attention}=Self-Attention(Q,K,V)\]

<h3 id="add--norm">add &amp; norm</h3>

\[X_{attention}=LayerNorm(X+X_{attention})\]

<h3 id="feed-forward">feed forward</h3>

\[X_{hidden}=Linear(ReLU(Linear(X_{attention})))\]

<h3 id="add--norm-1">add &amp; norm</h3>

\[X_{hidden}=LayerNorm(X_{hidden}+X_{attention})\]

<p><code class="language-plaintext highlighter-rouge">multi-head attention</code> trong <code class="language-plaintext highlighter-rouge">encoder</code> l√† m·ªôt c∆° ch·∫ø t·ª± ch√∫ √Ω (<code class="language-plaintext highlighter-rouge">self-attention mechanism</code>). \(k\), \(q\) v√† \(v\) trong c∆° ch·∫ø t·ª± ch√∫ √Ω ƒë·ªÅu xu·∫•t ph√°t t·ª´ c√πng m·ªôt v·ªã tr√≠, m·ªói l·ªõp c·ªßa encoder c√≥ th·ªÉ nh·∫≠n ƒë∆∞·ª£c t·∫•t c·∫£ v·ªã tr√≠ c·ªßa l·ªõp tr∆∞·ªõc.</p>

<h1 id="-decoder"># Decoder</h1>
<p>B·ªô gi·∫£i m√£ bao g·ªìm s√°u l·ªõp gi·ªëng h·ªát x·∫øp ch·ªìng l√™n nhau; trong Multi-head Attention, \(q\) ƒë∆∞·ª£c ƒë·∫øn t·ª´ l·ªõp tr∆∞·ªõc ƒë√≥ c·ªßa decoder, k v√† v ƒë·∫øn t·ª´ ƒë·∫ßu ra c·ªßa encoder. ƒêi·ªÅu cho ph√©p m·ªói v·ªã tr√≠ trong b·ªô gi·∫£i m√£ nh·∫≠n bi·∫øt ƒë∆∞·ª£c t·∫•t c·∫£ c√°c v·ªã tr√≠ c·ªßa chu·ªói ƒë·∫ßu v√†o.</p>

<p>Ngo√†i hai l·ªõp con trong b·ªô m√£ h√≥a, b·ªô gi·∫£i m√£ th√™m m·ªôt l·ªõp con m·ªõi x·ª≠ l√Ω ƒë·∫ßu ra c·ªßa b·ªô m√£ h√≥a - <code class="language-plaintext highlighter-rouge">masked multi-head self-attention mechanism</code>. B·ªô m√£ h√≥a trong seq2seq truy·ªÅn th·ªëng s·ª≠ d·ª•ng m√¥ h√¨nh RNN, v√¨ v·∫≠y n·∫øu c√°c t·ª´ t·∫°i th·ªùi ƒëi·ªÉm t ƒë∆∞·ª£c nh·∫≠p v√†o trong qu√° tr√¨nh hu·∫•n luy·ªán th√¨ m√¥ h√¨nh s·∫Ω kh√¥ng th·ªÉ nh√¨n th·∫•y c√°c t·ª´ tr∆∞·ªõc ƒë√≥ v√†o c√°c th·ªùi ƒëi·ªÉm trong t∆∞∆°ng lai, b·ªüi v√¨ RNN ho·∫°t ƒë·ªông theo th·ªùi gian v√† ch·ªâ khi thao t√°c t·∫°i th·ªùi ƒëi·ªÉm t ho√†n th√†nh, ch·ªâ khi ƒë√≥ ta m·ªõi c√≥ th·ªÉ nh√¨n th·∫•y c√°c t·ª´ t·∫°i th·ªùi ƒëi·ªÉm t + 1. V√† Transformer Decoder ƒë√£ kh√¥ng s·ª≠ d·ª•ng RNN, thay ƒë·ªïi sang Self-Attention, ƒëi·ªÅu n√†y t·∫°o ra m·ªôt v·∫•n ƒë·ªÅ, trong qu√° tr√¨nh hu·∫•n luy·ªán, to√†n b·ªô ground truth ƒë√£ ƒë∆∞·ª£c hi·ªÉn th·ªã v·ªõi Decoder, ƒëi·ªÅu n√†y r√µ r√†ng l√† sai, ch√∫ng ta c·∫ßn ph·∫£i th·ª±c hi·ªán m·ªôt s·ªë x·ª≠ l√Ω tr√™n ƒë·∫ßu v√†o c·ªßa Decoder, qu√° tr√¨nh n√†y ƒë∆∞·ª£c g·ªçi l√† <code class="language-plaintext highlighter-rouge">Mask</code> - ƒê·∫∑t t·∫•t c·∫£ c√°c gi√° tr·ªã sau postion th√†nh \(-\infty\) tr∆∞·ªõc khi v√†o softmax.</p>

<p>V√≠ d·ª•, ground truth c·ªßa Decoder l√† ‚Äú<start> I am fine", ch√∫ng ta cho c√¢u n√†y v√†o b·ªô Decoder, sau khi Word Embedding v√† Positional Encoding, th·ª±c hi·ªán ph√©p bi·∫øn ƒë·ªïi tuy·∫øn t√≠nh b·∫≠c 3 tr√™n ma tr·∫≠n thu ƒë∆∞·ª£c $$(W_Q,W_K,W_V)$$ Sau ƒë√≥ th·ª±c hi·ªán self-attention, tr∆∞·ªõc ti√™n, nh·∫≠n Scaled Scores th√¥ng qua $$\frac{Q√óK^T}{\sqrt{d_k}}$$, b∆∞·ªõc ti·∫øp theo r·∫•t quan tr·ªçng, ch√∫ng ta c·∫ßn mask theo Scaled Scores, v√≠ d·ª•, khi nh·∫≠p "I", hi·ªán t·∫°i m√¥ h√¨nh ch·ªâ bi·∫øt th√¥ng tin c·ªßa t·∫•t c·∫£ c√°c t·ª´ tr∆∞·ªõc ƒë√≥ c·ªßa "I", t·ª©c th√¥ng tin c·ªßa "<start>" v√† "I", kh√¥ng ƒë∆∞·ª£c ph√©p bi·∫øt ƒë∆∞·ª£c th√¥ng tin c·ªßa c√°c t·ª´ sau "I". L√Ω do r·∫•t ƒë∆°n gi·∫£n, khi d·ª± ƒëo√°n l√† ch√∫ng ta d·ª± ƒëo√°n theo th·ª© t·ª± t·ª´ng ch·ªØ, l√†m sao c√≥ th·ªÉ bi·∫øt ƒë∆∞·ª£c th√¥ng tin c·ªßa nh·ªØng t·ª´ sau tr∆∞·ªõc khi d·ª± ƒëo√°n xong t·ª´ n√†y? Mask r·∫•t ƒë∆°n gi·∫£n, ƒë·∫ßu ti√™n t·∫°o m·ªôt ma tr·∫≠n c√≥ tam gi√°c ho√†n to√†n ph√≠a d∆∞·ªõi b·∫±ng 0 v√† tam gi√°c ho√†n t√≤an ph√≠a tr√™n b·∫±ng √¢m v√¥ c√πng, sau ƒë√≥ ch·ªâ c·∫ßn th√™m n√≥ v√†o Scaled Scores.</start></start></p>

<h1 id="-word-embedding-v√†-positional-embedding"># Word Embedding v√† Positional Embedding</h1>
<h2 id="word-embedding">Word Embedding</h2>
<p>Ph·∫ßn nh√∫ng t·ª´ s·ª≠ d·ª•ng nh√∫ng t·ª´ c√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c, k√≠ch th∆∞·ªõc c·ªßa n√≥ l√† \(d_{model}\).
H√¨nh th·ª©c m√£ h√≥a <code class="language-plaintext highlighter-rouge">One-hot</code> ng·∫Øn g·ªçn, nh∆∞ng qu√° th∆∞a th·ªõt, n√≥ kh√¥ng ph·∫£n √°nh s·ª± gi·ªëng nhau v·ªÅ nghƒ©a c·ªßa t·ª´. V√¨ v·∫≠y h√£y s·ª≠ d·ª•ng <code class="language-plaintext highlighter-rouge">the Skip-Gram Model</code> ho·∫∑c <code class="language-plaintext highlighter-rouge">continuous bag of words model</code> ho·∫∑c c√°c nh√∫ng t·ª´ kh√°c c√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c kh√°c.</p>

<h2 id="positional-embedding">Positional Embedding</h2>
<p>B·ªüi v√¨ m√¥ h√¨nh kh√¥ng bao g·ªìm c√°c c·∫•u tr√∫c tu·∫ßn ho√†n, v√¨ v·∫≠y n·∫Øm b·∫Øt ƒë∆∞·ª£c c√°c th√¥ng tin th·ª© t·ª± tu·∫ßn t·ª±, v√≠ d·ª• n·∫øu \(K\) v√† \(V\) ƒë∆∞·ª£c x√≥a tr·ªôn theo t·ª´ng h√†ng th√¨ k·∫øt qu·∫£ sau Attention s·∫Ω gi·ªëng nhau. Tuy nhi√™n, th√¥ng tin tu·∫ßn t·ª± r·∫•t quan tr·ªçng v√† th·ªÉ hi·ªán c·∫•u tr√∫c to√†n c·∫ßu, do ƒë√≥ th√¥ng tin position tuy·ªát ƒë·ªëi v√† t∆∞∆°ng ƒë·ªëi c·ªßa token tu·∫ßn t·ª± ph·∫£i ƒë∆∞·ª£c s·ª≠ d·ª•ng.</p>
<h3 id="nh√∫ng-v·ªã-tr√≠-t√πy-chinh">Nh√∫ng v·ªã tr√≠ t√πy chinh</h3>
<p>M·ªôt √Ω t∆∞·ªüng l√† l·∫•y m·ªôt s·ªë trong kho·∫£ng \([0, 1]\) v√† g√°n n√≥ cho m·ªói t·ª´, trong ƒë√≥ 0 ƒë∆∞·ª£c trao cho t·ª´ ƒë·∫ßu ti√™n, 1 cho t·ª´ cu·ªëi c√πng, c√¥ng th·ª©c c·ª• th·ªÉ l√† \(PE=\dfrac{pos}{T‚àí1}\).</p>

<p><strong>‚Ä¶ (Ph·∫ßn n√†y s·∫Ω vi·∫øt ti·∫øp trong t∆∞∆°ng lai xa ch·ª© gi·ªù 2h s√°ng r·ªìi ƒëi ng·ªß).</strong></p>

<h1 id="-q--a"># Q &amp; A</h1>
<h2 id="t·∫°i-sao-transformer-c·∫ßn-multi-head-attention-">T·∫°i sao Transformer c·∫ßn Multi-head Attention ?</h2>
<p>B√†i b√°o ƒë·ªÅ c·∫≠p l√Ω do vi·ªác ti·∫øn h√†nh Multi-head Attention l√† ƒë·ªÉ chia m√¥ h√¨nh th√†nh nhi·ªÅu ƒë·∫ßu ƒë·ªÉ t·∫°o th√†nh nhi·ªÅu kh√¥ng gian con, cho ph√©p m√¥ h√¨nh ch√∫ √Ω ƒë·∫øn c√°c kh√≠a c·∫°nh kh√°c nhau c·ªßa th√¥ng tin v√† cu·ªëi c√πng t·ªïng h·ª£p th√¥ng tin t·ª´ t·∫•t c·∫£ c√°c kh√≠a c·∫°nh. Tr√™n th·ª±c t·∫ø, c√≥ th·ªÉ h√¨nh dung b·∫±ng tr·ª±c gi√°c r·∫±ng n·∫øu b·∫°n t·ª± thi·∫øt k·∫ø m·ªôt m√¥ h√¨nh nh∆∞ v·∫≠y, attention s·∫Ω kh√¥ng ch·ªâ ƒë∆∞·ª£c th·ª±c hi·ªán m·ªôt l·∫ßn, k·∫øt qu·∫£ t·ªïng h·ª£p c·ªßa nhi·ªÅu l·∫ßn ch√∫ √Ω √≠t nh·∫•t c√≥ th·ªÉ n√¢ng cao m√¥ h√¨nh v√† c≈©ng c√≥ th·ªÉ ƒë∆∞·ª£c so s√°nh v·ªõi vai tr√≤ c·ªßa vi·ªác s·ª≠ d·ª•ng nhi·ªÅu t√≠ch ch·∫≠p c√πng l√∫c trong CNN, theo tr·ª±c gi√°c, s·ª± ch√∫ √Ω c·ªßa nhi·ªÅu ng∆∞·ªùi ƒë·ª©ng ƒë·∫ßu gi√∫p m·∫°ng n·∫Øm b·∫Øt ƒë∆∞·ª£c c√°c t√≠nh nƒÉng/ th√¥ng tin phong ph√∫ h∆°n.</p>
<h2 id="∆∞u-ƒëi·ªÉm-c·ªßa-transformer-so-v·ªõi-rnnlstm-l√†-g√¨-t·∫°i-sao">∆Øu ƒëi·ªÉm c·ªßa Transformer so v·ªõi RNN/LSTM l√† g√¨? T·∫°i sao?</h2>
<ol>
  <li>C√°c m√¥ h√¨nh RNN kh√¥ng th·ªÉ t√≠nh to√°n song song v√¨ vi·ªác t√≠nh to√°n t·∫°i th·ªùi ƒëi·ªÉm T ph·ª• thu·ªôc v√†o k·∫øt qu·∫£ t√≠nh to√°n c·ªßa l·ªõp ·∫©n t·∫°i th·ªùi ƒëi·ªÉm T - 1, c√≤n vi·ªác t√≠nh to√°n t·∫°i th·ªùi ƒëi·ªÉm T - 1 l·∫°i ph·ª• thu·ªôc t√≠nh to√°n c·ªßa l·ªõp ·∫©n t·∫°i th·ªùi ƒëi·ªÉm T - 2.</li>
  <li>Kh·∫£ nƒÉng tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng c·ªßa Transformer t·ªët h∆°n so v·ªõi c√°c m√¥ h√¨nh RNN.</li>
</ol>

<h2 id="t·∫°i-sao-transformer-c√≥-th·ªÉ-thay-th·∫ø-seq2seq">T·∫°i sao Transformer c√≥ th·ªÉ thay th·∫ø seq2seq?</h2>
<p>T·ª´ thay th·∫ø ·ªü ƒë√¢y h∆°i kh√¥ng ph√π h·ª£p, seq2seq tuy c≈© nh∆∞ng v·∫´n c√≥ ch·ªó ƒë·ª©ng, v·∫•n ƒë·ªÅ l·ªõn nh·∫•t c·ªßa seq2seq l√† ·ªü ch·ªó <strong>N√©n th√¥ng tin ·ªü ph√≠a Encoder th√†nh m·ªôt vector c√≥ ƒë·ªô d√†i c·ªë ƒë·ªãnh</strong> v√† s·ª≠ d·ª•ng n√≥ l√†m ƒë·∫ßu v√†o c·ªßa tr·∫°ng th√°i ƒë·∫ßu ti√™n ·ªü ph√≠a Decoder, ƒë·ªÉ d·ª± ƒëo√°n tr·∫°ng th√°i ·∫©n c·ªßa t·ª´ ƒë·∫ßu ti√™n (m√£ th√¥ng b√°o) ·ªü ph√≠a Decoder. Khi chu·ªói ƒë·∫ßu v√†o t∆∞∆°ng ƒë·ªëi d√†i, ƒëi·ªÅu n√†y r√µ r√†ng s·∫Ω m·∫•t r·∫•t nhi·ªÅu th√¥ng tin ·ªü ph√≠a Encoder v√† vector c·ªë ƒë·ªãnh s·∫Ω ƒë∆∞·ª£c g·ª≠i ƒë·∫øn ph√≠a Decoder c√πng m·ªôt l√∫c, <strong>b√™n Decoder kh√¥ng th·ªÉ ch√∫ √Ω ƒë·∫øn th√¥ng tin m√† n√≥ mu·ªën ch√∫ √Ω</strong>. M√¥ hinh transformer kh√¥ng ch·ªâ c·∫£i thi·ªán ƒë√°ng k·ªÉ hai khuy·∫øt ƒëi·ªÉm n√†y c·ªßa m√¥ h√¨nh seq2seq (M√¥-ƒëun attention t∆∞∆°ng t√°c nhi·ªÅu ƒë·∫ßu), v√† c≈©ng gi·ªõi thi·ªáu m√¥-ƒëun self-attention, tr∆∞·ªõc ti√™n h√£y ƒë·ªÉ tr√¨nh t·ª± ngu·ªìn v√† tr√¨nh t·ª± ƒë√≠ch ƒë∆∞·ª£c ‚Äút·ª± li√™n k·∫øt‚Äù, trong tr∆∞·ªùng h·ª£p n√†y, th√¥ng tin ch·ª©a trong embedding c·ªßa tr√¨nh t·ª± ngu·ªìn v√† tr√¨nh t·ª± ƒë√≠ch s·∫Ω phong ph√∫ h∆°n v√† l·ªõp FFN ti·∫øp theo c≈©ng n√¢ng cao kh·∫£ nƒÉng bi·ªÉu ƒë·∫°t c·ªßa m√¥ h√¨nh, v√† t√≠nh to√°n song song c·ªßa Transfomer v∆∞·ª£t xa c√°c model seq2seq.</p>

<h1 id="tham-kh·∫£o">Tham kh·∫£o</h1>

<p>[1] <a href="https://arxiv.org/abs/1706.03762">Attention is All You Need</a></p>

        </div>

        <div class="ml-8 mt-8 mb-8 font-normal">
          <b class="font-semibold text-lg"> Tags: </b>
          
          <a class="text-sm	inline-block px-4 py-1 mx-2 my-2 border border-solid rounded-lg bg-gray-100 border-slate-400 dark:text-white dark:bg-zinc-800"
            href="/tag/nlp"> nlp </a>
          
          <a class="text-sm	inline-block px-4 py-1 mx-2 my-2 border border-solid rounded-lg bg-gray-100 border-slate-400 dark:text-white dark:bg-zinc-800"
            href="/tag/paper"> paper </a>
          
          <a class="text-sm	inline-block px-4 py-1 mx-2 my-2 border border-solid rounded-lg bg-gray-100 border-slate-400 dark:text-white dark:bg-zinc-800"
            href="/tag/model"> model </a>
          
        </div>

        <div class="flex justify-between items-center w-full flex-wrap gap-4 pb-4"><a class="" rel="noreferrer noopener"
            target="_blank" href="https://github.com/zhaospei/zhaospei.github.io/tree/main/_posts/2023-10-06-attention-is-all-you-need.md"><span
              class="font-normal link-hover">Edit on Github</span><span class="inline-block w-3 h-3 ml-2 mt-1 mr-4"><svg
                class="block w-full h-full" viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
                <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2" stroke-linecap="round"
                  stroke-linejoin="round"></path>
              </svg></span></a>
          <div class="flex justify-between items-center flex-wrap gap-8"><a class="cursor-pointer"
              rel="noreferrer noopener" target="_blank"
              onclick="window.open('ttps://twitter.com/intent/tweet?text=https://zhaospei.github.io/');"><span
                class="font-normal link-hover">Tweet this article</span><span
                class="inline-block w-3 h-3 ml-2 mt-1 mr-4"><svg class="block w-full h-full" viewBox="0 0 14 14"
                  fill="none" xmlns="http://www.w3.org/2000/svg">
                  <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2" stroke-linecap="round"
                    stroke-linejoin="round"></path>
                </svg></span></a><a class="cursor-pointer" rel="noreferrer noopener" target="_blank"
              onclick="window.open('http://www.facebook.com/share.php?u=https://zhaospei.github.io/');"><span
                class="font-normal link-hover">Share on Facebook</span><span
                class="inline-block w-3 h-3 ml-2 mt-1 mr-4"><svg class="block w-full h-full" viewBox="0 0 14 14"
                  fill="none" xmlns="http://www.w3.org/2000/svg">
                  <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2" stroke-linecap="round"
                    stroke-linejoin="round"></path>
                </svg></span></a></div>
          <div>
            <div
              class="flex justify-between items-start w-full gap-16 flex-row flew-nowrap pt-16 pl-2 pr-2 border-gray-300 dark:boder-gray-900 border-0 border-t border-solid">
              <div class="relative w-36 h-36">
                <div class="relative overflow-hidden h-full rounded-full">
                  <div class="">
                    <div class="skeleton_skeleton__w5sWr" style="max-width: 100%; height: 100%;"></div>
                  </div>
                  <div class="rounded-full" data-loaded="true"><img alt="Bartosz Zagrodzki" loading="lazy"
                      decoding="async" data-nimg="fill" sizes="100vw" src="/assets/media/author/zhao-img.png"
                      style="position: absolute; height: 100%; width: 100%; inset: 0px; object-fit: cover; object-position: center center; color: transparent;">
                  </div>
                </div>
              </div>
              <div class="" style="flex: 1 1">
                <div class="font-bold text-2xl mb-4">Written by Tuan-Dung Bui</div>
                <p class="text-gray-700 dark:text-gray-200">Tuan-Dung Bui is a blogger, software engineer and the main
                  coordinator of this blog, he has lots of ideas and won't hesitate to use them! He lives in Vietnam.
                </p>
                <div class="mt-4"><a class="" rel="noreferrer noopener" target="_blank"
                    href="/about"><span class="font-normal link-hover">Learn more about
                      Tuan-Dung Bui</span><span class="inline-block w-3 h-3 ml-2 mt-1 mr-4"><svg viewBox="0 0 14 14"
                        fill="none" xmlns="http://www.w3.org/2000/svg">
                        <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2" stroke-linecap="round"
                          stroke-linejoin="round"></path>
                      </svg></span></a></div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>


    <section class="page-navigation flex justify-center items-start flex-col flex-wrap w-full p-0 mt-16 mb-8">
      <h2 class="mb-4 text-3xl max-w-3xl font-bold mb-8">Check my other posts üìö</h2>
      <div class="w-full items-start justify-center grid gap-8" style="grid-template-columns: 1fr 1fr 1fr; grid-gap: 3rem">
        
        <a class="w-full container-hover" href="/nlp/2023/10/06/seq2seq-pytorch/">
          <div class="w-full">
            <div class="mb-4 h-48 content-hover" style="transform: none;">
              <div class="h-full relative">
                <div data-loaded="true"><img class="transition-all" alt="" loading="lazy"
                    decoding="async" data-nimg="fill" sizes="100vw"
                    src="/assets/media/feature/pytorch.png"
                    style="position: absolute; height: 100%; width: 100%; inset: 0px; object-fit: cover; object-position: center center; color: transparent; border-radius: 1rem;">
                </div>
              </div>
            </div>
            <div class="flex items-center justify-start text-gray-600 dark:text-gray-300 text-sm gap-2 px-2"><time>October 6, 2023</time>‚Ä¢
              <span>
              </span></div>
            <h3 class="px-2 py-2 text-black dark:text-white text-2xl font-bold">Tri·ªÉn khai seq2seq v·ªõi Pytorch</h3>
          </div>
        </a>
        
        <a class="w-full container-hover" href="/nlp/2023/10/06/attention-is-all-you-need/">
          <div class="w-full">
            <div class="mb-4 h-48 content-hover" style="transform: none;">
              <div class="h-full relative">
                <div data-loaded="true"><img class="transition-all" alt="" loading="lazy"
                    decoding="async" data-nimg="fill" sizes="100vw"
                    src="/assets/media/feature/transformers.jpg"
                    style="position: absolute; height: 100%; width: 100%; inset: 0px; object-fit: cover; object-position: center center; color: transparent; border-radius: 1rem;">
                </div>
              </div>
            </div>
            <div class="flex items-center justify-start text-gray-600 dark:text-gray-300 text-sm gap-2 px-2"><time>October 6, 2023</time>‚Ä¢
              <span>
              
              11 min read
              </span></div>
            <h3 class="px-2 py-2 text-black dark:text-white text-2xl font-bold">Attention is All You Need</h3>
          </div>
        </a>
        
        <a class="w-full container-hover" href="/nlp/2023/10/05/bert-pretrained-model/">
          <div class="w-full">
            <div class="mb-4 h-48 content-hover" style="transform: none;">
              <div class="h-full relative">
                <div data-loaded="true"><img class="transition-all" alt="" loading="lazy"
                    decoding="async" data-nimg="fill" sizes="100vw"
                    src="/assets/media/feature/bert.png"
                    style="position: absolute; height: 100%; width: 100%; inset: 0px; object-fit: cover; object-position: center center; color: transparent; border-radius: 1rem;">
                </div>
              </div>
            </div>
            <div class="flex items-center justify-start text-gray-600 dark:text-gray-300 text-sm gap-2 px-2"><time>October 5, 2023</time>‚Ä¢
              <span>
              
              2 min read
              </span></div>
            <h3 class="px-2 py-2 text-black dark:text-white text-2xl font-bold">[NLP] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</h3>
          </div>
        </a>
        
      </div>
    </section>

  </article>
</div>

<!-- <script async src="/assets/js/scroll.js"></script> -->
<script async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
  type="text/javascript"></script>
      </main>
    </div>
    <footer class="flex flex-col justify-center items-center w-full bg-gray-200 dark:bg-zinc-800">
        <div
                class="pt-8 pb-4 px-4 flex justify-center items-start w-full flex-col flex-wrap max-w-5xl box-border gap-4">
                <h4 class="text-3xl font-extrabold"> Let's build something together!</h4>
                <p class="max-w-xl">Feel free to reach out if you're looking for a developer, have a question or just
                        want to connect üì≠</p>
                <a rel="noreferrer noopener" target="_blank" href="mailto:dungbuit1k28@gmail.com">
                        <span>dungbuit1k28@gmail.com</span>
                        <span class="inline-block w-3 h-3 mt-2 ml-1"><svg class="block w-full h-full"
                                        viewBox="0 0 14 14" fill="none" xmlns="http://www.w3.org/2000/svg">
                                        <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2"
                                                stroke-linecap="round" stroke-linejoin="round"></path>
                                </svg></span>
                </a>

                <div class="flex justify-between items-center w-full mt-8">
                        <a style="max-width: 3rem;" href="/">
                                <span class="sr-only">home</span>
                                <img class="rounded-full" alt="blog-icon" src="/assets/media/blog-icon.jpg">
                        </a>
                        <div class="justify-center items-center gap-8 hidden md:flex">
                                <a href="/sitemap.xml" rel="noreferrer noopener" target="_blank">
                                        <span> sitemap</span>
                                        <span class="inline-block w-3 h-3 mt-2 ml-1"><svg class="block w-full h-full"
                                                        viewBox="0 0 14 14" fill="none"
                                                        xmlns="http://www.w3.org/2000/svg">
                                                        <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2"
                                                                stroke-linecap="round" stroke-linejoin="round"></path>
                                                </svg></span>
                                </a>
                                <a href="/feed.xml" rel="noreferrer noopener" target="_blank">
                                        <span> rss</span>
                                        <span class="inline-block w-3 h-3 mt-2 ml-1"><svg class="block w-full h-full"
                                                        viewBox="0 0 14 14" fill="none"
                                                        xmlns="http://www.w3.org/2000/svg">
                                                        <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2"
                                                                stroke-linecap="round" stroke-linejoin="round"></path>
                                                </svg></span>
                                </a>
                                <a href="/assets/BuiTuanDung-Resume-1.pdf" rel="noreferrer noopener" target="_blank">
                                        <span> resume</span>
                                        <span class="inline-block w-3 h-3 mt-2 ml-1"><svg class="block w-full h-full"
                                                        viewBox="0 0 14 14" fill="none"
                                                        xmlns="http://www.w3.org/2000/svg">
                                                        <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2"
                                                                stroke-linecap="round" stroke-linejoin="round"></path>
                                                </svg></span>
                                </a>
                                <a href="https://github.com/zhaospei" rel="noreferrer noopener" target="_blank">
                                        <span> github</span>
                                        <span class="inline-block w-3 h-3 mt-2 ml-1"><svg class="block w-full h-full"
                                                        viewBox="0 0 14 14" fill="none"
                                                        xmlns="http://www.w3.org/2000/svg">
                                                        <path d="M13 1L1 13M3 1h10v10" stroke="#448af6" stroke-width="2"
                                                                stroke-linecap="round" stroke-linejoin="round"></path>
                                                </svg></span>
                                </a>
                        </div>

                </div>

                <span class="text-sm text-gray-700 dark:text-gray-200">
                        ¬© 2023 <span class="text-cyan-900 dark:text-cyan-600">real</span> day, <span class="text-cyan-900 dark:text-cyan-600">real</span>
                        time. Dung BTuan. All rights reserved.
                </span>
                <small class="text-gray-600 dark:text-gray-300"> <a href="https://github.com/zhaospei/zhaospei.github.io" target="_blank"
                                rel="noreferrer noopener">polygon</a> theme on <a href="https://jekyllrb.com"
                                target="_blank" rel="noreferrer noopener">jekyll</a> </small>
        </div>

</footer>

<div class="fill-dark fill-white"></div>
    <script src="/assets/js/vanilla-tilt.js"></script>
    <script src="/assets/js/app.js"></script>
  </body>
</html>