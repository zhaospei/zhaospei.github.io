<?xml version="1.0" encoding="UTF-8"?><?xml-stylesheet href="/scripts/pretty-feed-v3.xsl" type="text/xsl"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:h="http://www.w3.org/TR/html4/"><channel><title>TDung&apos;s Site</title><description>Stay hungry, stay foolish</description><link>https://zhaospei.github.io</link><item><title>Why did I decide to pursue a PhD?</title><link>https://zhaospei.github.io/blog/why-i-did-phd</link><guid isPermaLink="true">https://zhaospei.github.io/blog/why-i-did-phd</guid><description>Vì tôi bị NGUUUUUUUUUUUUUU</description><pubDate>Thu, 05 Sep 2024 18:00:00 GMT</pubDate><content:encoded>&lt;p&gt;To be completely honest, I had no particularly strong reasons to do a PhD. I in fact started it believing I would not finish. In the end, I picked based on gut, and my gut said that the PhD would be the most new and exciting possibility.&lt;/p&gt;
&lt;p&gt;If I were to go back and re-justify my choice to myself, with the power of hindsight, I&apos;d say I asked myself &lt;strong&gt;&quot;which option opens the most doors?&quot;&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Pick the option that opens the most doors.&lt;/p&gt;
&lt;/blockquote&gt;
</content:encoded><h:img src="/_astro/phd_progress.DkmBFVAU.jpg"/><enclosure url="/_astro/phd_progress.DkmBFVAU.jpg"/></item><item><title>Triển khai seq2seq với Pytorch</title><link>https://zhaospei.github.io/blog/seq2seq-pytorch</link><guid isPermaLink="true">https://zhaospei.github.io/blog/seq2seq-pytorch</guid><description>Bài viết này giới thiệu cách sử dụng Pytorch để xây dựng mô hình seq2seq và triển khai một ứng dụng dịch máy đơn giản</description><pubDate>Fri, 20 Oct 2023 08:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Bài viết này giới thiệu cách sử dụng &lt;code&gt;Pytorch&lt;/code&gt; để xây dựng mô hình seq2seq và triển khai một ứng dụng dịch máy đơn giản, vui lòng đọc sơ qua bài báo sau trước, &lt;a href=&quot;https://arxiv.org/pdf/1406.1078.pdf&quot;&gt;Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation(2014)&lt;/a&gt;, để hiểu rõ cấu trúc seq2seq hoạt động như thế nào, sau đó đọc bài viết này để đạt được hiệu quả gấp đôi chỉ với một nửa công sức.&lt;/p&gt;
&lt;p&gt;Tôi đã thấy rất nhiều sơ đồ cấu trúc mạng seq2seq và tôi cảm thấy sơ đồ này do Pytorch cung cấp là dễ hiểu nhất.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;src/assets/media/post/seq2seq.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Trước hết, từ hình trên ta có thể thấy rõ ràng, seq2seq cần hoạt động trên ba &quot;biến&quot;, khác với tất cả các cấu trúc mạng mà tôi đã tiếp xúc trước đây. Chúng ta gọi đầu vào cho Encoder là &lt;code&gt;enc_input&lt;/code&gt;, đầu vào cho Decoder là &lt;code&gt;dec_input&lt;/code&gt; và đầu ra của Decoder là &lt;code&gt;dec_output&lt;/code&gt;. Phần sau đây sử dụng một ví dụ cụ thể để minh họa cho toàn bộ quy trình thực hiện của seq2seq.&lt;/p&gt;
&lt;p&gt;Hình bên dưới là cấu trúc Encoder cấu tạo từ LTSM, đầu vào là từng chữ cái (bao gồm cả dấu cách) trong &quot;go away&quot;, chúng ta cần thông tin của &lt;code&gt;hidden state&lt;/code&gt; ở thời điểm cuối cùng, bao gồm $$h_{t}$$ và $$c_{t}$$.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;src/assets/media/post/LSTM_Encoder.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Sau đó sử dụng đầu ra gồm $$h_{t}$$ và $$c_{t}$$ làm đầu vào cho hidden state đầu tiên của Decoder là $$h_{0}, c_{0}$$, như hình bên dưới. Đồng thời, lớp đầu vào (&lt;code&gt;input layer&lt;/code&gt;) đầu của Decoder sẽ được nhập một ký tự đại diện cho phần đầu của câu (Do người dùng tự định nghĩa có thể là &quot;&amp;lt;SOS&amp;gt;&quot;, &quot;\t&quot;, &quot;S&quot;, .... đều được chấp nhận. Ở đây, tôi lấy &quot;\t&quot; làm ví dụ), và sau đó nhận đầu ra &quot;m&quot;, và một hidden state mới $$h_{1}$$ và $$c_{1}$$&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;src/assets/media/post/gFRVkq.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Sau đó lấy $$h_{1}$$, $$c_{1}$$ và &quot;m&quot; làm đầu vào, nhận đầu ra là &quot;a&quot;, và một hidden state mới $$h_{2}$$ và $$c_{2}$$&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;src/assets/media/post/gFR1B9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Lặp lại các bước trên cho đến khi ký tự kết thúc của câu cuối cùng được xuất ra (do người dùng xác định, &quot;&amp;lt;EOS&amp;gt;&quot;, &quot;\n&quot;, &quot;E&quot;, ... ở đây tôi lấy &quot;\n&quot; làm ví dụ).&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;src/assets/media/post/gFRGA1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Trong phần Decoder, bạn sẽ có thể có những câu hỏi sau và tôi sẽ trả lời chúng theo hiểu biết cá nhân.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tôi phài làm như thế nào nếu Decoder không thể dừng lại trong quá trình đào tạo? Tức là ký tự kết thúc của câu không bao giờ được đưa ra.
&lt;ul&gt;
&lt;li&gt;Đầu tiên, trong quá trình huấn luyện, &lt;strong&gt;độ dài của câu mà Decoder sẽ xuất ra sẽ được biết&lt;/strong&gt;. Giả sử thời điểm hiện tại đã đến ký tự cuối cùng của độ dài câu và dự đoán không phải là ký tự kết thúc thì cũng không sao, chỉ dừng lại ở đây và tính toán tổn thất.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tôi phải làm như thế nào nếu Decocder không thể dừng lại trong quá trình kiểm tra? Ví dụ, dự đoán là &quot;wasd s w \n sdsw \n ...... (tiếp tục sinh từ)&quot;
&lt;ul&gt;
&lt;li&gt;Nó sẽ không dừng lại, vì trong quá trình kiểm tra, Decoder cũng có đầu vào, nhưng đầu vào này có rất nhiều placeholder vô nghĩa, chẳng hạn rất nhiều &quot;&amp;lt;pad&amp;gt;&quot;. Vì Decoder phải có đầu ra có độ dài hữu hạn. Khi đó bạn chỉ lấy tất cả các ký tự trước ký tự kết thúc đầu tiên. Ví dụ trên kết quả dự đoán cuối cùng là &quot;wasd s w&quot;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Mối quan hệ giữa đầu vào và đầu ra của Decoder, tức là &lt;code&gt;dec_input&lt;/code&gt; và &lt;code&gt;dec_output&lt;/code&gt; là gì?
&lt;ul&gt;
&lt;li&gt;Trong quá trình huấn luyện, bất kể Decoder sinh ra kí tự nào tại thời điểm hiện tại, Decoder tại thời điểm tiếp theo sẽ nhập theo &quot;kế hoạch&quot; ban đầu. Ví dụ: giả sử &lt;code&gt;dec_input = &quot;\twasted&quot;&lt;/code&gt;, sau khi nhập &quot;\t&quot; lần đầu, Decoder sẽ xuất ra chữ &quot;m&quot;, ghi lại thôi, nó sẽ không ảnh hưởng đến thời điểm tiếp theo khi Decoder tiếp tục nhập chữ &quot;w&quot;.&lt;/li&gt;
&lt;li&gt;Trong quá trình valid và testing, đầu ra của Decoder tại mỗi thời điểm sẽ ảnh hưởng đến đầu vào, vì trong quá trình valid và testing, mạng không thể nhìn thấy kết quả nên chỉ tiến hành theo vòng lặp. Ví dụ, bây giờ tôi muốn dịch từ &quot;wasted&quot; trong tiếng anh sang tiếng &quot;lãng phí&quot; trong tiếng việt. Sau đó, Decoder bắt đầu với việc nhập ký tự &quot;\t&quot;, nhận kết quả đầu ra nếu là &quot;m&quot;, tại thời điểm tiếp theo, Decoder sẽ nhập &quot;m&quot;, nhận đầu ra, nếu là &quot;a&quot;, sau đó nhận &quot;a&quot; là đầu vào, nhận đầu ra, ... và cứ thế cho đến khi gặp kí tự cuối cùng hoặc đạt độ dài tối đa. Mặc dù từ sinh ra không đúng nhưng mong đợi nhưng phải chấp nhận thôi :smiley:.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hơi lạc đề một chút, cá nhân tôi nghĩ seq2seq rất giống với AutoEncoder.&lt;/p&gt;
&lt;h2&gt;Hãy bắt đầu giải thích mã&lt;/h2&gt;
&lt;p&gt;Đầu tiên, import thư viện, ở đây tôi dùng &apos;S&apos; làm ký tự bắt đầu và &apos;E&apos; làm ký tự kết thúc, nếu đầu vào hoặc đầu ra quá ngắn, tôi sẽ padding nó bằng ký tự &apos;?&apos;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# code by Tae Hwan Jung(Jeff Jung) @graykode, modify by zhaospei
import torch
import numpy as np
import torch.nn as nn
import torch.utils.data as Data

device = torch.device(&apos;cuda&apos; if torch.cuda.is_available() else &apos;cpu&apos;)
# S: Symbol that shows starting of decoding input
# E: Symbol that shows starting of decoding output
# ?: Symbol that will fill in blank sequence if current batch data size is short than n_step
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Xác định tập dữ liệu và các tham số tập dữ liệu ở đây rất đơn giản, có thể coi như một công việc dịch thuật, chỉ là dịch tiếng Anh sang tiếng Anh.
`n_step`` là độ dài của từ dài nhất, tất cả các từ khác không đủ dài sẽ được padding bằng &apos;?&apos;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;letter = [c for c in &apos;SE?abcdefghijklmnopqrstuvwxyz&apos;]
letter2idx = {n: i for i, n in enumerate(letter)}

seq_data = [[&apos;man&apos;, &apos;women&apos;], [&apos;black&apos;, &apos;white&apos;], [&apos;king&apos;, &apos;queen&apos;], [&apos;girl&apos;, &apos;boy&apos;], [&apos;up&apos;, &apos;down&apos;], [&apos;high&apos;, &apos;low&apos;]]

# Seq2Seq Parameter
n_step = max([max(len(i), len(j)) for i, j in seq_data]) # max_len(=5)
n_hidden = 128
n_class = len(letter2idx) # classfication problem
batch_size = 3
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sau đây là xử lý dữ liệu, trước tên là xử lý các từ không đủ độ dài, sử dụng ký tự &apos;?&apos; để padding; Sau đó thêm flag kết thúc &apos;E&apos; vào cuối dữ liệu đầu vào của Encoder, thêm flag bắt đầu &apos;S&apos; vào đầu dữ liệu đầu vào của Decoder và flag kết thúc &apos;E&apos; vào cuối dữ liệu đầu ra của Decoder. Xem hình phía dưới để hiểu rõ hơn.&lt;/p&gt;
&lt;p&gt;{% include image.html url=&quot;/assets/media/post/gFRU1O.png&quot; %}&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;def make_data(seq_data):
    enc_input_all, dec_input_all, dec_output_all = [], [], []

    for seq in seq_data:
        for i in range(2):
            seq[i] = seq[i] + &apos;?&apos; * (n_step - len(seq[i])) # &apos;man??&apos;, &apos;women&apos;

        enc_input = [letter2idx[n] for n in (seq[0] + &apos;E&apos;)] # [&apos;m&apos;, &apos;a&apos;, &apos;n&apos;, &apos;?&apos;, &apos;?&apos;, &apos;E&apos;]
        dec_input = [letter2idx[n] for n in (&apos;S&apos; + seq[1])] # [&apos;S&apos;, &apos;w&apos;, &apos;o&apos;, &apos;m&apos;, &apos;e&apos;, &apos;n&apos;]
        dec_output = [letter2idx[n] for n in (seq[1] + &apos;E&apos;)] # [&apos;w&apos;, &apos;o&apos;, &apos;m&apos;, &apos;e&apos;, &apos;n&apos;, &apos;E&apos;]

        enc_input_all.append(np.eye(n_class)[enc_input])
        dec_input_all.append(np.eye(n_class)[dec_input])
        dec_output_all.append(dec_output) # not one-hot

    # make tensor
    return torch.Tensor(enc_input_all), torch.Tensor(dec_input_all), torch.LongTensor(dec_output_all)

&apos;&apos;&apos;
enc_input_all: [6, n_step+1 (because of &apos;E&apos;), n_class]
dec_input_all: [6, n_step+1 (because of &apos;S&apos;), n_class]
dec_output_all: [6, n_step+1 (because of &apos;E&apos;)]
&apos;&apos;&apos;
enc_input_all, dec_input_all, dec_output_all = make_data(seq_data)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ví có ba dữ liệu trả về ở đây, vì vậy cần tùy chỉnh Dataset, cụ thể là kế thừa lớp &lt;code&gt;torch.utils.data.Dataset&lt;/code&gt;, sau đó triển khai các phương thức &lt;code&gt;__len__&lt;/code&gt; và &lt;code&gt;__getitem__&lt;/code&gt; bên trong.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class TranslateDataSet(Data.Dataset):
    def __init__(self, enc_input_all, dec_input_all, dec_output_all):
        self.enc_input_all = enc_input_all
        self.dec_input_all = dec_input_all
        self.dec_output_all = dec_output_all
    
    def __len__(self): # return dataset size
        return len(self.enc_input_all)
    
    def __getitem__(self, idx):
        return self.enc_input_all[idx], self.dec_input_all[idx], self.dec_output_all[idx]

loader = Data.DataLoader(TranslateDataSet(enc_input_all, dec_input_all, dec_output_all), batch_size, True)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Xác định mô hình seq2seq bên dưới, tôi sử dụng RNN đơn giản làm Encoder và Decoder. Nếu bạn đã quen thuộc với RNN thì thực sự không có gì phải nói về việc xác định cấu trúc mạng, tôi cũng đã viết nhận xét rất rõ ràng, bao gồm cả những thay đổi về kích thước dữ liệu.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Model
class Seq2Seq(nn.Module):
    def __init__(self):
        super(Seq2Seq, self).__init__()
        self.encoder = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5) # encoder
        self.decoder = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5) # decoder
        self.fc = nn.Linear(n_hidden, n_class)

    def forward(self, enc_input, enc_hidden, dec_input):
        # enc_input(=input_batch): [batch_size, n_step+1, n_class]
        # dec_inpu(=output_batch): [batch_size, n_step+1, n_class]
        enc_input = enc_input.transpose(0, 1) # enc_input: [n_step+1, batch_size, n_class]
        dec_input = dec_input.transpose(0, 1) # dec_input: [n_step+1, batch_size, n_class]

        # h_t : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]
        _, h_t = self.encoder(enc_input, enc_hidden)
        # outputs : [n_step+1, batch_size, num_directions(=1) * n_hidden(=128)]
        outputs, _ = self.decoder(dec_input, h_t)

        model = self.fc(outputs) # model : [n_step+1, batch_size, n_class]
        return model

model = Seq2Seq().to(device)
criterion = nn.CrossEntropyLoss().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sau đây là phần training. Vì giá trị đầu ra là dữ liệu ba chiều nên việc tính toán loss đòi hỏi phải tính toán từng mẫu riêng biệt, do đó có mã vòng for sau đây.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for epoch in range(5000):
  for enc_input_batch, dec_input_batch, dec_output_batch in loader:
      # make hidden shape [num_layers * num_directions, batch_size, n_hidden]
      h_0 = torch.zeros(1, batch_size, n_hidden).to(device)

      (enc_input_batch, dec_intput_batch, dec_output_batch) = (enc_input_batch.to(device), dec_input_batch.to(device), dec_output_batch.to(device))
      # enc_input_batch : [batch_size, n_step+1, n_class]
      # dec_intput_batch : [batch_size, n_step+1, n_class]
      # dec_output_batch : [batch_size, n_step+1], not one-hot
      pred = model(enc_input_batch, h_0, dec_intput_batch)
      # pred : [n_step+1, batch_size, n_class]
      pred = pred.transpose(0, 1) # [batch_size, n_step+1(=6), n_class]
      loss = 0
      for i in range(len(dec_output_batch)):
          # pred[i] : [n_step+1, n_class]
          # dec_output_batch[i] : [n_step+1]
          loss += criterion(pred[i], dec_output_batch[i])
      if (epoch + 1) % 1000 == 0:
          print(&apos;Epoch:&apos;, &apos;%04d&apos; % (epoch + 1), &apos;cost =&apos;, &apos;{:.6f}&apos;.format(loss))
          
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Như có thể thấy từ mã testing bên dưới, trong quá trình testing, đầu vào của Decoder là một phần giữ chỗ vô nghĩa và độ dài của vị trí bị chiếm giữ là độ dài tối đa &lt;code&gt;n_step&lt;/code&gt;. Và tìm vị trí của dấu kết thúc đầu tiên ở đầu ra, chặn tất cả các ký tự trước nó.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Test
def translate(word):
    enc_input, dec_input, _ = make_data([[word, &apos;?&apos; * n_step]])
    enc_input, dec_input = enc_input.to(device), dec_input.to(device)
    # make hidden shape [num_layers * num_directions, batch_size, n_hidden]
    hidden = torch.zeros(1, 1, n_hidden).to(device)
    output = model(enc_input, hidden, dec_input)
    # output : [n_step+1, batch_size, n_class]

    predict = output.data.max(2, keepdim=True)[1] # select n_class dimension
    decoded = [letter[i] for i in predict]
    translated = &apos;&apos;.join(decoded[:decoded.index(&apos;E&apos;)])

    return translated.replace(&apos;?&apos;, &apos;&apos;)

print(&apos;test&apos;)
print(&apos;man -&amp;gt;&apos;, translate(&apos;man&apos;))
print(&apos;mans -&amp;gt;&apos;, translate(&apos;mans&apos;))
print(&apos;king -&amp;gt;&apos;, translate(&apos;king&apos;))
print(&apos;black -&amp;gt;&apos;, translate(&apos;black&apos;))
print(&apos;up -&amp;gt;&apos;, translate(&apos;up&apos;))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Mã hoàn chỉnh như sau&lt;/h2&gt;
&lt;p&gt;Phần thực thi bạn có thể xem tại notebook trên kaggle tại &lt;a href=&quot;https://www.kaggle.com/code/overvisual/seq2seq-torch?scriptVersionId=145596925&quot;&gt;https://www.kaggle.com/code/overvisual/seq2seq-torch?scriptVersionId=145596925&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# code by Tae Hwan Jung(Jeff Jung) @graykode, modify by zhaospei
import torch
import numpy as np
import torch.nn as nn
import torch.utils.data as Data

device = torch.device(&apos;cuda&apos; if torch.cuda.is_available() else &apos;cpu&apos;)
# S: Symbol that shows starting of decoding input
# E: Symbol that shows starting of decoding output
# ?: Symbol that will fill in blank sequence if current batch data size is short than n_step

letter = [c for c in &apos;SE?abcdefghijklmnopqrstuvwxyz&apos;]
letter2idx = {n: i for i, n in enumerate(letter)}

seq_data = [[&apos;man&apos;, &apos;women&apos;], [&apos;black&apos;, &apos;white&apos;], [&apos;king&apos;, &apos;queen&apos;], [&apos;girl&apos;, &apos;boy&apos;], [&apos;up&apos;, &apos;down&apos;], [&apos;high&apos;, &apos;low&apos;]]

# Seq2Seq Parameter
n_step = max([max(len(i), len(j)) for i, j in seq_data]) # max_len(=5)
n_hidden = 128
n_class = len(letter2idx) # classfication problem
batch_size = 3

def make_data(seq_data):
    enc_input_all, dec_input_all, dec_output_all = [], [], []

    for seq in seq_data:
        for i in range(2):
            seq[i] = seq[i] + &apos;?&apos; * (n_step - len(seq[i])) # &apos;man??&apos;, &apos;women&apos;

        enc_input = [letter2idx[n] for n in (seq[0] + &apos;E&apos;)] # [&apos;m&apos;, &apos;a&apos;, &apos;n&apos;, &apos;?&apos;, &apos;?&apos;, &apos;E&apos;]
        dec_input = [letter2idx[n] for n in (&apos;S&apos; + seq[1])] # [&apos;S&apos;, &apos;w&apos;, &apos;o&apos;, &apos;m&apos;, &apos;e&apos;, &apos;n&apos;]
        dec_output = [letter2idx[n] for n in (seq[1] + &apos;E&apos;)] # [&apos;w&apos;, &apos;o&apos;, &apos;m&apos;, &apos;e&apos;, &apos;n&apos;, &apos;E&apos;]

        enc_input_all.append(np.eye(n_class)[enc_input])
        dec_input_all.append(np.eye(n_class)[dec_input])
        dec_output_all.append(dec_output) # not one-hot

    # make tensor
    return torch.Tensor(enc_input_all), torch.Tensor(dec_input_all), torch.LongTensor(dec_output_all)

&apos;&apos;&apos;
enc_input_all: [6, n_step+1 (because of &apos;E&apos;), n_class]
dec_input_all: [6, n_step+1 (because of &apos;S&apos;), n_class]
dec_output_all: [6, n_step+1 (because of &apos;E&apos;)]
&apos;&apos;&apos;
enc_input_all, dec_input_all, dec_output_all = make_data(seq_data)

class TranslateDataSet(Data.Dataset):
    def __init__(self, enc_input_all, dec_input_all, dec_output_all):
        self.enc_input_all = enc_input_all
        self.dec_input_all = dec_input_all
        self.dec_output_all = dec_output_all
    
    def __len__(self): # return dataset size
        return len(self.enc_input_all)
    
    def __getitem__(self, idx):
        return self.enc_input_all[idx], self.dec_input_all[idx], self.dec_output_all[idx]

loader = Data.DataLoader(TranslateDataSet(enc_input_all, dec_input_all, dec_output_all), batch_size, True)

# Model
class Seq2Seq(nn.Module):
    def __init__(self):
        super(Seq2Seq, self).__init__()
        self.encoder = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5) # encoder
        self.decoder = nn.RNN(input_size=n_class, hidden_size=n_hidden, dropout=0.5) # decoder
        self.fc = nn.Linear(n_hidden, n_class)

    def forward(self, enc_input, enc_hidden, dec_input):
        # enc_input(=input_batch): [batch_size, n_step+1, n_class]
        # dec_inpu(=output_batch): [batch_size, n_step+1, n_class]
        enc_input = enc_input.transpose(0, 1) # enc_input: [n_step+1, batch_size, n_class]
        dec_input = dec_input.transpose(0, 1) # dec_input: [n_step+1, batch_size, n_class]

        # h_t : [num_layers(=1) * num_directions(=1), batch_size, n_hidden]
        _, h_t = self.encoder(enc_input, enc_hidden)
        # outputs : [n_step+1, batch_size, num_directions(=1) * n_hidden(=128)]
        outputs, _ = self.decoder(dec_input, h_t)

        model = self.fc(outputs) # model : [n_step+1, batch_size, n_class]
        return model

model = Seq2Seq().to(device)
criterion = nn.CrossEntropyLoss().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(5000):
  for enc_input_batch, dec_input_batch, dec_output_batch in loader:
      # make hidden shape [num_layers * num_directions, batch_size, n_hidden]
      h_0 = torch.zeros(1, batch_size, n_hidden).to(device)

      (enc_input_batch, dec_intput_batch, dec_output_batch) = (enc_input_batch.to(device), dec_input_batch.to(device), dec_output_batch.to(device))
      # enc_input_batch : [batch_size, n_step+1, n_class]
      # dec_intput_batch : [batch_size, n_step+1, n_class]
      # dec_output_batch : [batch_size, n_step+1], not one-hot
      pred = model(enc_input_batch, h_0, dec_intput_batch)
      # pred : [n_step+1, batch_size, n_class]
      pred = pred.transpose(0, 1) # [batch_size, n_step+1(=6), n_class]
      loss = 0
      for i in range(len(dec_output_batch)):
          # pred[i] : [n_step+1, n_class]
          # dec_output_batch[i] : [n_step+1]
          loss += criterion(pred[i], dec_output_batch[i])
      if (epoch + 1) % 1000 == 0:
          print(&apos;Epoch:&apos;, &apos;%04d&apos; % (epoch + 1), &apos;cost =&apos;, &apos;{:.6f}&apos;.format(loss))
          
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()
    
# Test
def translate(word):
    enc_input, dec_input, _ = make_data([[word, &apos;?&apos; * n_step]])
    enc_input, dec_input = enc_input.to(device), dec_input.to(device)
    # make hidden shape [num_layers * num_directions, batch_size, n_hidden]
    hidden = torch.zeros(1, 1, n_hidden).to(device)
    output = model(enc_input, hidden, dec_input)
    # output : [n_step+1, batch_size, n_class]

    predict = output.data.max(2, keepdim=True)[1] # select n_class dimension
    decoded = [letter[i] for i in predict]
    translated = &apos;&apos;.join(decoded[:decoded.index(&apos;E&apos;)])

    return translated.replace(&apos;?&apos;, &apos;&apos;)

print(&apos;test&apos;)
print(&apos;man -&amp;gt;&apos;, translate(&apos;man&apos;))
print(&apos;mans -&amp;gt;&apos;, translate(&apos;mans&apos;))
print(&apos;king -&amp;gt;&apos;, translate(&apos;king&apos;))
print(&apos;black -&amp;gt;&apos;, translate(&apos;black&apos;))
print(&apos;up -&amp;gt;&apos;, translate(&apos;up&apos;))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2&gt;Tham khảo&lt;/h2&gt;
&lt;p&gt;[1] &lt;a href=&quot;https://www.kaggle.com/code/overvisual/seq2seq-torch?scriptVersionId=145596925&quot;&gt;https://www.kaggle.com/code/overvisual/seq2seq-torch?scriptVersionId=145596925&lt;/a&gt;&lt;/p&gt;
</content:encoded><h:img src="/_astro/pytorch.BgBWJuWz.png"/><enclosure url="/_astro/pytorch.BgBWJuWz.png"/></item><item><title>Attention is All You Need</title><link>https://zhaospei.github.io/blog/attention-is-all-you-need</link><guid isPermaLink="true">https://zhaospei.github.io/blog/attention-is-all-you-need</guid><description>Transformer là mô hình seq2seq được Google Brain đề xuất trong một bài báo xuất bản vào cuối năm 2017</description><pubDate>Fri, 06 Oct 2023 08:00:00 GMT</pubDate><content:encoded>&lt;blockquote&gt;
&lt;p&gt;[!NOTE]
&lt;code&gt;Transformer&lt;/code&gt; là mô hình seq2seq được Google Brain đề xuất trong một bài báo xuất bản vào cuối năm 2017. Giờ đây, nó đã đạt được nhiều ứng dụng và tiện ích mở rộng và &lt;code&gt;BERT&lt;/code&gt; là mô hình ngôn ngữ được đào tạo trước có nguồn gốc từ Transformer.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Việc đào tạo &lt;code&gt;RNN&lt;/code&gt; truyền thống là nối tiếp và nó phải đợi từ hiện tại được xử lý trước khi có thể xử lý từ tiếp theo. Transformer được huấn luyện song song, tức là tất cả các từ đều được huấn luyện cùng một lúc, điêu này làm tăng đáng kể hiệu quả tính toán.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;src/assets/media/post/transformer-model-architecture.png&quot; alt=&quot;Kiến trúc mô hình Transformer&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;Self-Attention&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Scaled Dot-Product Attention&lt;/code&gt; là tích chấm chuẩn hóa Attention, chi tiết cụ thể được thể hiện trong hình.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;src/assets/media/post/attention.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;$$
Attention(Q,K,V)=softmax(\dfrac{QK^T}{\sqrt{d_k}})V
$$&lt;/p&gt;
&lt;p&gt;Sự chú ý của nhiều đầu vào sử dụng nhiều bộ trọng số (&lt;code&gt;weights&lt;/code&gt;) ($$ W_q,W_k,W_v $$), ghép lại cho ra kết quả cuối cùng.&lt;/p&gt;
&lt;p&gt;$$
MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O
$$&lt;/p&gt;
&lt;p&gt;trong đó&lt;/p&gt;
&lt;p&gt;$$
head_i=Attention(QW^Q_i,KW^K_i,VW^V_i)
$$&lt;/p&gt;
&lt;p&gt;Trong đó $$h = 8$$, $$ d_q=d_k=d_v=d_{model}/4=64 $$.&lt;/p&gt;
&lt;h2&gt;Encoder&lt;/h2&gt;
&lt;p&gt;Encoder được xếp chồng lên nhau bởi sáu lớp giống hệt nhau, mỗi lớp bao gồm hai lớp con - cơ chế tự chú ý nhiều đầu (&lt;code&gt;multi-head self-attention mechanism&lt;/code&gt;) và mạng nơ ron vị trí chuyển tiếp được kết nối đầy đủ (&lt;code&gt;position-wise fully connected feed-forward network&lt;/code&gt;). Mỗi lớp con sử dụng các kết nối dư (&lt;code&gt;residual connection&lt;/code&gt;) và lớp chuẩn hóa (&lt;code&gt;layer normalization&lt;/code&gt;). Kích thước đầu ra của các lớp con là $$ d_{model} = 512 $$.&lt;/p&gt;
&lt;p&gt;Đầu ra của lớp con có thể được biểu diễn dưới dạng:&lt;/p&gt;
&lt;p&gt;$$
LayerNorm(x+Sublayer(x))
$$&lt;/p&gt;
&lt;h3&gt;position-wise fully connected feed-forward network&lt;/h3&gt;
&lt;p&gt;Mạng nơ-ron chuyển tiếp được kết nối đầy đủ (&lt;code&gt;position-wise fully connected feed-forward network&lt;/code&gt;) bao gồm hai phép biến đổi tuyến tính với kích hoạt &lt;code&gt;ReLU&lt;/code&gt; ở giữa.&lt;/p&gt;
&lt;p&gt;$$
FFN(x)=ReLU(xW_1+b_1)W_2+b_2
$$&lt;/p&gt;
&lt;p&gt;Kích thước lớp bên trong (inner-layer) là 2048.&lt;/p&gt;
&lt;h3&gt;residual connection&lt;/h3&gt;
&lt;p&gt;Mạng dư (&lt;code&gt;Residual Network&lt;/code&gt;), các kết nối tắt có khả năng bỏ qua một hoặc nhiều lớp, do sự tồn tại của kết nối tắt nên hiệu suất của mạng sâu (có nhiều lớp) không kém hơn so với các mạng nông (mạng có ít lớp). Phương pháp này giải quyết vấn đề suy thoái do các lớp chập xếp chồng lên nhau gây ra, số lượng lớp của mạng nơ-ron tích chập đã được tăng lên rất nhiều lên hàng trăm lớp, và cải thiện đáng kể hiệu suất của mạng thần kinh tích chập (&lt;code&gt;resnet&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;src/assets/media/post/resnet.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;h3&gt;Batch Norm và Layer Norm&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;src/assets/media/post/normalization.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Đặt kích thước hình ảnh đầu vào là $$ [N, C, H, W] $$:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Batch Norm&lt;/code&gt;, chuẩn hóa theo từng batch NHW, là để chuẩn hóa đầu vào từng kênh đơn, đều này không hiệu quả đối với &lt;code&gt;batch-size&lt;/code&gt; nhỏ.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Layer Norm&lt;/code&gt;, chuẩn hóa theo từng layer CHW, là để chuẩn hóa đầu vào ở mỗi độ sâu, chủ yếu có tác dụng rõ ràng trên RNN.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sự hiểu biết cá nhân:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dành cho CNN, nếu hạt nhân tích chập quét hình ảnh đầu vào, nó được tính là thao tác tích chập, cần có tổng thao tác batchsize. Do đó, chuẩn hóa cần được thực hiện theo batch.&lt;/li&gt;
&lt;li&gt;Dành cho RNN, batchsize thường là 1, số vòng lặp là số độ dài đầu vào (số channel). Do đó, chuẩn hóa cần được thực hiện theo channel.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Toàn bộ kiến trúc Encoder&lt;/h2&gt;
&lt;h3&gt;input &amp;amp; positional embedding&lt;/h3&gt;
&lt;p&gt;$$
X=Embedding-Lookup(X)+Positional-Encoding
$$&lt;/p&gt;
&lt;h3&gt;multi-head attention&lt;/h3&gt;
&lt;p&gt;$$
Q=Linear_q(X)=XW_q
$$&lt;/p&gt;
&lt;p&gt;$$
K=Linear_q(X)=XW_k
$$&lt;/p&gt;
&lt;p&gt;$$
V=Linear_v(X)=XW_v
$$&lt;/p&gt;
&lt;p&gt;$$
X_{attention}=Self-Attention(Q,K,V)
$$&lt;/p&gt;
&lt;h3&gt;add &amp;amp; norm&lt;/h3&gt;
&lt;p&gt;$$
X_{attention}=LayerNorm(X+X_{attention})
$$&lt;/p&gt;
&lt;h3&gt;feed forward&lt;/h3&gt;
&lt;p&gt;$$
X_{hidden}=Linear(ReLU(Linear(X_{attention})))
$$&lt;/p&gt;
&lt;h3&gt;add &amp;amp; norm&lt;/h3&gt;
&lt;p&gt;$$
X_{hidden}=LayerNorm(X_{hidden}+X_{attention})
$$&lt;/p&gt;
&lt;p&gt;&lt;code&gt;multi-head attention&lt;/code&gt; trong &lt;code&gt;Encoder&lt;/code&gt; là một cơ chế tự chú ý (&lt;code&gt;self-attention mechanism&lt;/code&gt;). $$k$$, $$q$$ và $$v$$ trong cơ chế tự chú ý đều xuất phát từ cùng một vị trí, mỗi lớp của Encoder có thể nhận được tất cả vị trí của lớp trước.&lt;/p&gt;
&lt;h2&gt;Decoder&lt;/h2&gt;
&lt;p&gt;Decoder bao gồm sáu lớp giống hệt xếp chồng lên nhau; trong Multi-head Attention, $$q$$ được đến từ lớp trước đó của Decoder, k và v đến từ đầu ra của Encoder. Điều cho phép mỗi vị trí trong Decoder nhận biết được tất cả các vị trí của chuỗi đầu vào.&lt;/p&gt;
&lt;p&gt;Ngoài hai lớp con trong Encoder, Decoder thêm một lớp con mới xử lý đầu ra của Encoder - &lt;code&gt;masked multi-head self-attention mechanism&lt;/code&gt;. Encoder trong seq2seq truyền thống sử dụng mô hình RNN, vì vậy nếu các từ tại thời điểm t được nhập vào trong quá trình huấn luyện thì mô hình sẽ không thể nhìn thấy các từ trước đó vào các thời điểm trong tương lai, bởi vì RNN hoạt động theo thời gian và chỉ khi thao tác tại thời điểm t hoàn thành, chỉ khi đó ta mới có thể nhìn thấy các từ tại thời điểm t + 1. Và Transformer Decoder đã không sử dụng RNN, thay đổi sang Self-Attention, điều này tạo ra một vấn đề, trong quá trình huấn luyện, toàn bộ ground truth đã được hiển thị với Decoder, điều này rõ ràng là sai, chúng ta cần phải thực hiện một số xử lý trên đầu vào của Decoder, quá trình này được gọi là &lt;code&gt;Mask&lt;/code&gt; - Đặt tất cả các giá trị sau postion thành $$-\infty $$ trước khi vào softmax.&lt;/p&gt;
&lt;p&gt;Ví dụ, ground truth của Decoder là &quot;&amp;lt;start&amp;gt; I am fine&quot;, chúng ta cho câu này vào bộ Decoder, sau khi Word Embedding và Positional Encoding, thực hiện phép biến đổi tuyến tính bậc 3 trên ma trận thu được $$(W_Q,W_K,W_V)$$ Sau đó thực hiện self-attention, trước tiên, nhận Scaled Scores thông qua $$\dfrac{Q×K^T}{\sqrt{d_k}}$$, bước tiếp theo rất quan trọng, chúng ta cần mask theo Scaled Scores, ví dụ, khi nhập &quot;I&quot;, hiện tại mô hình chỉ biết thông tin của tất cả các từ trước đó của &quot;I&quot;, tức thông tin của &quot;&amp;lt;start&amp;gt;&quot; và &quot;I&quot;, không được phép biết được thông tin của các từ sau &quot;I&quot;. Lý do rất đơn giản, khi dự đoán là chúng ta dự đoán theo thứ tự từng chữ, làm sao có thể biết được thông tin của những từ sau trước khi dự đoán xong từ này? Mask rất đơn giản, đầu tiên tạo một ma trận có tam giác hoàn toàn phía dưới bằng 0 và tam giác hoàn tòan phía trên bằng âm vô cùng, sau đó chỉ cần thêm nó vào Scaled Scores.&lt;/p&gt;
&lt;h2&gt;Word Embedding và Positional Embedding&lt;/h2&gt;
&lt;h3&gt;Word Embedding&lt;/h3&gt;
&lt;p&gt;Phần nhúng từ sử dụng nhúng từ có thể học được, kích thước của nó là $$d_{model}$$.
Hình thức mã hóa &lt;code&gt;One-hot&lt;/code&gt; ngắn gọn, nhưng quá thưa thớt, nó không phản ánh sự giống nhau về nghĩa của từ. Vì vậy hãy sử dụng &lt;code&gt;the Skip-Gram Model&lt;/code&gt; hoặc &lt;code&gt;continuous bag of words model&lt;/code&gt; hoặc các nhúng từ khác có thể học được khác.&lt;/p&gt;
&lt;h3&gt;Positional Embedding&lt;/h3&gt;
&lt;p&gt;Bởi vì mô hình không bao gồm các cấu trúc tuần hoàn, vì vậy nắm bắt được các thông tin thứ tự tuần tự, ví dụ nếu $$K$$ và $$V$$ được xóa trộn theo từng hàng thì kết quả sau Attention sẽ giống nhau. Tuy nhiên, thông tin tuần tự rất quan trọng và thể hiện cấu trúc toàn cầu, do đó thông tin position tuyệt đối và tương đối của token tuần tự phải được sử dụng.&lt;/p&gt;
&lt;h4&gt;Nhúng vị trí tùy chinh&lt;/h4&gt;
&lt;p&gt;Một ý tưởng là lấy một số trong khoảng $$[0, 1]$$ và gán nó cho mỗi từ, trong đó 0 được trao cho từ đầu tiên, 1 cho từ cuối cùng, công thức cụ thể là $$PE=\dfrac{pos}{T−1}$$. Vấn đề của việc gán theo công thức này là nó bị phụ thuộc và kích thước của văn bản. Tức
là văn bản có số kí tự là 30. Khi đó theo công thức trên, thì khoảng cách giữa hai từ sẽ là 0.0333. Khi văn bản khác có số lượng kí từ &amp;lt; 30, thì con số 0.0333 vẫn mô tả đúng vị trí tương đối giữa chúng, tuy nhiên với văn bản &amp;gt; 30, ví dụ 90 thì 0.0333 đang gộp khoảng cách thực tế đang được phân tách bởi hai ký tự. Điều này rõ ràng là không phù hợp, vì sự khác biệt giống nhau không có nghĩa là giống nhau trong các câu khác nhau.&lt;/p&gt;
&lt;p&gt;Một ý tưởng khác là gắn tuyến tính mỗi bước theo thời gian, nghĩa là từ đầu tiên được gán là 1, từ thứ hai được gán là 2, ... Phương pháp này cũng có những vấn đề lớn: 1. Nó lớn hơn giá trị nhúng từ thông từ, có thể gây nhiễu cho mô hình; 2. Ký tự cuối cùng lớn hơn nhiều ký tự đầu tiên, sau khi hợp nhất với các từ nhúng, giá trị của các đặc trưng sẽ bị sai lệch.&lt;/p&gt;
&lt;h4&gt;Nhúng từ vị trí &quot;lý tuởng&quot;&lt;/h4&gt;
&lt;p&gt;Một lý tưởng là thiết kế nhúng vị trí phải đáp ứng những tiêu chí sau:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nó sẽ xuất ra mã hóa duy nhất cho mỗi từ.&lt;/li&gt;
&lt;li&gt;Sự khác biệt giữa hai từ phải nhất quán giữa các câu có độ dài khác nhau.&lt;/li&gt;
&lt;li&gt;Giá trị của nó phải được giới hạn.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Do đó việc nhúng vị trí sin và cosin đã được sử dụng cho Transformer.&lt;/p&gt;
&lt;p&gt;Bây giờ hãy định nghĩa lại Positional Embedding, kích thước của việc nhúng vị trí là &lt;code&gt;[max_sequence_length, embedding_dimension]&lt;/code&gt;, kích thước của phần nhúng vị trí giống với kích thước của vector từ, đều bằng &lt;code&gt;embedding_dimension&lt;/code&gt;. &lt;code&gt;max_sequence_length&lt;/code&gt; là một hyperparameter, đề cập đến số lượng tối đa mà một câu bao gồm.&lt;/p&gt;
&lt;p&gt;Kích thước của việc nhúng vị trí cũng giống như kích thước của việc nhúng từ, cùng là $$d_{model}$$. Công thước tính toán của nó là:&lt;/p&gt;
&lt;p&gt;$$
PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})
$$&lt;/p&gt;
&lt;p&gt;$$
PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})
$$&lt;/p&gt;
&lt;p&gt;Trong đó, $$pos$$ đại diện cho chỉ mục vị trí, $$i$$ đại diện cho chỉ số chiều. Nghĩa là mỗi chiều $$i$$ của positional embedding pos tương ứng với một sóng sin.&lt;/p&gt;
&lt;p&gt;Trong hình dưới này minh họa cho cách tính position embedding của tác giả với số chiều là 6. Giá trị của các vector tại mỗi vị trí được tính toán theo công thức ở hình dưới.
&lt;img src=&quot;src/assets/media/post/pe.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Bản thân việc nhúng vị trí là một thông tin vị trí tuyệt đối, nhưng trong ngôn ngữ, vị trí tương đối cũng rất quan trọng, bởi vì&lt;/p&gt;
&lt;p&gt;$$
sin(\alpha+\beta)=sin\alpha cos\beta+cos\alpha sin\beta\cos(\alpha+\beta)=cos\alpha cos\beta-sin\alpha sin\beta
$$&lt;/p&gt;
&lt;p&gt;cho thấy vector tại vị trí $$p + k$$ có thể được biểu diễn dưới dạng phép biến đổi tuyến tính của vectơ tại vị trí $$p$$, điều này cung cấp khả năng thể hiện thông tin vị trí tương đối. Phiên bản hình sin cũng cho phép mô hình ngoại suy với độ dài chuỗi dài hơn so với độ dài chuỗi gặp phải trong quá trình huấn luyện.&lt;/p&gt;
&lt;h2&gt;Q &amp;amp; A&lt;/h2&gt;
&lt;h3&gt;Tại sao Transformer cần Multi-head Attention ?&lt;/h3&gt;
&lt;p&gt;Bài báo đề cập lý do việc tiến hành Multi-head Attention là để chia mô hình thành nhiều đầu để tạo thành nhiều không gian con, cho phép mô hình chú ý đến các khía cạnh khác nhau của thông tin và cuối cùng tổng hợp thông tin từ tất cả các khía cạnh. Trên thực tế, có thể hình dung bằng trực giác rằng nếu bạn tự thiết kế một mô hình như vậy, attention sẽ không chỉ được thực hiện một lần, kết quả tổng hợp của nhiều lần chú ý ít nhất có thể nâng cao mô hình và cũng có thể được so sánh với vai trò của việc sử dụng nhiều tích chập cùng lúc trong CNN, theo trực giác, sự chú ý của nhiều người đứng đầu giúp mạng nắm bắt được các tính năng/ thông tin phong phú hơn.&lt;/p&gt;
&lt;h3&gt;Ưu điểm của Transformer so với RNN/LSTM là gì? Tại sao?&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Các mô hình RNN không thể tính toán song song vì việc tính toán tại thời điểm T phụ thuộc vào kết quả tính toán của lớp ẩn tại thời điểm T - 1, còn việc tính toán tại thời điểm T - 1 lại phụ thuộc tính toán của lớp ẩn tại thời điểm T - 2.&lt;/li&gt;
&lt;li&gt;Khả năng trích xuất đặc trưng của Transformer tốt hơn so với các mô hình RNN.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Tại sao Transformer có thể thay thế seq2seq?&lt;/h3&gt;
&lt;p&gt;Từ thay thế ở đây hơi không phù hợp, seq2seq tuy cũ nhưng vẫn có chỗ đứng, vấn đề lớn nhất của seq2seq là ở chỗ &lt;strong&gt;Nén thông tin ở phía Encoder thành một vector có độ dài cố định&lt;/strong&gt; và sử dụng nó làm đầu vào của trạng thái đầu tiên ở phía Decoder, để dự đoán trạng thái ẩn của từ đầu tiên (mã thông báo) ở phía Decoder. Khi chuỗi đầu vào tương đối dài, điều này rõ ràng sẽ mất rất nhiều thông tin ở phía Encoder và vector cố định sẽ được gửi đến phía Decoder cùng một lúc, &lt;strong&gt;bên Decoder không thể chú ý đến thông tin mà nó muốn chú ý&lt;/strong&gt;. Mô hinh transformer không chỉ cải thiện đáng kể hai khuyết điểm này của mô hình seq2seq (Mô-đun attention tương tác nhiều đầu), và cũng giới thiệu mô-đun self-attention, trước tiên hãy để trình tự nguồn và trình tự đích được &quot;tự liên kết&quot;, trong trường hợp này, thông tin chứa trong embedding của trình tự nguồn và trình tự đích sẽ phong phú hơn và lớp FFN tiếp theo cũng nâng cao khả năng biểu đạt của mô hình, và tính toán song song của Transfomer vượt xa các model seq2seq.&lt;/p&gt;
&lt;h2&gt;Tham khảo&lt;/h2&gt;
&lt;p&gt;[1] &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention is All You Need&lt;/a&gt;&lt;/p&gt;
</content:encoded><h:img src="/_astro/transformers.B3j2LRDv.jpg"/><enclosure url="/_astro/transformers.B3j2LRDv.jpg"/></item><item><title>Bọ là gì? Defect, Fault, Error, Bug, Failure?</title><link>https://zhaospei.github.io/blog/what-is-bug</link><guid isPermaLink="true">https://zhaospei.github.io/blog/what-is-bug</guid><description>Defects, giống như Quality (Chất lượng phần mềm), có thể định nghĩa bằng nhiều cách khác nhau</description><pubDate>Sun, 02 Jul 2023 08:00:00 GMT</pubDate><content:encoded>&lt;blockquote&gt;
&lt;p&gt;[!CAUTION]
Người đọc tự chịu trách nhiệm về tính xác thực của bài viết.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Context&lt;/h2&gt;
&lt;p&gt;Chuyện là tuần vừa rồi tôi tham gia lab nghiên cứu ở trường và được giao đọc một bài báo khoa học. Nếu không nhầm thì mục đích của thầy khi bảo tôi đọc bài báo này là nắm được sơ qua &quot;context&quot; nghiên cứu của nhóm đang thực hiện.&lt;/p&gt;
&lt;p&gt;Do lần đầu tôi được giao đọc một bài báo khoa học (à trước đó có đọc, dịch gì đấy một bài rồi nhưng mà khi chú tâm thực hiện), thì tất nhiên chưa có kinh nghiệm nên là chỉ nghĩ đọc hiểu nghĩa, cái chung chung bài báo đang nói là gì, hiểu cách bài báo đang thực hiện, vân vân mây mây... Và rồi, sau một tuần được giao đọc bài báo thì mình lên gặp thầy trao đổi về hiểu biết của mình về bài báo. À thì mọi việc sẽ chả có gì nếu như cách đọc của mình là đúng.&lt;/p&gt;
&lt;p&gt;Nhưng &lt;strong&gt;KHÔNG&lt;/strong&gt;, mình đã được khai sáng một đống tri thức mới. Thầy bảo mình không thể hiểu chung chung bài báo nói gì được mà phải nắm rõ và chính xác toàn bộ khái niệm mà bài báo đã đề cập như bài báo mình &quot;được&quot; đọc là &lt;a href=&quot;https://ieeexplore.ieee.org/document/4408585?denied=&quot;&gt;&quot;Classifying Software Changes: Clean or Buggy?&quot;&lt;/a&gt; thì đầu tiên phải đặt câu hỏi &lt;em&gt;Software Changes&lt;/em&gt; là gì? &lt;em&gt;Clean, Buggy&lt;/em&gt; là gì? (Tất nhiên là trong &quot;context&quot; của bài báo).&lt;/p&gt;
&lt;p&gt;Trong một đống thứ thầy nói và hỏi mình thì chắc thứ tồn đọng lại (khắc sâu mình nhất) là câu hỏi (cũng là câu hỏi đầu tiên) là: &lt;strong&gt;&quot;Theo em, bug là gì?&quot;&lt;/strong&gt; (Vì tiêu đề bài báo có việc phân loại hai nhãn là Clean và Buggy mà). Lúc đấy, mình kiểu :))). Tại mình nghĩ là thầy sẽ vấn đề và hướng giải quyết của bài báo chứ không phải mấy câu hỏi lí thuyết như này. Và thầy doạ mình là không trả lời được sẽ hạ điểm (tất nhiên là đùa) môn mình kì trước (Do kì trước mình có tham gia lớp kiểm thử của thầy, không hiểu sao thầy vẫn còn nhớ mình :)).&lt;/p&gt;
&lt;p&gt;Và một đứa thường bỏ qua những thứ căn bản như mình thì tất nhiên chả bao giờ tìm hiểu định nghĩa nó là gì rồi và khi một từ thông dụng và được dùng rất nhiều trong cộng đồng lập trình như từ &quot;bug&quot; như thế thì mình trả lời đại khái là: &lt;em&gt;&quot;Bug là lỗi phần mềm. Nó xảy ra khi phần mềm thực hiện sai và không đúng mong đợi đã được đề ra ban đầu&quot;.&lt;/em&gt; Và thầy nhìn mình, nhìn ánh mắt &quot;trìu mến&quot; mà thầy nhìn mình là mình biết chất lượng trả lời câu hỏi của mình như thế nào rồi. Xong thầy bảo, trong phát triển phần mềm, lỗi phần mềm không chỉ là bug, mà còn có &lt;em&gt;defect&lt;/em&gt;, &lt;em&gt;fault&lt;/em&gt;, &lt;em&gt;error&lt;/em&gt;, &lt;em&gt;failure&lt;/em&gt;. Lúc thầy nói mấy từ này mình chả nghe ra đâu (do trình nghe đọc tiếng anh mình hơi kém, thực ra là rất kém) đến lúc thầy viết ra mình và giải thích mình mới có thể hình dung ra những từ này. Và đấy là lí do mình viết bài viết này để &quot;flex&quot; (đùa chứ xem mình hiểu tới đâu và nhờ bạn đọc xác nhận lại giùm mình.).&lt;/p&gt;
&lt;h2&gt;Main&lt;/h2&gt;
&lt;p&gt;Thực ra trên mạng có rất nhiều bài viết viết về vấn đề này rồi, bạn chỉ cần hỏi &quot;ông Gu Gồ&quot; là nó ra một đống cho bạn đọc nên là mình không đi sâu vào lắm mà chỉ sơ qua những gì mình hiểu được về nó và phân biệt những khái niệm đã nêu ở phần tiêu đề.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;src/assets/media/post/bug-vs-defect-vs-error-vs-fault-vs-failure.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Defects&lt;/strong&gt;, like quality, can be defined in many different ways but are more commonly defined as deviations from specifications or expectations which might lead to &lt;strong&gt;failures&lt;/strong&gt; in operation. &lt;em&gt;(Có thể dịch là: Defects, giống như Quality (Chất lượng phần mềm), có thể định nghĩa bằng nhiều cách khách nhau nhưng thường được định nghĩa là sai lệch so với thông số kỹ thuật hoặc mong đợi có thể dẫn đến failures khi vận hành.)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A software &lt;strong&gt;failure&lt;/strong&gt; is &lt;strong&gt;observable&lt;/strong&gt; software misbehavior; however, a &lt;strong&gt;defect&lt;/strong&gt; may not always lead to a &lt;strong&gt;failure&lt;/strong&gt;. &lt;em&gt;(Có thể dịch là: Failure là hành vi sai trái của phần mềm có thể quan sát được. Tuy nhiên, một defect không phải lúc nào cũng dẫn đến một failure.)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Defects&lt;/strong&gt; colloquially called &lt;strong&gt;bugs&lt;/strong&gt; in software artifacts, typically in the software source code. &lt;em&gt;(Có thể dịch là: Defects thường được gọi (một cách không chính thức) là bugs trong software artifacts (bất kỳ thứ gì tạo ra phần mềm), điển hình là mã nguồn.)&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt;Exit&lt;/h2&gt;
&lt;p&gt;Bài viết này khoảng một nửa là mình bịa, một nửa còn lại cũng bịa nốt.&lt;/p&gt;
</content:encoded><h:img src="/_astro/Software-bugs.DW3F99J-.jpg"/><enclosure url="/_astro/Software-bugs.DW3F99J-.jpg"/></item><item><title>Lựa chọn đặc trưng trong học máy bằng kiểm tra Chi-Square</title><link>https://zhaospei.github.io/blog/chi-square-feature-selection-ml</link><guid isPermaLink="true">https://zhaospei.github.io/blog/chi-square-feature-selection-ml</guid><description>Lựa chọn đặc trưng là một trong những vấn đề quan trọng trong học máy, khi chúng ta ...</description><pubDate>Tue, 27 Jun 2023 08:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Lựa chọn đặc trưng là một trong những vấn đề quan trọng trong học máy, khi chúng ta có một đống cái đặc trưng và quyết định xem những đặc trưng nào là tốt nhất để xây dựng mô hình.&lt;/p&gt;
&lt;p&gt;Có nhiều phương pháp để lựa chọn đặc trưng, trong bài viết này tôi sẽ đưa giải pháp thực hiện bằng &lt;strong&gt;Chi-Square&lt;/strong&gt;.&lt;/p&gt;
&lt;h2&gt;Phân phối Chi-Square&lt;/h2&gt;
&lt;p&gt;Một biến ngẫu nhiên $$ \chi $$ tuân theo phân phối chi-square nếu nó có thể viết được viết dưới dạng tổng bình phương các biến chuẩn chuẩn hoá.&lt;/p&gt;
&lt;p&gt;$$
\chi^{2} = \sum_{}^{}Z_{i}^{2}
$$&lt;/p&gt;
&lt;p&gt;Trong đó $$ Z_{1}, Z_{2}, ... $$ là các biến chuẩn chuẩn hoá.&lt;/p&gt;
&lt;h2&gt;Bậc tự do (Degrees of freedom)&lt;/h2&gt;
&lt;p&gt;Bậc tự do đề cập đến số lượng tối đa các giá trị độc lập logic, có quyền tự do thay đổi. Nói một cách đơn giản, nó có thể được định nghĩa là tổng số mẫu dữ liệu trừ đi số lượng ràng buộc độc lập áp đặt cho các mẫu dữ liệu.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;src/assets/media/post/Chi-squared-distribution.png&quot; alt=&quot;Phân phối Chi-Square&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Trong hình trên, chúng ta có thể thấy phân phối Chi-Square cho các bậc tự do khác nhau. Chúng ta cũng có thể quan sát thấy rằng khi bậc tự do tăng thì phân phối Chi-Square xấp xỉ với phân phối chuẩn.&lt;/p&gt;
&lt;h2&gt;Kiểm tra tính độc lập của hai biến cố bằng Chi-Square&lt;/h2&gt;
&lt;p&gt;Chi-Square được sử dụng trong thống kê để kiểm tra tính độc lập của hai sự kiện. Với dữ liệu của hai biến, chúng ta có thể nhận được số lượng thực tế quan sát (observed) $$ O $$ và số lượng kỳ vọng (expected) $$ ​​E $$. Chi-Square đo lường mức độ chênh lệch của hai đại lượng này.&lt;/p&gt;
&lt;p&gt;$$
\chi_{c}^{2} = \sum_{}^{}\frac{(O_{i} - E_{i})^{2}}{E_{i}}
$$&lt;/p&gt;
&lt;p&gt;Trong đó:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$$c$$ : Số bậc tự do&lt;/li&gt;
&lt;li&gt;$$O$$ : Số lượng thực tế quan sát&lt;/li&gt;
&lt;li&gt;$$E$$ : Số lượng kỳ vọng&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Khi hai sự kiện độc lập, số lượng được quan sát gần với số lượng dự kiến, do đó chúng ta sẽ có giá trị Chi-Square nhỏ hơn. Vì vậy, giá trị Chi-Square cao cho thấy giả thuyết về tính độc lập là không chính xác. Nói một cách đơn giản, giá trị Chi-Square càng cao thì các sự kiện này càng phụ thuộc vào nhau. Hay nếu ta xem một sự kiện là một đặc trưng của mô hình và sự kiện còn lại là phân loại mà mô hình cần dự đoán (Phản hồi). Khi đó nếu giá trị Chi-Square càng cao thì đặc trưng này càng phụ thuộc vào phản hồi và nó có thể được chọn để đào tạo mô hình.&lt;/p&gt;
&lt;p&gt;Đối với lựa chọn đặc trưng bằng Chi-Square, chúng ta mong đợi rằng trong tổng số các đặc trưng được chọn có một phần nhỏ trong chúng vẫn độc lập với lớp phân loại. Tuy nhiên, trong phân loại văn bản hiếm khi các đặc trưng này được thêm vào trong tập đặc trưng trích xuất cuối cùng. Tất nhiên là nó vẫn tốt miễn là phương pháp vẫn xếp hàng các đặc trưng theo tính hữu ích của nó đối với mô hình chứ không phải chỉ sử dụng để đưa ra tuyên bố về sự phụ thuộc hay tính độc lập của các biến trong thống kê.&lt;/p&gt;
&lt;h3&gt;Các bước thực hiện kiểm tra Chi-Square&lt;/h3&gt;
&lt;p&gt;Hãy xem xét một tập dữ liệu mà chúng ta phải xác định lý do tại sao khách hàng rời khỏi ngân hàng, hãy thực hiện kiểm tra Chi-Square cho hai biến. &lt;strong&gt;Giới tính&lt;/strong&gt; của khách hàng với các giá trị là &lt;strong&gt;Nam/Nữ&lt;/strong&gt; và &lt;strong&gt;Rời khỏi&lt;/strong&gt; mô tả liệu khách hàng có rời ngân hàng hay không với các giá trị &lt;strong&gt;Có/Không&lt;/strong&gt;. Trong thử nghiệm này, chúng tôi sẽ kiểm tra xem có mối quan hệ nào giữa &lt;strong&gt;Giới tính&lt;/strong&gt; và &lt;strong&gt;Rời khỏi&lt;/strong&gt;.&lt;/p&gt;
&lt;h4&gt;1. Xác định giả thuyết&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Giả thuyết rỗng&lt;/strong&gt; ($$H_{0}$$): Hai biến đã cho độc lập&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Giả thuyết thay thế&lt;/strong&gt; ($$H_{1}$$): Hai biến đã cho phụ thuộc nhau&lt;/p&gt;
&lt;h4&gt;2. Xây dựng bảng tương quan&lt;/h4&gt;
&lt;p&gt;Một bảng hiển thị phân phối của một biến trong hàng và biến khác trong cột. Nó được sử dụng để nghiên cứu mối quan hệ giữa hai biến.&lt;/p&gt;
&lt;p&gt;|&lt;strong&gt;Giới tính&lt;/strong&gt; \ &lt;strong&gt;Rời bỏ&lt;/strong&gt; |&lt;strong&gt;Có&lt;/strong&gt;	|&lt;strong&gt;Không&lt;/strong&gt;| &lt;strong&gt;Tổng&lt;/strong&gt; |
|&lt;strong&gt;Nam&lt;/strong&gt;| 38| 178| 216|
|&lt;strong&gt;Nữ&lt;/strong&gt;|44	|140| 184|
|&lt;strong&gt;Tổng&lt;/strong&gt;| 82 | 318 | 400|&lt;/p&gt;
&lt;p&gt;Bậc tự do của bảng tương quan được tính bằng công thức: $$ (r-1) * (c-1) $$ trong đó  $$r$$, $$c$$ là số hàng và số cột.
Như bảng trên, ta có:&lt;/p&gt;
&lt;p&gt;$$
df = (2–1) * (2–1) = 1.
$$&lt;/p&gt;
&lt;p&gt;Trong bảng trên, chúng ta đã tìm ra tất cả các giá trị được quan sát và các bước tiếp theo của chúng tôi là tìm các giá trị kỳ vọng, tính giá trị Chi-Square và kiểm tra mối quan hệ giữa chúng.&lt;/p&gt;
&lt;h4&gt;3. Tìm giá trị kỳ vọng&lt;/h4&gt;
&lt;p&gt;Dựa trên giả thiết rỗng là hai biến đã cho độc lập lẫn nhau. Nếu hai biến A, B là biến cố độc lập ta có:&lt;/p&gt;
&lt;p&gt;$$
P(A \cap B) = P(A) * P(B)
$$&lt;/p&gt;
&lt;p&gt;Hãy tính giá trị kỳ vọng cho ô đầu tiên là những người là &lt;strong&gt;Nam&lt;/strong&gt; và &lt;strong&gt;Có&lt;/strong&gt; rời khỏi ngân hàng.&lt;/p&gt;
&lt;p&gt;$$
p = p(Yes) * p(Male)
$$&lt;/p&gt;
&lt;p&gt;$$
p = (82/400) * (216/400)
$$&lt;/p&gt;
&lt;p&gt;$$
p = 0.1107
$$&lt;/p&gt;
&lt;p&gt;$$
E_{1} = n * p = 400 * 0.1107 = 44
$$&lt;/p&gt;
&lt;p&gt;Tương tự, ta tính toán được có giá trị $$E_{2}$$,  $$E_{3}$$, $$E_{4}$$ và được kết quả như bên dưới.&lt;/p&gt;
&lt;p&gt;|&lt;strong&gt;Giới tính&lt;/strong&gt; \ &lt;strong&gt;Rời bỏ&lt;/strong&gt; |&lt;strong&gt;Có&lt;/strong&gt;	|&lt;strong&gt;Không&lt;/strong&gt;|
|&lt;strong&gt;Nam&lt;/strong&gt;| 44| 172|
|&lt;strong&gt;Nữ&lt;/strong&gt;|38	|146|&lt;/p&gt;
&lt;h4&gt;4. Tính toán giá trị Chi-Square&lt;/h4&gt;
&lt;p&gt;$$
\chi_{c}^{2} = \sum_{}^{}\frac{(O_{i} - E_{i})^{2}}{E_{i}}
$$&lt;/p&gt;
&lt;p&gt;Sử dụng công thức đã cho ở trên và các giá trị đã tính toán được, ta dễ dàng có giá trị của &lt;strong&gt;Chi-Square&lt;/strong&gt; bằng &lt;strong&gt;2.22&lt;/strong&gt;&lt;/p&gt;
&lt;h4&gt;5. Bác bỏ Giả thuyết rỗng&lt;/h4&gt;
&lt;p&gt;Với độ tin cậy $$95%$$ là $$\alpha = 0,05$$, chúng ta sẽ kiểm tra xem giá trị &lt;strong&gt;Chi-Square&lt;/strong&gt; tính được có nằm trong vùng chấp nhận hay từ chối hay không.&lt;/p&gt;
&lt;p&gt;Các giá trị &lt;strong&gt;Chi-Square&lt;/strong&gt; chấp thuận có thể xác định bẳng &lt;strong&gt;Bảng Chi-Square&lt;/strong&gt;. Bạn đọc có thể tham khảo tại &lt;a href=&quot;https://people.richland.edu/james/lecture/m170/tbl-chi.html&quot;&gt;https://people.richland.edu/james/lecture/m170/tbl-chi.html&lt;/a&gt;. Dưới đây là phần của bảng trên.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;df&lt;/th&gt;
&lt;th&gt;0.995&lt;/th&gt;
&lt;th&gt;0.99&lt;/th&gt;
&lt;th&gt;0.975&lt;/th&gt;
&lt;th&gt;0.95&lt;/th&gt;
&lt;th&gt;0.90&lt;/th&gt;
&lt;th&gt;0.10&lt;/th&gt;
&lt;th&gt;0.05&lt;/th&gt;
&lt;th&gt;0.025&lt;/th&gt;
&lt;th&gt;0.01&lt;/th&gt;
&lt;th&gt;0.005&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;---&lt;/td&gt;
&lt;td&gt;---&lt;/td&gt;
&lt;td&gt;0.001&lt;/td&gt;
&lt;td&gt;0.004&lt;/td&gt;
&lt;td&gt;0.016&lt;/td&gt;
&lt;td&gt;2.706&lt;/td&gt;
&lt;td&gt;3.841&lt;/td&gt;
&lt;td&gt;5.024&lt;/td&gt;
&lt;td&gt;6.635&lt;/td&gt;
&lt;td&gt;7.879&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2&lt;/td&gt;
&lt;td&gt;0.010&lt;/td&gt;
&lt;td&gt;0.020&lt;/td&gt;
&lt;td&gt;0.051&lt;/td&gt;
&lt;td&gt;0.103&lt;/td&gt;
&lt;td&gt;0.211&lt;/td&gt;
&lt;td&gt;4.605&lt;/td&gt;
&lt;td&gt;5.991&lt;/td&gt;
&lt;td&gt;7.378&lt;/td&gt;
&lt;td&gt;9.210&lt;/td&gt;
&lt;td&gt;10.597&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;3&lt;/td&gt;
&lt;td&gt;0.072&lt;/td&gt;
&lt;td&gt;0.115&lt;/td&gt;
&lt;td&gt;0.216&lt;/td&gt;
&lt;td&gt;0.352&lt;/td&gt;
&lt;td&gt;0.584&lt;/td&gt;
&lt;td&gt;6.251&lt;/td&gt;
&lt;td&gt;7.815&lt;/td&gt;
&lt;td&gt;9.348&lt;/td&gt;
&lt;td&gt;11.345&lt;/td&gt;
&lt;td&gt;12.838&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;4&lt;/td&gt;
&lt;td&gt;0.207&lt;/td&gt;
&lt;td&gt;0.297&lt;/td&gt;
&lt;td&gt;0.484&lt;/td&gt;
&lt;td&gt;0.711&lt;/td&gt;
&lt;td&gt;1.064&lt;/td&gt;
&lt;td&gt;7.779&lt;/td&gt;
&lt;td&gt;9.488&lt;/td&gt;
&lt;td&gt;11.143&lt;/td&gt;
&lt;td&gt;13.277&lt;/td&gt;
&lt;td&gt;14.860&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;5&lt;/td&gt;
&lt;td&gt;0.412&lt;/td&gt;
&lt;td&gt;0.554&lt;/td&gt;
&lt;td&gt;0.831&lt;/td&gt;
&lt;td&gt;1.145&lt;/td&gt;
&lt;td&gt;1.610&lt;/td&gt;
&lt;td&gt;9.236&lt;/td&gt;
&lt;td&gt;11.070&lt;/td&gt;
&lt;td&gt;12.833&lt;/td&gt;
&lt;td&gt;15.086&lt;/td&gt;
&lt;td&gt;16.750&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;6&lt;/td&gt;
&lt;td&gt;0.676&lt;/td&gt;
&lt;td&gt;0.872&lt;/td&gt;
&lt;td&gt;1.237&lt;/td&gt;
&lt;td&gt;1.635&lt;/td&gt;
&lt;td&gt;2.204&lt;/td&gt;
&lt;td&gt;10.645&lt;/td&gt;
&lt;td&gt;12.592&lt;/td&gt;
&lt;td&gt;14.449&lt;/td&gt;
&lt;td&gt;16.812&lt;/td&gt;
&lt;td&gt;18.548&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;7&lt;/td&gt;
&lt;td&gt;0.989&lt;/td&gt;
&lt;td&gt;1.239&lt;/td&gt;
&lt;td&gt;1.690&lt;/td&gt;
&lt;td&gt;2.167&lt;/td&gt;
&lt;td&gt;2.833&lt;/td&gt;
&lt;td&gt;12.017&lt;/td&gt;
&lt;td&gt;14.067&lt;/td&gt;
&lt;td&gt;16.013&lt;/td&gt;
&lt;td&gt;18.475&lt;/td&gt;
&lt;td&gt;20.278&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;8&lt;/td&gt;
&lt;td&gt;1.344&lt;/td&gt;
&lt;td&gt;1.646&lt;/td&gt;
&lt;td&gt;2.180&lt;/td&gt;
&lt;td&gt;2.733&lt;/td&gt;
&lt;td&gt;3.490&lt;/td&gt;
&lt;td&gt;13.362&lt;/td&gt;
&lt;td&gt;15.507&lt;/td&gt;
&lt;td&gt;17.535&lt;/td&gt;
&lt;td&gt;20.090&lt;/td&gt;
&lt;td&gt;21.955&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;9&lt;/td&gt;
&lt;td&gt;1.735&lt;/td&gt;
&lt;td&gt;2.088&lt;/td&gt;
&lt;td&gt;2.700&lt;/td&gt;
&lt;td&gt;3.325&lt;/td&gt;
&lt;td&gt;4.168&lt;/td&gt;
&lt;td&gt;14.684&lt;/td&gt;
&lt;td&gt;16.919&lt;/td&gt;
&lt;td&gt;19.023&lt;/td&gt;
&lt;td&gt;21.666&lt;/td&gt;
&lt;td&gt;23.589&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;2.156&lt;/td&gt;
&lt;td&gt;2.558&lt;/td&gt;
&lt;td&gt;3.247&lt;/td&gt;
&lt;td&gt;3.940&lt;/td&gt;
&lt;td&gt;4.865&lt;/td&gt;
&lt;td&gt;15.987&lt;/td&gt;
&lt;td&gt;18.307&lt;/td&gt;
&lt;td&gt;20.483&lt;/td&gt;
&lt;td&gt;23.209&lt;/td&gt;
&lt;td&gt;25.188&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;td&gt;2.603&lt;/td&gt;
&lt;td&gt;3.053&lt;/td&gt;
&lt;td&gt;3.816&lt;/td&gt;
&lt;td&gt;4.575&lt;/td&gt;
&lt;td&gt;5.578&lt;/td&gt;
&lt;td&gt;17.275&lt;/td&gt;
&lt;td&gt;19.675&lt;/td&gt;
&lt;td&gt;21.920&lt;/td&gt;
&lt;td&gt;24.725&lt;/td&gt;
&lt;td&gt;26.757&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12&lt;/td&gt;
&lt;td&gt;3.074&lt;/td&gt;
&lt;td&gt;3.571&lt;/td&gt;
&lt;td&gt;4.404&lt;/td&gt;
&lt;td&gt;5.226&lt;/td&gt;
&lt;td&gt;6.304&lt;/td&gt;
&lt;td&gt;18.549&lt;/td&gt;
&lt;td&gt;21.026&lt;/td&gt;
&lt;td&gt;23.337&lt;/td&gt;
&lt;td&gt;26.217&lt;/td&gt;
&lt;td&gt;28.300&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;13&lt;/td&gt;
&lt;td&gt;3.565&lt;/td&gt;
&lt;td&gt;4.107&lt;/td&gt;
&lt;td&gt;5.009&lt;/td&gt;
&lt;td&gt;5.892&lt;/td&gt;
&lt;td&gt;7.042&lt;/td&gt;
&lt;td&gt;19.812&lt;/td&gt;
&lt;td&gt;22.362&lt;/td&gt;
&lt;td&gt;24.736&lt;/td&gt;
&lt;td&gt;27.688&lt;/td&gt;
&lt;td&gt;29.819&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;14&lt;/td&gt;
&lt;td&gt;4.075&lt;/td&gt;
&lt;td&gt;4.660&lt;/td&gt;
&lt;td&gt;5.629&lt;/td&gt;
&lt;td&gt;6.571&lt;/td&gt;
&lt;td&gt;7.790&lt;/td&gt;
&lt;td&gt;21.064&lt;/td&gt;
&lt;td&gt;23.685&lt;/td&gt;
&lt;td&gt;26.119&lt;/td&gt;
&lt;td&gt;29.141&lt;/td&gt;
&lt;td&gt;31.319&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;15&lt;/td&gt;
&lt;td&gt;4.601&lt;/td&gt;
&lt;td&gt;5.229&lt;/td&gt;
&lt;td&gt;6.262&lt;/td&gt;
&lt;td&gt;7.261&lt;/td&gt;
&lt;td&gt;8.547&lt;/td&gt;
&lt;td&gt;22.307&lt;/td&gt;
&lt;td&gt;24.996&lt;/td&gt;
&lt;td&gt;27.488&lt;/td&gt;
&lt;td&gt;30.578&lt;/td&gt;
&lt;td&gt;32.801&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;16&lt;/td&gt;
&lt;td&gt;5.142&lt;/td&gt;
&lt;td&gt;5.812&lt;/td&gt;
&lt;td&gt;6.908&lt;/td&gt;
&lt;td&gt;7.962&lt;/td&gt;
&lt;td&gt;9.312&lt;/td&gt;
&lt;td&gt;23.542&lt;/td&gt;
&lt;td&gt;26.296&lt;/td&gt;
&lt;td&gt;28.845&lt;/td&gt;
&lt;td&gt;32.000&lt;/td&gt;
&lt;td&gt;34.267&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Ta có &lt;strong&gt;bậc tự do (df)&lt;/strong&gt; bằng &lt;strong&gt;1&lt;/strong&gt; (được tính toán dựa vào bảng tương quan phía trên) và  $$\alpha = 0,05$$ thì giá trị &lt;strong&gt;Chi-Square&lt;/strong&gt; chấp nhận là &lt;strong&gt;3.84&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Nhận thấy giá trị &lt;strong&gt;Chi-Square&lt;/strong&gt; tính được thấp hơn giá trị &lt;strong&gt;Chi-Square&lt;/strong&gt; chấp nhận thì ta chấp nhận giả thiết rỗng. Hay ta có thể kết luận được rằng hai biến cố đã cho độc lập nhau. Vậy nếu ta xem &lt;strong&gt;Giới tính&lt;/strong&gt; là một đặc trưng cần xem xét của mô hình và &lt;strong&gt;Rời bỏ ngân hàng&lt;/strong&gt; hay không là lớp giá trị mô hình cần phân loại, thì ta có thể kết luận, &lt;strong&gt;Giới tính&lt;/strong&gt; không thể được sử dụng để huấn luyện mô hình vì hai biến cố này không có mối liên hệ lẫn nhau.&lt;/p&gt;
&lt;h2&gt;Sử dụng Chi-Square để lựa chọn đặc trưng cho mô hình phân loại văn bản&lt;/h2&gt;
&lt;p&gt;Một phương pháp lựa chọn đặc trưng phổ biến được sử dụng với dữ liệu văn bản là lựa chọn đặc trưng với &lt;strong&gt;Chi-Square&lt;/strong&gt;. $$ \chi^{2} $$ như chúng ta thấy ở trên có thể được sử dụng trong thống kê để kiểm tra tính độc lập của hai biến cố. Cụ thể hơn, trong lựa chọn đặc trưng cho mô hình, chúng ta sử dụng nó để kiểm tra một thuật ngữ cụ thể và một lớp phân loại cụ thể có độc lập hay không.&lt;/p&gt;
&lt;p&gt;Cho một văn bản $$D$$, chúng tôi ước tính số lượng sau cho mỗi thuật ngữ và xếp hạng chúng theo điểm số của chúng:&lt;/p&gt;
&lt;p&gt;$$
\chi^2(D, t, c) = \sum_{e_t \in {0, 1}} \sum_{e_c \in {0, 1}}  \frac{ (O_{e_te_c} - E_{e_te_c} )^2 }{ E_{e_te_c} }
$$&lt;/p&gt;
&lt;p&gt;Trong đó:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;$$O$$ là tần số quan sát và $$E$$ tần số kỳ vọng&lt;/li&gt;
&lt;li&gt;$$e_{t}$$ nhận giá trị $$1$$ nếu văn bản có chứa thuật ngữ $$t$$, $$0$$ với trường hợp ngược lại.&lt;/li&gt;
&lt;li&gt;$$e_{c}$$ nhận giá trị $$1$$ nếu văn bản thuộc lớp phân loại $$c$$, $$0$$ với trường hợp ngược lại.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Với mỗi đặc trưng (thuật ngữ), một điểm  $$ \chi^{2} $$ tương ứng chỉ ra &lt;strong&gt;Giả thuyết rỗng&lt;/strong&gt; $$H_{0}$$ về tính độc lập của hai biến cố (có nghĩa là lớp của văn bản được phân loại không ảnh hưởng đến tần suất xuất hiện của thuật ngữ) nên bị bác bỏ hay sự xuất hiện của thuật ngữ và lớp của văn bản phụ thuộc lẫn nhau. Hay trong trường hợp này, chúng ta sẽ chọn thuật ngữ này làm đặc trưng cho mô hình phân loại văn bản.&lt;/p&gt;
&lt;h2&gt;Kết luận&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Chi-Square&lt;/strong&gt; nhạy cảm với kích thước mẫu. Các mối quan hệ có thể có ý nghĩa khi chúng không chỉ đơn giản là do một mẫu rất lớn được sử dụng. Ngoài ra, &lt;strong&gt;Chi-Square&lt;/strong&gt; không thể xác định liệu một biến cố có mối quan hệ nhân quả với biến khác hay không. Nó chỉ có thể xác định liệu hai biến cố có liên quan với nhau hay không.. Nói chung, khi giá trị kỳ vọng ​​trong một ô của bảng tương quan nhỏ hơn 5, &lt;strong&gt;Chi-Square&lt;/strong&gt; có thể dẫn đến sai sót trong kết luận. Hy vọng bài viết có thể giúp bạn có cái nhìn tổng quan về phương pháp này và có thể áp dụng nó cho mô hình của bạn.&lt;/p&gt;
</content:encoded><h:img src="/_astro/Chi-Square.CgjiRe5x.jpg"/><enclosure url="/_astro/Chi-Square.CgjiRe5x.jpg"/></item><item><title>Từ nơ-ron sinh học đến nơ-ron nhân tạo</title><link>https://zhaospei.github.io/blog/neuron2neuron</link><guid isPermaLink="true">https://zhaospei.github.io/blog/neuron2neuron</guid><description>Con người lấy cảm hứng từ các loài chim để bay,...</description><pubDate>Sat, 13 May 2023 08:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Con người lấy cảm hứng từ các loài chim để bay, loài sứa biển để điều trị ung thư, da cá mập để làm bề mặt vật liệu chống bám và còn rất nhiều phát minh khác lấy cảm hứng từ thiên nhiên.&lt;/p&gt;
&lt;p&gt;Vì thế, rất dễ hiểu khi ta xem xét cấu trúc bộ não sinh học để tìm cảm hứng cho việc xây dựng một bộ máy thông minh. Đây cũng chính là ý tưởng đằng sau của &lt;strong&gt;Mạng nơ-ron nhân tạo (artificial neural network - ANN)&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Tuy nhiên, dù máy bay lấy cảm hứng từ loài chim, chúng lại không cần phải vỗ cánh. Tương tự, ANN đã dần trở nên rất khác biệt so với phiên bản sinh học của nó. Một số nhà nghiên cứu còn cho rằng chúng ta nên ngừng sử dụng phép so sánh với sinh học (ví dụ như sử dụng từ &quot;&lt;em&gt;đơn vị&lt;/em&gt;&quot; - &quot;&lt;em&gt;unit&lt;/em&gt;&quot; - thay cho &quot;&lt;em&gt;nơ-ron&lt;/em&gt;&quot;), vì lo rằng phép so sánh này sẽ giới hạn sự sáng tạo trong các hệ thống hợp lý về mặt sinh học.&lt;/p&gt;
&lt;h2&gt;Nơ-ron Sinh học&lt;/h2&gt;
&lt;p&gt;Trước khi chúng ta bàn về nơ-ron nhân tạo, chúng ta cùng tìm hiểu qua nơ-ron sinh học. &lt;em&gt;Nơ-ron sinh học&lt;/em&gt; là một tế bào với vẻ ngoài khác thường được tìm thấy trong não động vật. Nó bao gồm một &lt;em&gt;thân tế bào&lt;/em&gt; chứa nhân và hầu hết các thành phần phức tạp khác, với các nhánh mở rộng được gọi là &lt;em&gt;sợi nhánh&lt;/em&gt;, cùng với một phần mở rộng rất dài được gọi là &lt;em&gt;sợi trục&lt;/em&gt;. Chiều dài của sợi trục lớn hơn thân tế bào từ vài lần cho đến hàng chục nghìn lần. Ở gần cuối, sợi trục tách thành nhiều nhành được gọi là &lt;em&gt;telodendria&lt;/em&gt;, và tại đỉnh của những nhành này là các cấu trúc siêu nhỏ được gọi là &lt;em&gt;điểm tiếp hợp synap&lt;/em&gt; (hoặc đơn giản là &lt;em&gt;synap&lt;/em&gt;), được nối với các sợi nhánh hoặc thân tế bào của những nơ-ron khác. Các nơ-ron sinh học sinh ra các xung điện ngắn được gọi là &lt;em&gt;điện thế hoạt động&lt;/em&gt; (hoặc đơn giản  là &lt;em&gt;tín hiệu&lt;/em&gt;). Chúng di chuyển dọc theo sợi trục và kích thích synap giải phóng ra tín hiệu hoá học được gọi &lt;em&gt;chất dẫn truyền thần kinh&lt;/em&gt;. Khi một nơ-ron nhận đủ một lượng chất dẫn truyền thần kinh này trong một vài mili giây, nó sẽ phát ra các xung điện của chính nó (thật ra, điều này còn phụ thuộc vào chất dẫn truyền thần kinh bởi có một số chất ức chế sự kích hoạt của nơ-ron).&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;src/assets/media/post/no-ron-sinh-hocc.png&quot; alt=&quot;Nơ-ron sinh học&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Do đó, mặc dù các nơ-ron sinh học riêng lẻ dường như có cách hoạt động khá đơn giản, chúng lại được tổ chức trong một mạng lưới rộng lớn với hàng tỷ nơ-ron, và ở đố mỗi nơ-ron được kết nối với hàng nghìn nơ-ron khác. Các phép tính với độ phức tạp cao có thể được xử lý bởi một mạng nơ-ron khá đơn giản, tương tự như cách một tổ kiến phức tạp được tạo nên bởi nỗ lực tổng hợp từ những các thể kiến riêng lẻ. &lt;em&gt;Kiến trúc mạng nơ-ron sinh học (biological neural network - BNN)&lt;/em&gt; vẫn đang là chủ đề được tích cực nghiên cứ, tuy nhiên một vài phần của não đã được khám phá, và có vẻ như các nơ-ron thường được sắp xếp thành các tầng liên tiếp, đặc biệt là ở vùng đại não (lớp ngoài cùng của bộ não), như có thể thấy ở hình bên dưới.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;src/assets/media/post/cac-tang-trong-mang-no-ron-sinh-hoc.png&quot; alt=&quot;Các tầng trong mạng nơ-ron sinh học (Võ não người)&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;Nơ-ron Nhân tạo&lt;/h2&gt;
&lt;p&gt;Mạng nơ-ron Nhân tạo là một phương thức, công cụ trong lĩnh vực trí tuệ nhân tạo, được lấy cảm hứng từ cấu trúc bộ não con người, để máy tính có thể xử lý dữ liệu một cách tự động. Đây là một loại Học máy(Machine Learning), còn được gọi là Học sâu (Deep Learning), sử dụng các nơ-ron kết nối với nhau trong một cấu trúc phân lớp tương tự như bộ não con người. Quá trình này cho phép máy tính học hỏi từ sai lầm và cải thiện liên tục, tạo ra một hệ thống thích ứng. Mạng nơ-ron nhân tạo được áp dụng để giải quyết các vấn đề phức tạp, chẳng hạn như tóm tắt tài liệu hoặc nhận diện khuôn mặt, với độ chính xác cao hơn.&lt;/p&gt;
&lt;p&gt;Bộ não sinh học chính là nguồn cảm hứng cho kiến trúc mạng nơ ron. Các tế bào não của con người, có được gọi là nơ-ron, tạo thành một mạng lưới phức tạp, có tính liên kết cao và gửi các tín hiệu điện đến nhau để giúp con người xử lý thông tin. Tương tự, một mạng nơ-ron nhân tạo được tạo ra từ các tế bào nơ-ron nhân tạo, cùng nhau phối hợp để giải quyết một vấn đề. Nơ-ron nhân tạo là các mô đun phần mềm, được gọi là nút và mạng nơ-ron nhân tạo là các chương trình phần mềm hoặc thuật toán mà về cơ bản, sử dụng hệ thống máy tính để giải quyết các phép toán.&lt;/p&gt;
&lt;p&gt;McCulloch và Pitts đề xuất một mô hình rất đơn giản để mô tả mạng nơ-ron sinh học, và mô hình này về sau được biết đến là nơ-ron nhân tạo: nó có một hoặc nhiều đầu vào nhị phân (bật/tắt) và một đầu ra nhị phân. Nơ-ron nhân tạo kích hoạt đầu vào của nó khi có nhiều hơn một lượng đầu vào nhất định được kích hoạt. Trong bài báo, họ đã chứng minh rằng ngay cả với một mô hình đơn giản như trên, ta vẫn có thể xây dựng một mạng chứa các nơ-ron nhân tạo với khả năng tính toán bất kỳ mệnh đề logic nào. Hình phía dưới là một vài ANN thực hiện các phép toán khác nhau, với giả định rằng một nơ-ron được kích hoạt khi ít nhất hai trong số các đầu vào của nó được kích hoạt.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;src/assets/media/post/ANN-phep-tinh-logic.png&quot; alt=&quot;ANN thực hiện các phép tính logic đơn giản&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Các mạng nơ-ron nhân tạo được nghiên cứu và phát triển thay đổi liên tục trong nhiều năm, với nhiều kiến trúc mạng nơ-ron nhân tạo khác nhau. Ngày nay, một mạng nơ-ron nhân tạo bao gồm 3 &lt;em&gt;lớp (layer)&lt;/em&gt;:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lớp đầu vào&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Thông tin từ thế giới bên ngoài đi vào mạng nơ-ron nhân tạo thông qua lớp đầu vào. Các nút đầu vào xử lý dữ liệu, phân tích hoặc phân loại và sau đó chuyển dữ liệu sang lớp tiếp theo.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lớp ẩn&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Dữ liệu đi vào lớp ẩn đến từ lớp đầu vào hoặc các lớp ẩn khác. Mạng nơ-ron nhân tạo có thể có một số lượng lớn lớp ẩn. Mỗi lớp ẩn phân tích dữ liệu đầu ra từ lớp trước, xử lý dữ liệu đó sâu hơn và rồi chuyển dữ liệu sang lớp tiếp theo.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Lớp đầu ra&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Lớp đầu ra cho ra kết quả cuối cùng của tất cả dữ liệu được xử lý bởi mạng nơ-ron nhân tạo. Lớp này có thể có một hoặc nhiều nút. Ví dụ: giả sử chúng ta gặp phải một vấn đề phân loại nhị phân (có/không), lớp đầu ra sẽ có một nút đầu ra, nút này sẽ cho kết quả 1 hoặc 0. Tuy nhiên, nếu chúng ta gặp phải vấn đề phân loại nhiều lớp, lớp đầu ra sẽ có thể bao gồm nhiều hơn một nút đầu ra.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kiến trúc mạng nơ-ron chuyên sâu&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Mạng nơ-ron chuyên sâu, hoặc mạng deep learning, có nhiều lớp ẩn với hàng triệu nơ-ron nhân tạo liên kết với nhau. Một con số, có tên gọi là trọng số, đại diện cho các kết nối giữa hai nút. Trọng số sẽ dương nếu một nút kích thích nút còn lại, hoặc âm nếu một nút ngăn cản nút còn lại. Các nút với trọng số cao hơn sẽ có ảnh hưởng lớn hơn lên các nút khác.&lt;/p&gt;
&lt;p&gt;Về mặt lý thuyết, mạng nơ-ron chuyên sâu có thể ánh xạ bất kỳ loại dữ liệu đầu vào với bất kỳ loại dữ liệu đầu ra nào. Tuy nhiên, chúng cũng cần được đào tạo hơn rất nhiều so với các phương pháp máy học khác. Chúng cần hàng triệu ví dụ về dữ liệu đào tạo thay vì hàng trăm hoặc hàng nghìn ví dụ mà một mạng đơn giản hơn thường cần.&lt;/p&gt;
&lt;p&gt;Phía dưới là một ví dụ cho kiến trúc mạng nơ-ron nhân tạo&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;src/assets/media/post/no-ron-nhan-tao.png&quot; alt=&quot;Mạng nơ-ron nhân tạo&quot; /&gt;&lt;/p&gt;
&lt;h2&gt;Từ Nơ-ron Sinh học đến Nơ-ron Nhân tạo&lt;/h2&gt;
&lt;p&gt;Một điều đáng nhiên là ANN đã tồn tại từ khá lâu: chúng được giới thiệu vào năm 1943 bởi nhà sinh lý học thần kinh &lt;strong&gt;Warren McCulloch&lt;/strong&gt; và nhà toán học &lt;strong&gt;Walter Pitts&lt;/strong&gt;. Trong bài báo mang tính bước ngoặt của họ &quot;&lt;em&gt;A Logical Calculus of Ideas Immanent in Nervous Activity&lt;/em&gt;&quot;, McCulloch và Pitts đã trình bày một mô hình tính toán giản lược của cách mà các nơ-ron sinh học có thể làm việc cùng nhau trong não bộ động vật để thực hiện các phép tính phức tạp bằng &lt;em&gt;logic mệnh đề (propositional logic)&lt;/em&gt;. Đây chính là kiến trúc mạng nơ-ron nhân tạo đầu tiên. Kể từ đó, hàng loạt các kiến trúc khác đã được phát minh, xử lý tính toán linh hoạt và hoạt động hiệu quả hơn.&lt;/p&gt;
&lt;p&gt;Sự thành công sớm của ANN đã khiến nhiều người tin rằng họ sẽ sớm được nói chuyện với những cố máy thực sự thông minh. Vào thập niên 1960, khi rõ ràng là lời hứa này sẽ không được thực hiện (ít nhất là trong một khoảng thời gian dài), các nguồn tài trợ được chuyển sang lĩnh vực khác, và ANN bước vào một mùa đông dài. Vào thập niên 1980, các kiến trúc mới được phát minh và các kỹ thuật huấn luyện tốt hơn được phát triển, giúp cho &lt;em&gt;thuyết kết nối (connectionism - ngành nghiên cứu về mạng nơ-ron)&lt;/em&gt; bắt đầu được quan tâm trở lại. Tuy nhiên, tiến độ trong ngành này khá chậm, vào vào thập niên 1990, các kỹ thuật Học Máy mạnh mẽ khác đã được phát minh, ví dụ &lt;em&gt;Máy Vector Hỗ trợ&lt;/em&gt;,... Có vẻ những kỹ thuật này đem lại kết quả tốt hơn và nền tảng lý thuyết vững chắc hơn so với ANN, nên lần nữa việc nghiên cứu ANN lại bị trì hoãn.&lt;/p&gt;
&lt;p&gt;Giờ đây, khi lượng dữ liệu lớn bùng nổ và sự phát triển vượt bậc về năng lực tính toán từ thập niên 1990, chúng ta lại đang chứng kiến thêm một làn sống quan tâm khác tới ANN. Liệu xu hướng này sẽ lại lụi tàn như trước?&lt;/p&gt;
</content:encoded><h:img src="/_astro/no-ron-sinh-hoc.D7jpf-Pg.png"/><enclosure url="/_astro/no-ron-sinh-hoc.D7jpf-Pg.png"/></item><item><title>Bài toán sinh test cho kỹ thuật kiểm thử theo cặp</title><link>https://zhaospei.github.io/blog/two-pairs-testing</link><guid isPermaLink="true">https://zhaospei.github.io/blog/two-pairs-testing</guid><description>Trong bài viết này, chúng ta sẽ tìm hiểu về một kỹ thuật kiểm thử tổ hợp (Combinatorial Testing)</description><pubDate>Sat, 25 Mar 2023 08:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Trong bài viết này, chúng ta sẽ tìm hiểu về một kỹ thuật kiểm thử tổ hợp (Combinatorial Testing) được gọi là Kiểm thử theo cặp
(Pairwise Testing hay All-Pairs Testing).&lt;/p&gt;
&lt;h2&gt;Định nghĩa: Thế nào là All-Pairs Testing?&lt;/h2&gt;
&lt;p&gt;Theo &lt;strong&gt;ISTQB&lt;/strong&gt;, &lt;strong&gt;All-Pairs Testing (hay Pairwise Testing)&lt;/strong&gt; là một kỹ thuật kiểm thử hộp đen trong đó các test cases được thiết kế để thực hiện tất cả các tổ hợp có thể có của từng cặp tham số đầu vào. Tức là với mỗi cặp input đầu vào, tất cả các giá trị của cặp input đấy được phủ toàn bộ (Một test case có thể phủ được nhiều cặp input khác nhau).&lt;/p&gt;
&lt;p&gt;Kỹ thuật &lt;strong&gt;All-Pairs&lt;/strong&gt; rất hữu ích để thiết kế các bài kiểm tra cho các ứng dụng liên quan đến nhiều tham số. Các thử nghiệm được thiết kế sao cho đối với mỗi cặp tham số đầu vào của một hệ thống, có tất cả các tổ hợp riêng biệt có thể có của các tham số đó. Do bộ thử nghiệm bao gồm tất cả các tổ hợp nên nó không toàn diện nhưng rất hiệu quả trong việc tìm lỗi.&lt;/p&gt;
&lt;p&gt;Kiểm thử &lt;strong&gt;All-Pairs&lt;/strong&gt; được áp dụng phổ biến trong công nghiệp khi mà một số vấn đề chỉ được xảy ra bởi sự tương tác giữa các tham số đầu vào hoặc components. Kiểm thử &lt;strong&gt;All-Pairs&lt;/strong&gt; có thể tìm được đến &lt;strong&gt;50 - 90%&lt;/strong&gt; lỗi của phần mềm, hệ thống.&lt;/p&gt;
&lt;h2&gt;Số test cases cần sinh cho All-Pairs Testing&lt;/h2&gt;
&lt;p&gt;Ta có công thức tính số lượng test cases cần sinh như sau:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Số lượng test case = Số lượng miền giá trị lớn nhất của các biến * Số lượng miền giá trị lớn nhất của các biến&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Chứng minh tính đúng đắn của công thức:&lt;/p&gt;
&lt;h2&gt;Cách sinh test cho All-Pairs Testing&lt;/h2&gt;
&lt;p&gt;Chúng ta sẽ lấy ví dụ cho &lt;strong&gt;Ứng dụng giao dịch xe&lt;/strong&gt; với các yêu cầu sau:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ứng dụng giao dịch xe cho phép Mua và Bán xe.&lt;/li&gt;
&lt;li&gt;Nó sẽ hỗ trợ giao dịch ở Delhi và Mumbai.&lt;/li&gt;
&lt;li&gt;Ứng dụng phải có số đăng ký có thể hợp lệ hoặc không hợp lệ.&lt;/li&gt;
&lt;li&gt;Nó sẽ cho phép giao dịch với các hãng xe: BMW, Audi và Mercedes.&lt;/li&gt;
&lt;li&gt;Có thể thực hiện hai loại booking: E-booking và In-store.&lt;/li&gt;
&lt;li&gt;Chỉ có thể đặt xe trong giờ giao dịch.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Chúng ta sẽ thực hiện việc sinh test cho ca kiểm thử &lt;strong&gt;All-Pairs&lt;/strong&gt; lần lượt theo các bước sau:&lt;/p&gt;
&lt;h3&gt;Bước 1: Liệt kê các giá trị của các tham số đầu vào.&lt;/h3&gt;
&lt;p&gt;Chúng ta có thể dễ dàng liêt kê các giá trị có thể có của các tham số đầu vào của ứng dụng như sau:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Loại giao dịch&lt;/strong&gt;: Mua, Bán.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vị trí&lt;/strong&gt;: Delhi, Mumbai.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Hãng xe&lt;/strong&gt;: BMW, Audi, Mercedes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Số đăng ký xe&lt;/strong&gt;: Valid (5000 giá trị), Invalid&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cách thức giao dịch&lt;/strong&gt;: E-Booking, In-store&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Thời gian giao dịch&lt;/strong&gt;: Thời gian trong giờ làm việc, Thời gian ngoài giờ làm việc&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nếu chúng ta sinh test cases cho toàn bộ các tổ hợp hợp lệ thì sẽ có đến:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2 . 2 . 3 . 5000 . 2 . 2 = &lt;strong&gt;24000&lt;/strong&gt; test cases tổ hợp hợp lệ.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Chưa kể chúng ta còn chưa tính đến các test cases không hợp lệ trong quá trình sinh test.&lt;/p&gt;
&lt;h3&gt;Bước 2: Đơn giản hóa việc sinh test cases (Simplify)&lt;/h3&gt;
&lt;p&gt;Chúng ta sẽ đơn giản việc sinh các test cases theo các cách:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sử dụng một cách lẫy mẫu thông minh&lt;/li&gt;
&lt;li&gt;Sử dụng các nhóm hay ranh giới, ngay cả khi dữ liệu không rời rạc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Theo đó chúng ta có thể giảm số đăng ký xuống còn hai loại: Valid và Invaild.&lt;/p&gt;
&lt;p&gt;Bây giờ, số test cases chúng ta sẽ phải sinh sẽ còn lại:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2 . 2 . 3 . 2 . 2 . 2  = &lt;strong&gt;96&lt;/strong&gt; test cases.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Woa Woa Woa! Chúng ta đã giảm số lượng các test case cần phải sinh một số rất lớn rồi đúng không? Nhưng đây vẫn chưa phải các sinh hợp lí để cho số lượng test cases nhỏ nhất có thể. Tiếp tục nào!&lt;/p&gt;
&lt;h3&gt;Bước 3: Sắp xếp các biến theo miền giá trị&lt;/h3&gt;
&lt;p&gt;Chúng ta tiến hành sắp xếp các biến theo thứ tự giảm dần số lượng miền giá trị: Tức biến có nhiều miền giá trị nhất sẽ được xếp đầu tiên  và biến có ít miền giá trị nhất được xếp cuối cùng. Sau khi sắp xếp, chúng ta sẽ có một bảng trông như thế này đây.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Hãng xe&lt;/th&gt;
&lt;th&gt;Loại giao dịch&lt;/th&gt;
&lt;th&gt;Vị trí&lt;/th&gt;
&lt;th&gt;Số đăng ký xe&lt;/th&gt;
&lt;th&gt;Cách thức giao dịch&lt;/th&gt;
&lt;th&gt;Thời gian giao dịch&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3&gt;Bước 4: Sắp xếp các miền giá trị của các biến để tạo bộ kiểm thử&lt;/h3&gt;
&lt;p&gt;Chúng ta sẽ lần lượt điền giá trị của bảng trống đã tạo phía trên theo từng cột một.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Hãng xe&lt;/th&gt;
&lt;th&gt;Loại giao dịch&lt;/th&gt;
&lt;th&gt;Vị trí&lt;/th&gt;
&lt;th&gt;Số đăng ký xe&lt;/th&gt;
&lt;th&gt;Cách thức giao dịch&lt;/th&gt;
&lt;th&gt;Thời gian giao dịch&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;BMW&lt;/td&gt;
&lt;td&gt;Mua&lt;/td&gt;
&lt;td&gt;Delhi&lt;/td&gt;
&lt;td&gt;Valid&lt;/td&gt;
&lt;td&gt;E-booking&lt;/td&gt;
&lt;td&gt;Trong giờ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Audi&lt;/td&gt;
&lt;td&gt;Bán&lt;/td&gt;
&lt;td&gt;Mumbai&lt;/td&gt;
&lt;td&gt;Invalid&lt;/td&gt;
&lt;td&gt;In-store&lt;/td&gt;
&lt;td&gt;Ngoài giờ&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mercedes&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</content:encoded><h:img src="/_astro/testing.CWowBVtC.png"/><enclosure url="/_astro/testing.CWowBVtC.png"/></item><item><title>Một số mô hình phát triển phần mềm (Phần 1)</title><link>https://zhaospei.github.io/blog/software-models-chapter-1</link><guid isPermaLink="true">https://zhaospei.github.io/blog/software-models-chapter-1</guid><description>Mô hình phát triển phần mềm là thể hiện trừu tượng của các tiến trình phát triển phần mềm. Nó xác định các pha/ giai đoạn trong phát triển phần mềm.</description><pubDate>Sun, 05 Mar 2023 08:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Mô hình phát triển phần mềm là thể hiện trừu tượng của các tiến trình phát triển phần mềm. Nó xác định các pha/ giai đoạn trong phát triển phần mềm.&lt;/p&gt;
&lt;p&gt;Trong các dự án phát triển phần mềm, mô hình đóng vai trò rất quan trọng. Mô hình đóng vai trò là hướng đi và quyết định đến chất lượng đầu ra của sản phẩm.&lt;/p&gt;
&lt;p&gt;Có nhiều loại mô hình phát triển phần mềm khác nhau như:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mô hình thác nước&lt;/li&gt;
&lt;li&gt;Mô hình xoắn ốc&lt;/li&gt;
&lt;li&gt;Mô hình tăng trưởng&lt;/li&gt;
&lt;li&gt;Mô hình Scrum
…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Trong bài viết này, tôi sẽ mô tả và phân tích chi tiết bốn loại mô hình phát triển phần mềm cơ bản nhất bao gồm: Mô hình thác nước (Waterfall model), Mô hình chữ V (V-Shaped Model), Bản mẫu và Mô hình xoắn ốc.&lt;/p&gt;
&lt;h2&gt;1. Mô hình thác nước&lt;/h2&gt;
&lt;p&gt;Mô hình thác nước được xem là mô hình đầu tiên được giới thiệu. Đây là mô hình SDLC (Software Development Life Cycle) lâu đời với đơn giản nhất. Mô hình thác nước là mô hình điển hình cho phát triển phần mềm đảm bảo bảo tốt.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;src/assets/media/post/waterfall-model.png&quot; alt=&quot;Mô hình thác nước&quot; /&gt;&lt;/p&gt;
&lt;h3&gt;a. Đặc điểm&lt;/h3&gt;
&lt;p&gt;Trong mô hình thác nước, toàn bộ quá trình phát triển phần mềm được chia thành các giai đoạn riêng biệt. Trong mô hình thác nước, thông thường, kết quả của một pha/ giai đoạn đóng vai trò là đầu vào cho pha/ giai đoạn tiếp theo.&lt;/p&gt;
&lt;p&gt;Đặc điểm quan trọng nhất của mô hình thác nước là phát triển tuần tự tuyến tính (Vì vậy, nó còn có tên gọi khác là linear - sequential life cycle model). Tức trong mô hình thác nước, giai đoạn tiếp theo chỉ được bắt đầu sau khi các mục tiêu đã được xác định của giai đoạn trước được hoàn thành. Trong mô hình này, các giai đoạn phát triển một cách độc lập mà không xếp chồng lẫn nhau.&lt;/p&gt;
&lt;p&gt;Tuy nhiên, trong thực tế việc thực hiện tốt một pha không phải dễ và việc xảy ra vấn đề ở là điều hiển nhiên. Vậy nếu trong quá trình phát triển gặp vấn đề có liên quan đến giai đoạn đã hoàn thành trước đó thì phải giải quyết như nào? Câu trả lời là chúng ta vẫn có thể quay lại giai đoạn trước đó sau khi đã thực hiện xong giai đoạn đấy. Tuy nhiên, sau khi quay lại, chúng ta vẫn phải tuân theo tính tuần tự của model.&lt;/p&gt;
&lt;h4&gt;Các giai đoạn tuần tự trong mô hình thác nước&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Thu thập và phân tích yêu cầu&lt;/strong&gt;: Tất cả mọi yêu cầu có thể của hệ thống được xác định trong giai đoạn này và được ghi lại đầy đủ trong tài liệu đặc tả yêu cầu.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Thiết kế&lt;/strong&gt;: Các thông số kỹ thuật trong giai đoạn đầu tiên được nghiên cứu trong giai đoạn này. Thiết kế giúp ta xác định những yêu cầu về phần cứng hệ thống, giúp ta xác định được kiến thức tổng thể của hệ thống.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Triển khai và kiểm thử đơn vị:&lt;/strong&gt; Với số liệu đầu vào từ giai đoạn thiết kế, chúng ta phát triển các chương trình nhỏ gọi là Unit, sẽ được tích hợp trong giai đoạn tiếp theo. Mỗi đơn vị được triển khai và kiểm thử chức năng của nó được gọi là Unit Testing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tích hợp và kiểm thử hệ thống:&lt;/strong&gt; Tất cả các đơn vị được thực hiện trong giai đoạn trước được tích hợp vào một hệ thống xác định. Sau khi tích hợp, toàn bộ hệ thống sẽ được kiểm tra chức năng và phi chức năng.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vận hành và bảo trì:&lt;/strong&gt;  Sản phẩm được triển khai trong môi trường của khách hàng hoặc được tung ra thị trường. Có thể có một số lỗi xảy ra trong môi trường của khách hàng, bảo trì được thực hiện để khắc phục những vấn đề trong môi trường khách hàng. Hoặc khi cần nâng cấp sản phẩm, thêm tính năng mới, bảo trì sẽ được sẽ được thực hiện.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;b. Ưu, nhược điểm&lt;/h3&gt;
&lt;p&gt;Mô hình thác nước được thực hiện tuần tự tuyến tính, do vậy ưu điểm lớn nhất của nó là đơn giản, dễ dàng để học và thực hiện. Mô hình đảm bảo chất lượng cao cho sản phẩm.&lt;/p&gt;
&lt;p&gt;Mỗi pha của mô hình được thực hiện khi pha trước đấy đã được hoàn thiện. Vì vậy, cần làm mỗi pha phải tốt, chất lượng, khi đó mới thực hiện pha tiếp theo do nếu xảy ra lỗi việc quay lại pha trước đáy sẽ mất rất nhiều thời gian.&lt;/p&gt;
&lt;p&gt;Do đó, công việc thực thiện của người phát triển nhiều hơn. Mỗi giai đoạn không chỉ thực thi mà còn viết docs và giảng giải cho những người giai đoạn tiếp theo. Dẫn đến người phát triển cần bỏ ra nhiều thời gian hơn làm cho sản phẩm thực hiện theo mô hình này thường thực hiện lâu, chi phí cao.&lt;/p&gt;
&lt;p&gt;Đây là nhược điểm rõ ràng nhất của mô hình này. Bù lại, sản phẩm sẽ đảm bảo chất lượng cao do mỗi pha được thực hiện một cách hoàn chỉnh, đầy đủ, chính xác.&lt;/p&gt;
&lt;p&gt;Chính vì vậy, mô hình thác nước chỉ phù hợp với các dự án vừa và nhỏ do những dự án này có thể xác định rõ yêu cầu từ đầu. Ngược lại, đối với những dự án lớn, việc xác định yêu cầu rõ ràng từ ban đầu là rất khó. Ngoài ra, việc làm tuần tự với dự án lớn là rất tốn thời gian khi mà khách hàng luôn mong muốn có sản phẩm chất lượng càng sớm càng tốt.&lt;/p&gt;
&lt;p&gt;Mô hình thác nước là mô hình đầu tiên, mọi mô hình cơ bản hiện nay đều xuất phát, phát triển từ mô hình thác nước.&lt;/p&gt;
&lt;h2&gt;2. Mô hình chữ V&lt;/h2&gt;
&lt;p&gt;Chất lượng kiểm thử phần mềm được đánh giá qua bộ test kiểm thử và quá trình thực hiện kiểm thử. Do vậy, chất lượng của bộ test kiểm thử là rất quan trọng. Tuy nhiên, mô hình thác nước đến giai đoạn kiểm thử mới được sinh test. Việc sinh test của mô hình thác nước khiến cho khả năng phát hiện ra lỗi của bộ test rất là thấp, dẫn đến việc kiểm thử, đánh giá chất lượng phần mềm không khách quan, chính xác. Và mô hình chữ V đã được cải tiến từ thác nước để giải quyết vấn đề sinh test muộn từ mô hình thấy nước.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;src/assets/media/post/v-model.png&quot; alt=&quot;Mô hình chữ V&quot; /&gt;&lt;/p&gt;
&lt;h3&gt;a. Đặc điểm&lt;/h3&gt;
&lt;p&gt;Cũng giống như mô hình thác nước, các giai đoạn trong mô hình chữ V được thực hiện một cách tuần tự theo hình chữ V. Tuy nhiên, cải tiến của mô hình chữ V đến từ việc sinh bộ test kiểm thử sớm bằng cách tích hợp liên kết giai đoạn kiểm thử cho từng giai đoạn phát triển tương ứng. Có nghĩa là, đối với mỗi giai đoạn trong chu kỳ phát triển sẽ có giai đoạn kiểm thử tương ứng. Mỗi giai đoạn bên trái model sẽ sinh bộ test cho giai đoạn kiểm thử bên phải của model:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Giai đoạn phân tích đặc tả yêu cầu sinh bộ test cho giai đoạn kiểm thử hệ thống, giai đoạn kiểm thử chấp nhận&lt;/li&gt;
&lt;li&gt;Giai đoạn thiết kế sinh bộ test cho giai đoạn kiểm thử tích hợp.&lt;/li&gt;
&lt;li&gt;Quá trình implementing thực hiện luôn giai đoạn kiểm thử unit.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;b. Ưu, nhược điểm&lt;/h3&gt;
&lt;p&gt;Do V Model kế thừa hầu hết từ Waterfall Model nên mọi ưu, nhược điểm đa số đều giống Waterfall Model. Tuy nhiên, ưu điểm nổi bật nhất so với Waterfall Model là giai đoạn kiểm thử được thực hiện ngay từ đầu qua công việc sinh bộ test sớm cho giai đoạn kiểm thử tương ứng nên bộ test trong V Model chính xác hơn. Nhờ vậy, việc kiểm thử được thực hiện nhanh chóng, chính xác và đánh giá phần mềm một cách khách quan nhất.&lt;/p&gt;
&lt;h2&gt;3. Bản mẫu&lt;/h2&gt;
&lt;p&gt;Đặc điểm của mô hình thác nước là cần xác định rõ yêu cầu từ đầu. Nhưng trong thực tế rất ít dự án xác định được rõ yêu cầu từ đầu. Vậy cần phải làm sao? Bản mẫu chính là giải pháp giải quyết vấn đề này.&lt;/p&gt;
&lt;h3&gt;a. Đặc điểm&lt;/h3&gt;
&lt;p&gt;Bản mẫu là mô hình phát triển phần mềm được phát triển dựa trên các yêu cầu của hệ thống. Dựa vào bản mẫu, khách hàng có cái nhìn tổng quan về hệ thống thực tế.&lt;/p&gt;
&lt;p&gt;Bản mẫu cho phép hiểu các yêu cầu của khách hàng ở giai đoạn phát triển ban đầu ngay cả những yêu cầu khó xác định. Nhờ nhận được những phản hồi có giá trị từ khách hàng, từ đó giúp các nhà thiết kế và phát triển phần mềm hiểu chính xác những gì được mong đợi từ sản phẩm đang được phát triển.&lt;/p&gt;
&lt;p&gt;Chúng ta cần dồn chi phí và nguồn lực nhiều cho làm bản mẫu, dẫn tới các công đoạn còn lại sẽ thiếu thời gian và chi phí, ... Kết quả là chất lượng sản phẩm nhất là tài liệu và chất lượng code dễ có vấn đề. Ngoài ra, việc chú trọng đến bản mẫu (thường cho yêu cầu chức năng) dẫn đến dễ bỏ qua các yêu cầu phi chức năng và đa số các dự án không đạt các yêu cầu phi chức năng. Chính vì vậy, thực tế nó không được dùng phổ biến như một mô hình phát triển phần mềm. Tuy nhiên, tư tưởng của bản mẫu rất quan trọng. Nó là mấu chốt để chúng ta lôi kéo khách hàng, giải quyết những yêu cầu không được rõ ràng. Do đó, người ta thường dùng bản mẫu như là một phương pháp thu thập yêu cầu cho các mô hình khác.
Bây giờ, người ta dùng bản mẫu như là một kỹ thuật thu thập yêu cầu.&lt;/p&gt;
&lt;h3&gt;b. Ưu, nhược điểm&lt;/h3&gt;
&lt;p&gt;Ưu điểm lớn nhất của bản mẫu giúp ta giải quyết việc xác định các yêu cầu khó của mô hình thác nước. Nhờ bản mẫu, việc xác định yêu cầu được diễn ra thuận chính xác nhờ đó việc thực hiện phần mềm dựa trên mô hình thác nước được diễn ra một cách thuận lợi.
Bù lại, chúng ta cần chi phí cao khi sử dụng bản mẫu do cần nhiều nguồn lực cho việc làm bản mẫu. Ngoài ra, sự tham gia của khách hàng vào bản mẫu là rất quan trọng.&lt;/p&gt;
&lt;p&gt;Tuy nhiên trong thực tế, việc nhiệt tình tham gia của khách hàng là khó, dẫn tới xây dựng bản mẫu khó chính xác. Từ đó, yêu cầu xác định không được rõ ràng.&lt;/p&gt;
&lt;p&gt;Mặc dù vậy, chúng ta vẫn có cách làm được nhưng không phải là điều dễ dàng.&lt;/p&gt;
&lt;h2&gt;4. Xoắn ốc&lt;/h2&gt;
&lt;p&gt;Mô hình thác nước, chữ V chỉ phù hợp các dự án vừa và nhỏ, những dự án được xác định rõ yêu cầu từ đầu. Việc xác định những yêu cầu khó, phức tạp đã được giải quyết nhờ bản mẫu. Tuy nhiên, trong thực tế, đa số có dự án đều lớn, phức tạp. Vậy với các dự án lớn thì giải quyết như nào? Mô hình xoắn ốc là một lựa chọn mô hình để thực hiện các dự án lớn, phức tạp.&lt;/p&gt;
&lt;p&gt;&amp;lt;!-- &lt;img src=&quot;src/assets/media/post/xoan-oc.png&quot; alt=&quot;Mô hình xoắn ốc&quot; /&gt; --&amp;gt;&lt;/p&gt;
&lt;h3&gt;a. Đặc điểm&lt;/h3&gt;
&lt;p&gt;Mô hình xoắn ốc là mô hình phát triển phần mềm kết hợp ý tưởng phát triển lặp đi lặp lại với các giai đoạn có hệ thống, được kiểm soát của mô hình thác nước.&lt;/p&gt;
&lt;p&gt;Mô hình xoắn ốc này là sự kết hợp giữa mô hình quy trình phát triển lặp và mô hình phát triển tuyến tính tuần tự, tức là mô hình thác nước với sự nhấn mạnh rất cao vào phân tích rủi ro. Nó cho phép phát hành sản phẩm qua mỗi lần lặp xung quanh hình xoắn ốc.
Thực chất, mô hình xoắn ốc được kết hợp từ mô hình thác nước sử dụng bản mẫu để giải quyết những yêu cầu khó. Ngoài ra nó còn có quá trình phân tích rủi ro của dự án.&lt;/p&gt;
&lt;p&gt;Mô hình xoắn ốc là quy trình phát triển định hướng rủi ro cho các dự án phần mềm, nghĩa là trọng tâm dự án quản lý rủi ro thông qua nhiều lần lặp lại quy trình phát triển phần mềm.&lt;/p&gt;
&lt;p&gt;Mặc dù, các quá trình được lặp đi lặp lại nhưng mô hình vẫn theo tư tưởng phát triển tăng dần.&lt;/p&gt;
&lt;h4&gt;Các giai đoạn trong phát triển phần mềm theo mô hình xoắn ốc&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Planning&lt;/strong&gt;: Lập kế hoạch, trong đó phạm vi của dự án được xác định. Một kế hoạch được tạo ra cho lần lần lặp tiếp theo của mô hình xoắn ốc.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Risk Analysis&lt;/strong&gt;: Trong giai đoạn này, các rủi ro liên quan đến dự án được xác định và đánh giá.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Engineering&lt;/strong&gt;: Trong giai đoạn này, phần mềm được phát triển dựa trên lần thu thập yêu cầu trước đó.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evaluation&lt;/strong&gt;: Phần mềm được đánh giá để xác định xem nó có đáp ứng các yêu cầu của khách hàng hay không và liệu nó có chất lượng cao hay không?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Planning&lt;/strong&gt;: Vòng lặp tiếp theo của vòng xoắn ốc bắt đầu bằng một giai đoạn lập kế hoạch mới, dựa trên kết quả đánh giá.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Trong thực tế, chúng ta không thể bê bản mẫu hay mô hình thác nước vào dự án lớn do đối với những dự án càng lớn, tỉ lệ thất bại càng cao.
Vậy chúng ta thực hiện nó như thế nào? Chúng ta có hai chiến lược để thực hiện quá trình phát triển:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Quá trình Implementing&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Theo thống kê tính năng khách hàng sử dụng, nhận thấy có 20% tính năng bắt buộc phải có của phần mềm (core), 40 % tính năng thường xuyên được sử dụng và 40% tính năng hiếm khi sử dụng.&lt;/p&gt;
&lt;p&gt;Chúng ta có thể thực hiện quá trình implementing một cách tuần tự, tức là thực hiện xong 20% tính năng bắt buộc, sau khi quá trình kiểm thử được diễn ra xong thành công, mới tiếp tục thực hiện 40% tính năng thường xuyên sử dụng. Tiếp tục với 40% tính năng hiếm khi sử dụng.
Nhờ vậy, giúp chúng ta giảm thiểu khi thực hiện. Đây gọi là chống rủi ro.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Phân tích rủi ro&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nhận thấy, khi chúng ta muốn xây một căn nhà cao 5 tầng, chúng ta có thể xây 3 tầng đầu tiên. Sau một thời gian, chúng ta có thể xây 2 tầng còn lại. Tuy nhiên, cái móng để xây nhà phải làm chắc có thể trụ được 5 tầng, không phải chỉ cho 3 tầng của tòa nhà được.
Chính vì vậy, khi chúng ta thực hiện 20% tính năng core của phần mềm thì base không chỉ đủ 20% core đấy mà phải làm base cho cả hệ thống. Nhờ vậy, giảm thiểu rủi ro cho hệ thống.&lt;/p&gt;
&lt;p&gt;Chúng ta, cần phải phát hiện rủi ro có thể xảy ra với dự án. Từ đó, xây dựng phương án giảm thiểu thiệt hại.&lt;/p&gt;
&lt;p&gt;Tuy nhiên, mô hình xoắn ốc lại không được dùng trong thực tế do tính khả thi của phân tích rủi ro. Để phân tích được rủi ro, chúng ta cần những người có kinh nghiệm lớn, trải qua nhiều số người như này rất hiếm.
Tuy nhiên, mô hình xoắn lại có giá trị cao về học thuật.&lt;/p&gt;
&lt;h3&gt;b. Ưu, nhược điểm&lt;/h3&gt;
&lt;p&gt;Ưu điểm của mô hình xoắn ốc là nó cho phép các yếu tố, tính năng của sản phẩm được thêm vào ngay cả khi chúng đã được thực thi. Điều này đảm bảo rằng không có xung đột với các yêu cầu và thiết kế trước đó của phần mềm.&lt;/p&gt;
&lt;p&gt;Mô hình xoắn ốc giải quyết được các hạn chế của mô hình thác nước. Các yêu cầu trong mô hình xoắn ốc có thể được thay đổi. Nhờ đó, các yêu cầu được xác định rõ ràng, chính xác hơn. Việc phát triển phần theo mô hình xoắn ốc giúp cho khách hàng có thể thấy toàn bộ hệ thống sớm hơn.
Quá trình phát triển có thể được chia các phần nhỏ hơn và các phần rủi ro có thể được phát triển sớm hơn giúp quản lý rủi ro tốt hơn.&lt;/p&gt;
&lt;p&gt;Tuy nhiên, việc quản lý, quy trình phát triển mô hình xoắn ốc sẽ phức tạp. Từ đó, mô hình xoắn ốc không phù hợp với các mô hình vừa và nhỏ hoặc rủi ro thấp. Ngoài ra, số lượng lớn các giai đoạn đòi hỏi cần một số lượng lớn tài liệu.&lt;/p&gt;
&lt;h2&gt;Kết luận&lt;/h2&gt;
&lt;p&gt;Mô hình đóng vai trò là hướng đi và quyết định đến chất lượng đầu ra của sản phẩm. Vì vậy, việc xác định đúng và chính xác mô hình cho phần mềm là rất quan trọng. Mỗi mô hình quy trình tuân theo một loạt các bước duy nhất đối với loại của nó để đảm bảo thành công trong quy trình phát triển phần mềm. Tùy vào cấu trúc phần mềm mà từ đó chọn đúng mô hình phần mềm để thực hiện. Trong thực tế, chúng ta không nhất thiết phải sử dụng đúng các mô hình phần mềm trên mà có thể &quot;biến tấu&quot;, thay đổi để phù hợp với dự án phần mềm.&lt;/p&gt;
</content:encoded><h:img src="/_astro/Software-Development-Life-Cycle.DybuXXTe.png"/><enclosure url="/_astro/Software-Development-Life-Cycle.DybuXXTe.png"/></item><item><title>Welcome to the new world!</title><link>https://zhaospei.github.io/blog/about-this-blog</link><guid isPermaLink="true">https://zhaospei.github.io/blog/about-this-blog</guid><description>The first post in the world</description><pubDate>Wed, 22 Feb 2023 08:00:00 GMT</pubDate><content:encoded>&lt;p&gt;Chào!
Hmm, đầu tiên phải giới thiệu đôi chút về bản thân nhỉ? À chắc không cần, vì mấy cái câu giới thiệu bản thân tớ
viết đi viết lại ở hầu hết mọi trang của blog rồi... LOL.&lt;/p&gt;
&lt;p&gt;Lần đầu tiên viết bài, không biết viết như nào :)). Chắc tớ sẽ nói vì sao tớ lại tạo cái blog nào và cách tớ thực hiện nó.&lt;/p&gt;
&lt;h2&gt;Tại sao tớ lại xây dựng cái blog này?&lt;/h2&gt;
&lt;p&gt;À chắc cái blog này được tạo bởi vì các ý nghĩ vớ vẩn của tớ tại một thời điểm nào đấy.
Tớ tạo blog này với mục đích ban đầu là tạo ra một chỗ lưu trữ những bài viết của tớ, nó có thể là các bài viết học thuật hay là
những bài viết vớ vẩn chia sẻ trải nghiệm của bản thân tớ. Nhưng mục đích cuối cùng tớ muốn hướng tới là cải thiện khả năng viết
lách của bản thân và tìm một niềm vui khác khi rảnh.&lt;/p&gt;
&lt;h2&gt;Cách tớ thực hiện cái blog này...&lt;/h2&gt;
&lt;p&gt;Ban đầu, tớ định dùng &lt;strong&gt;Django&lt;/strong&gt; để xây dựng blog nhưng nghĩ lại là dùng &lt;strong&gt;Django&lt;/strong&gt; thì host kiểu gì vì tớ không có tiền để duy trì hosting. Tớ cũng đã nghĩ đến các hosting free nhưng mà nó khá là ba chấm và deloy &lt;strong&gt;Django&lt;/strong&gt; rất rườm rà. Và rồi, tớ nghĩ sao tớ
không dùng chính &lt;strong&gt;Github Pages&lt;/strong&gt; để host nhỉ? Cuối cùng, tớ đã quyết định dùng nó để host và sử dụng &lt;strong&gt;jekyll&lt;/strong&gt; để xây dựng blog.
Và rồi, tớ đã bắt tay vào làm nó...&lt;/p&gt;
&lt;p&gt;Về giao diện của blog, tớ lấy cảm hứng từ &lt;a href=&quot;https://reaganhenke.com/&quot;&gt;Reaganhenke&lt;/a&gt; và &lt;a href=&quot;https://www.digitalocean.com/&quot;&gt;Digital Ocean&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Tớ biết đến &lt;a href=&quot;https://reaganhenke.com/&quot;&gt;Reaganhenke&lt;/a&gt; nhờ vào một hôm tìm cái trang web nghe nhạc lofi để học. Sau khi tìm hiểu, tớ có biết đến &lt;a href=&quot;https://imissmycafe.com/&quot;&gt;I Miss My Cafe&lt;/a&gt;. Đây là một trang đối với tớ khá là hay, recommend cậu trang này nhé!(à cậu có thể vào để tìm hiểu về nó nhé :))). Tính tớ hay tò mò nên tớ đã lục lọi để tìm ra source của nó để clone lại một trang giống như vậy :)). Kết quả là tớ chỉ tìm được page của người làm ra nó. Và rồi, tớ bị thu hút bởi thiết kế của page, nhất là mấy cái card đầu của page. Haha, và rồi tớ lấy nó để làm giao diện.
&lt;em&gt;(Reagan Henke, if you see this blog, please forgive me for using it without your permission!)&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Còn &lt;a href=&quot;https://www.digitalocean.com/&quot;&gt;Digital Ocean&lt;/a&gt; thì một lần tớ tìm tutorial và tìm ra nó. Đây cũng một trang chất lượng, recommend cậu trang này nữa nhé! (À thực ra ai cũng biết đến trang này :))).&lt;/p&gt;
&lt;p&gt;Ngoài ra tớ còn sử dụng &lt;a href=&quot;https://vincentgarreau.com/particles.js/&quot;&gt;particles.js&lt;/a&gt; để làm mấy cái lines nối nhau trông rườm rà ở background và &lt;a href=&quot;https://talk.hyvor.com/&quot;&gt;Hyvor&lt;/a&gt; để làm phần comment (Tớ chỉ được dùng thử 15 ngày nên không biết 15 ngày sau nó có bị mất không :))).&lt;/p&gt;
&lt;h2&gt;Mục đích hướng đến của blog...&lt;/h2&gt;
&lt;p&gt;Blog này tớ dự định sẽ là chia sẻ những bài viết, chủ đề mà tớ tìm hiểu và nghiên cứu. Hiện tại, tớ đang theo Lab CNPM nên đa số bài viết sẽ liên quan đến chủ đề này. Nếu cậu cũng đang tìm hiểu về chủ đề này thì có thể đọc blog của tớ nhé để giúp tớ cản thiện bài viết của bản thân. Ngoài ra, tớ cũng đang tìm hiểu về AI/ML nữa. Cũng có thể sẽ là những bài viết chia sẻ những trải nghiệm của bản thân. Nhưng mà tớ rất là lười viết nên có thể rất lâu tớ mới viết một bài mới _D:.&lt;/p&gt;
&lt;p&gt;Cậu cũng có thể gửi bài tới blog nhé. Rất là khuyến khích ủng hộ luôn.&lt;/p&gt;
&lt;h2&gt;Mong muốn của tớ...&lt;/h2&gt;
&lt;p&gt;Tớ tạo blog vì ý nghĩ nhất thời và mục đích cá nhân là nhiều nên tớ cũng không mong blog sẽ được biết đến rộng rãi. À được nhiều người biết thì cũng không sao. Mong rằng với những người biết đến blog của tớ sẽ nhận được điều gì đó mới mẻ khi đọc những bài viết vớ vẩn của tớ. Vậy thôi...&lt;/p&gt;
&lt;p&gt;Cậu có thể xem source code của blog tớ tại &lt;a href=&quot;https://github.com/zhaospei/zhaospei.github.io&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Cảm ơn cậu đã ghé thăm blog của tớ và đọc cái post mở đầu xàm này.
Nếu có gì góp ý cho blog thì cậu comment phía dưới nhé, tớ sẽ sớm khắc phục. (À comment sớm nhé chứ phần comment chỉ được dùng thử 15 ngày :)).&lt;/p&gt;
&lt;p&gt;&amp;lt;br&amp;gt;
&amp;lt;div style=&quot;text-align: right&quot;&amp;gt;
See yaaaaaaaaaaaaaaaaaaaaaaaaaaaa,
&amp;lt;/div&amp;gt;&lt;/p&gt;
&lt;p&gt;&amp;lt;div style=&quot;text-align: right&quot;&amp;gt;
&amp;lt;strong&amp;gt;Zhao&amp;lt;/strong&amp;gt;
&amp;lt;/div&amp;gt;&lt;/p&gt;
</content:encoded><h:img src="/_astro/about-this-blog.Cm-HNgCo.jpg"/><enclosure url="/_astro/about-this-blog.Cm-HNgCo.jpg"/></item></channel></rss>